{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b54619af",
      "metadata": {
        "id": "b54619af"
      },
      "source": [
        "# MFEGSN ‚Äî Pipeline Colab (Marker + LangExtract)\n",
        "\n",
        "Notebook optimis√© pour une ex√©cution **pas √† pas** sur Google Colab.\n",
        "\n",
        "**üöÄ Optimisation:** Les mod√®les (~4GB) sont sauvegard√©s sur Google Drive apr√®s le premier t√©l√©chargement.\n",
        "Les sessions suivantes chargeront les mod√®les depuis Drive en quelques secondes.\n",
        "\n",
        "**Ordre recommand√©:** √âtapes 1 ‚Üí 10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "014f8fe4",
      "metadata": {
        "id": "014f8fe4"
      },
      "source": [
        "## √âtape 1 ‚Äî Monter Google Drive\n",
        "**IMPORTANT:** Ex√©cutez cette cellule EN PREMIER pour permettre le cache des mod√®les."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "82dce4c1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82dce4c1",
        "outputId": "a0ec9951-f2f0-4a90-8312-2132ae67c838"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "============================================================\n",
            "‚úÖ Google Drive mont√©! \n",
            "============================================================\n",
            "üìÇ Cache Drive:  /content/drive/MyDrive/.mfegsn_cache\n",
            "üíæ Taille du cache: 3.53 GB\n",
            "üöÄ Mod√®les d√©j√† en cache!  Le chargement sera rapide.\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Cr√©er le dossier de cache sur Drive\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# === CONFIGURATION DU CACHE ===\n",
        "DRIVE_CACHE_DIR = Path(\"/content/drive/MyDrive/.mfegsn_cache\")\n",
        "DRIVE_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Sous-dossiers pour chaque type de cache\n",
        "HF_CACHE_DRIVE = DRIVE_CACHE_DIR / \"huggingface\"\n",
        "TORCH_CACHE_DRIVE = DRIVE_CACHE_DIR / \"torch\"\n",
        "DATALAB_CACHE_DRIVE = DRIVE_CACHE_DIR / \"datalab\"\n",
        "\n",
        "for cache_dir in [HF_CACHE_DRIVE, TORCH_CACHE_DRIVE, DATALAB_CACHE_DRIVE]:\n",
        "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Configurer les variables d'environnement pour utiliser le cache Drive\n",
        "os. environ[\"HF_HOME\"] = str(HF_CACHE_DRIVE)\n",
        "os.environ[\"TRANSFORMERS_CACHE\"] = str(HF_CACHE_DRIVE)\n",
        "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = str(HF_CACHE_DRIVE)\n",
        "os.environ[\"TORCH_HOME\"] = str(TORCH_CACHE_DRIVE)\n",
        "os.environ[\"XDG_CACHE_HOME\"] = str(DRIVE_CACHE_DIR)\n",
        "\n",
        "# V√©rifier la taille du cache existant\n",
        "def get_dir_size(path):\n",
        "    total = 0\n",
        "    if path.exists():\n",
        "        for f in path.rglob(\"*\"):\n",
        "            if f.is_file():\n",
        "                total += f.stat().st_size\n",
        "    return total / 1e9\n",
        "\n",
        "cache_size = get_dir_size(DRIVE_CACHE_DIR)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"‚úÖ Google Drive mont√©! \")\n",
        "print(\"=\"*60)\n",
        "print(f\"üìÇ Cache Drive:  {DRIVE_CACHE_DIR}\")\n",
        "print(f\"üíæ Taille du cache: {cache_size:.2f} GB\")\n",
        "if cache_size > 3:\n",
        "    print(\"üöÄ Mod√®les d√©j√† en cache!  Le chargement sera rapide.\")\n",
        "else:\n",
        "    print(\"üì• Premier lancement:  les mod√®les seront t√©l√©charg√©s et mis en cache.\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c063508",
      "metadata": {
        "id": "3c063508"
      },
      "source": [
        "## √âtape 2 ‚Äî Installer les d√©pendances\n",
        "Installation des packages Python n√©cessaires."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6a7a1e27",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a7a1e27",
        "outputId": "7ccb3668-75ae-434b-e37f-72a4250dc68f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Selecting previously unselected package zstd.\n",
            "(Reading database ... 121689 files and directories currently installed.)\n",
            "Preparing to unpack .../zstd_1.4.8+dfsg-3build1_amd64.deb ...\n",
            "Unpacking zstd (1.4.8+dfsg-3build1) ...\n",
            "Setting up zstd (1.4.8+dfsg-3build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "\n",
            "üì¶ Versions install√©es:\n",
            "Version: 0.36.0\n",
            "Version: 4.57.6\n",
            "Version: 1.10.1\n",
            "\n",
            "‚úÖ D√©pendances install√©es.\n"
          ]
        }
      ],
      "source": [
        "# D√©pendances syst√®me\n",
        "!apt-get update -qq\n",
        "! apt-get install -y zstd -qq\n",
        "\n",
        "# D√©pendances Python\n",
        "!python -m pip install -q --upgrade pip\n",
        "!python -m pip install -q marker-pdf[full] langextract google-generativeai pillow\n",
        "\n",
        "# Installer hf_transfer pour des t√©l√©chargements plus rapides (sans casser les d√©pendances)\n",
        "!pip install -q hf_transfer --no-deps\n",
        "\n",
        "# Activer hf_transfer\n",
        "import os\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "\n",
        "# V√©rifier les versions\n",
        "print(\"\\nüì¶ Versions install√©es:\")\n",
        "!pip show huggingface_hub 2>/dev/null | grep Version\n",
        "! pip show transformers 2>/dev/null | grep Version\n",
        "!pip show marker-pdf 2>/dev/null | grep Version\n",
        "\n",
        "print(\"\\n‚úÖ D√©pendances install√©es.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9950b81",
      "metadata": {
        "id": "e9950b81"
      },
      "source": [
        "## √âtape 3 ‚Äî Configurer les dossiers de travail\n",
        "Modifiez les chemins selon votre Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "749433f3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "749433f3",
        "outputId": "e12cafc3-cba0-4ebd-f2bd-2ae020671ea4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "üìÅ CONFIGURATION\n",
            "============================================================\n",
            "üìÇ Entr√©e: /content/drive/MyDrive/G√©opolitique et Souverainet√© Num√©riques/ALL/ALLPDF\n",
            "üìÇ Sortie: /content/drive/MyDrive/G√©opolitique et Souverainet√© Num√©riques/ALL/ALLMD\n",
            "üìÑ PDFs: 54\n",
            "üìÇ Structure par fichier:  <nom_fichier>/\n",
            "   ‚îú‚îÄ‚îÄ _ANALYSES/\n",
            "   ‚îú‚îÄ‚îÄ _FIGURES/\n",
            "   ‚îú‚îÄ‚îÄ _LOGS/\n",
            "   ‚îî‚îÄ‚îÄ _REFERENCES/\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# === MODIFIEZ CES CHEMINS ===\n",
        "INPUT_DIR = Path(\"/content/drive/MyDrive/G√©opolitique et Souverainet√© Num√©riques/ALL/ALLPDF\")\n",
        "OUTPUT_DIR = Path(\"/content/drive/MyDrive/G√©opolitique et Souverainet√© Num√©riques/ALL/ALLMD\")\n",
        "\n",
        "assert INPUT_DIR.exists(), f\"‚ùå Dossier introuvable: {INPUT_DIR}\"\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Dossier global pour les logs du pipeline\n",
        "GLOBAL_LOGS_DIR = OUTPUT_DIR / \"_PIPELINE_LOGS\"\n",
        "GLOBAL_LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def get_doc_folders(doc_name):\n",
        "    \"\"\"Cr√©e et retourne les dossiers pour un document donn√©.\"\"\"\n",
        "    doc_dir = OUTPUT_DIR / doc_name\n",
        "    doc_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    folders = {\n",
        "        \"root\": doc_dir,\n",
        "        \"figures\": doc_dir / \"_FIGURES\",\n",
        "        \"references\": doc_dir / \"_REFERENCES\",\n",
        "        \"analyses\": doc_dir / \"_ANALYSES\",\n",
        "        \"logs\": doc_dir / \"_LOGS\",\n",
        "    }\n",
        "\n",
        "    for folder in folders.values():\n",
        "        folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    return folders\n",
        "\n",
        "pdf_files = sorted([p for p in INPUT_DIR.iterdir() if p.suffix.lower() == \".pdf\"])\n",
        "print(\"=\"*60)\n",
        "print(\"üìÅ CONFIGURATION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"üìÇ Entr√©e: {INPUT_DIR}\")\n",
        "print(f\"üìÇ Sortie: {OUTPUT_DIR}\")\n",
        "print(f\"üìÑ PDFs: {len(pdf_files)}\")\n",
        "print(f\"üìÇ Structure par fichier:  <nom_fichier>/\")\n",
        "print(f\"   ‚îú‚îÄ‚îÄ _ANALYSES/\")\n",
        "print(f\"   ‚îú‚îÄ‚îÄ _FIGURES/\")\n",
        "print(f\"   ‚îú‚îÄ‚îÄ _LOGS/\")\n",
        "print(f\"   ‚îî‚îÄ‚îÄ _REFERENCES/\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a5c801b",
      "metadata": {
        "id": "8a5c801b"
      },
      "source": [
        "## √âtape 4 ‚Äî Ollama + Gemma 3 4B\n",
        "Installation et d√©marrage de Gemma 3 4B via Ollama."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "352143f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "352143f1",
        "outputId": "9c4caa0b-fc5d-4f3d-f2e7-1d37284e1efd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì¶ Installation d'Ollama...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "‚è≥ D√©marrage du serveur Ollama...\n",
            "‚úÖ Serveur Ollama pr√™t!\n",
            "\n",
            "üì• T√©l√©chargement de Gemma 3 4B (‚âà3GB)...\n",
            "   Cela peut prendre plusieurs minutes...\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "\n",
            "‚úÖ Gemma 3 4B t√©l√©charg√©!\n",
            "üî• Pr√©chauffage du mod√®le...\n",
            "‚úÖ Gemma 3 4B pr√™t et op√©rationnel!\n",
            "\n",
            "============================================================\n",
            "ü§ñ OLLAMA CONFIGUR√â\n",
            "============================================================\n",
            "Mod√®le: gemma3:4b\n",
            "Serveur: http://localhost:11434\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "USE_OLLAMA = True  # Activ√© par d√©faut pour Gemma local\n",
        "\n",
        "ollama_process = None\n",
        "\n",
        "if USE_OLLAMA:\n",
        "    import subprocess\n",
        "    import time\n",
        "    import os\n",
        "\n",
        "    # Installer requests si n√©cessaire\n",
        "    try:\n",
        "        import requests\n",
        "    except ImportError:\n",
        "        !pip install -q requests\n",
        "        import requests\n",
        "\n",
        "    # Installer Ollama\n",
        "    print(\"üì¶ Installation d'Ollama...\")\n",
        "    !curl -fsSL https://ollama.com/install.sh 2>/dev/null | sh 2>&1 | tail -n 3\n",
        "\n",
        "    # D√©marrer Ollama en arri√®re-plan\n",
        "    ollama_process = subprocess.Popen(\n",
        "        [\"ollama\", \"serve\"],\n",
        "        stdout=subprocess.DEVNULL,\n",
        "        stderr=subprocess.DEVNULL\n",
        "    )\n",
        "    print(\"‚è≥ D√©marrage du serveur Ollama...\")\n",
        "\n",
        "    # Attendre que le serveur soit pr√™t (avec timeout de 30 secondes)\n",
        "    server_ready = False\n",
        "    for i in range(30):\n",
        "        try:\n",
        "            response = requests.get(\"http://localhost:11434/api/tags\", timeout=2)\n",
        "            if response.status_code == 200:\n",
        "                print(\"‚úÖ Serveur Ollama pr√™t!\")\n",
        "                server_ready = True\n",
        "                break\n",
        "        except:\n",
        "            pass\n",
        "        time.sleep(1)\n",
        "\n",
        "    if not server_ready:\n",
        "        print(\"‚ö†Ô∏è Le serveur Ollama n'a pas d√©marr√© √† temps\")\n",
        "        print(\"   Essayez de r√©ex√©cuter cette cellule.\")\n",
        "\n",
        "    # T√©l√©charger Gemma 3 4B (SANS espace dans le nom!)\n",
        "    print(\"\\nüì• T√©l√©chargement de Gemma 3 4B (‚âà3GB)...\")\n",
        "    print(\"   Cela peut prendre plusieurs minutes...\")\n",
        "    !ollama pull gemma3:4b\n",
        "\n",
        "    # V√©rification\n",
        "    result = subprocess.run([\"ollama\", \"list\"], capture_output=True, text=True)\n",
        "    if \"gemma3:4b\" in result.stdout or \"gemma3\" in result.stdout:\n",
        "        print(\"\\n‚úÖ Gemma 3 4B t√©l√©charg√©!\")\n",
        "\n",
        "        # Pr√©chauffer le mod√®le avec une requ√™te test\n",
        "        print(\"üî• Pr√©chauffage du mod√®le...\")\n",
        "        test_result = subprocess.run(\n",
        "            [\"ollama\", \"run\", \"gemma3:4b\", \"R√©ponds uniquement: OK\"],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            timeout=30\n",
        "        )\n",
        "\n",
        "        if test_result.returncode == 0:\n",
        "            print(\"‚úÖ Gemma 3 4B pr√™t et op√©rationnel!\")\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"ü§ñ OLLAMA CONFIGUR√â\")\n",
        "            print(\"=\"*60)\n",
        "            print(\"Mod√®le: gemma3:4b\")\n",
        "            print(\"Serveur: http://localhost:11434\")\n",
        "            print(\"=\"*60)\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Avertissement: Le test du mod√®le a √©chou√©\")\n",
        "            print(\"   Le mod√®le devrait quand m√™me fonctionner.\")\n",
        "    else:\n",
        "        print(\"‚ùå Erreur: Gemma 3 4B non trouv√©\")\n",
        "        print(\"Mod√®les disponibles:\")\n",
        "        print(result.stdout if result.stdout else \"Aucun mod√®le\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è Ollama d√©sactiv√©. Mettez USE_OLLAMA = True pour l'activer.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04ae817d",
      "metadata": {
        "id": "04ae817d"
      },
      "source": [
        "## √âtape 5 ‚Äî Configurer Marker\n",
        "Chargement des mod√®les Marker (~4GB) depuis le cache Drive.\n",
        "\n",
        "**‚ö° Premier lancement:** 10-20 minutes (t√©l√©chargement + sauvegarde sur Drive)\n",
        "\n",
        "**üöÄ Lancements suivants:** 1-2 minutes (chargement depuis Drive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca0f4c3b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca0f4c3b",
        "outputId": "65415dac-6c19-4a09-8e44-e84604507857"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "üñ•Ô∏è CONFIGURATION MAT√âRIELLE\n",
            "============================================================\n",
            "Device: cuda\n",
            "GPU: Tesla T4\n",
            "VRAM: 15.8 GB\n",
            "\n",
            "üíæ Cache Drive: 3.53 GB\n",
            "üöÄ Mod√®les trouv√©s en cache! Chargement rapide...\n",
            "============================================================\n",
            "\n",
            "üì¶ Chargement des mod√®les Marker...\n",
            "   ‚ö° Chargement depuis le cache Drive...\n",
            "‚ö†Ô∏è Impossible d'importer OpenAILLMService. Le LLM sera d√©sactiv√© pour Marker.\n",
            "\n",
            "‚öôÔ∏è Cr√©ation du dictionnaire de mod√®les...\n",
            "‚úì Configuration du convertisseur...\n",
            "\n",
            "============================================================\n",
            "‚úÖ MARKER CONFIGUR√â AVEC SUCC√àS!\n",
            "============================================================\n",
            "‚è±Ô∏è Dur√©e: 0.4 minutes\n",
            "üì¶ T√©l√©charg√© cette session: 0.00 GB\n",
            "üíæ Cache total sur Drive: 3.53 GB\n",
            "üñºÔ∏è Extraction d'images: D√©sactiv√©e\n",
            "ü§ñ LLM: D√©sactiv√©\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "import shutil\n",
        "import base64\n",
        "import logging\n",
        "import warnings\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# === OPTION: EXTRACTION D'IMAGES ===\n",
        "EXTRACT_IMAGES = True  # ACTIV√â pour extraire les images/figures\n",
        "\n",
        "# === CONFIGURATION DES LOGS ===\n",
        "# D√©sactiver les warnings non critiques\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "# Configurer le logging pour r√©duire la verbosit√©\n",
        "logging.getLogger().setLevel(logging.WARNING)\n",
        "\n",
        "# R√©duire drastiquement les logs des biblioth√®ques tierces\n",
        "for logger_name in ['transformers', 'torch', 'PIL', 'urllib3', 'filelock',\n",
        "                     'huggingface_hub', 'marker', 'datasets']:\n",
        "    logging.getLogger(logger_name).setLevel(logging.ERROR)\n",
        "\n",
        "# Variables d'environnement pour r√©duire les logs\n",
        "os.environ['TRANSFORMERS_VERBOSITY'] = 'error'\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '0'  # Garder les barres de progression\n",
        "\n",
        "# S'assurer que le cache Drive est bien configur√©\n",
        "os.environ[\"HF_HOME\"] = str(HF_CACHE_DRIVE)\n",
        "os.environ[\"TRANSFORMERS_CACHE\"] = str(HF_CACHE_DRIVE)\n",
        "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = str(HF_CACHE_DRIVE)\n",
        "os.environ[\"TORCH_HOME\"] = str(TORCH_CACHE_DRIVE)\n",
        "# Forcer HF Transfer\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "\n",
        "# === FORCER LE T√âL√âCHARGEMENT DES MOD√àLES SURYA DEPUIS HUGGING FACE ===\n",
        "os.environ[\"SURYA_LAYOUT_MODEL\"] = \"vikp/surya_layout\"\n",
        "os.environ[\"SURYA_REC_MODEL\"] = \"vikp/surya_rec\"\n",
        "os.environ[\"SURYA_DET_MODEL\"] = \"vikp/surya_det\"\n",
        "os.environ[\"SURYA_ORDER_MODEL\"] = \"vikp/surya_order\"\n",
        "os.environ[\"SURYA_TABLE_REC_MODEL\"] = \"vikp/surya_tablerec\"\n",
        "os.environ[\"SURYA_DOWNLOAD_BACKEND\"] = \"huggingface\"\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"=\"*60)\n",
        "print(\"üñ•Ô∏è CONFIGURATION MAT√âRIELLE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Device: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "# V√©rifier le cache existant\n",
        "def get_dir_size(path):\n",
        "    total = 0\n",
        "    if path.exists():\n",
        "        for f in path.rglob(\"*\"):\n",
        "            if f.is_file():\n",
        "                try:\n",
        "                    total += f.stat().st_size\n",
        "                except:\n",
        "                    pass\n",
        "    return total / 1e9\n",
        "\n",
        "initial_cache_size = get_dir_size(DRIVE_CACHE_DIR)\n",
        "print(f\"\\nüíæ Cache Drive: {initial_cache_size:.2f} GB\")\n",
        "\n",
        "if initial_cache_size > 3:\n",
        "    print(\"üöÄ Mod√®les trouv√©s en cache! Chargement rapide...\")\n",
        "else:\n",
        "    print(\"üì• Premier t√©l√©chargement des mod√®les Marker (~4GB)\")\n",
        "    print(\"   ‚è±Ô∏è Dur√©e estim√©e: 10-20 minutes\")\n",
        "    print(\"   üíæ Les mod√®les seront sauvegard√©s sur Drive\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Chronom√®tre\n",
        "start_time = time.time()\n",
        "\n",
        "# Charger les mod√®les avec messages de progression\n",
        "print(\"\\nüì¶ Chargement des mod√®les Marker...\")\n",
        "if initial_cache_size > 3:\n",
        "    print(\"   ‚ö° Chargement depuis le cache Drive...\")\n",
        "else:\n",
        "    print(\"   üì• T√©l√©chargement en cours...\")\n",
        "\n",
        "from marker.converters.pdf import PdfConverter\n",
        "from marker.models import create_model_dict\n",
        "from marker.config.parser import ConfigParser\n",
        "\n",
        "# Tenter d'importer le service LLM appropri√©\n",
        "LLM_SERVICE_CLS = None\n",
        "USE_OLLAMA = True\n",
        "\n",
        "if USE_OLLAMA:\n",
        "    try:\n",
        "        from marker.llm.openai import OpenAILLMService\n",
        "        LLM_SERVICE_CLS = OpenAILLMService\n",
        "        print(\"‚úÖ Service LLM: OpenAILLMService (compatible Ollama)\")\n",
        "    except ImportError:\n",
        "        try:\n",
        "            from marker.services.openai import OpenAILLMService\n",
        "            LLM_SERVICE_CLS = OpenAILLMService\n",
        "            print(\"‚úÖ Service LLM: OpenAILLMService (via path alternatif)\")\n",
        "        except ImportError:\n",
        "            print(\"‚ö†Ô∏è Impossible d'importer OpenAILLMService. Le LLM sera d√©sactiv√© pour Marker.\")\n",
        "            USE_OLLAMA = False\n",
        "\n",
        "# Configuration pour Ollama via API OpenAI\n",
        "if USE_OLLAMA:\n",
        "    os.environ[\"MARKER_LLM_PROVIDER\"] = \"openai\"\n",
        "    os.environ[\"OPENAI_BASE_URL\"] = \"http://localhost:11434/v1\"\n",
        "    os.environ[\"OPENAI_API_KEY\"] = \"ollama\"\n",
        "    os.environ[\"OPENAI_MODEL\"] = \"gemma3:4b\"\n",
        "\n",
        "# Configuration Marker OPTIMIS√âE pour extraction compl√®te\n",
        "marker_config = {\n",
        "    \"workers\": 2,\n",
        "    \"extract_images\": EXTRACT_IMAGES,\n",
        "    \"images_as_base64\": True,  # R√©cup√©rer les images en base64 pour √©viter les erreurs de fichiers\n",
        "    \"use_llm\": USE_OLLAMA,\n",
        "    \"llm_provider\": \"openai\" if USE_OLLAMA else None,\n",
        "    \"llm_model\": \"gemma3:4b\" if USE_OLLAMA else None,\n",
        "    \"force_ocr\": False,\n",
        "    \"languages\": [\"fr\", \"en\"],\n",
        "    \"paginate_output\": True,\n",
        "    \"batch_size\": 4 if device == \"cuda\" else 2,\n",
        "    # Options suppl√©mentaires pour am√©liorer l'extraction\n",
        "    \"output_format\": \"markdown\",\n",
        "}\n",
        "\n",
        "print(\"\\n‚öôÔ∏è Cr√©ation du dictionnaire de mod√®les...\")\n",
        "model_dict = create_model_dict()\n",
        "\n",
        "print(\"‚úì Configuration du convertisseur...\")\n",
        "config_parser = ConfigParser(marker_config)\n",
        "\n",
        "converter = PdfConverter(\n",
        "    config=config_parser.generate_config_dict(),\n",
        "    artifact_dict=model_dict\n",
        ")\n",
        "\n",
        "# R√©sum√©\n",
        "elapsed = time.time() - start_time\n",
        "final_cache_size = get_dir_size(DRIVE_CACHE_DIR)\n",
        "downloaded = final_cache_size - initial_cache_size\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ MARKER CONFIGUR√â AVEC SUCC√àS!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"‚è±Ô∏è Dur√©e: {elapsed/60:.1f} minutes\")\n",
        "print(f\"üì¶ T√©l√©charg√© cette session: {max(0, downloaded):.2f} GB\")\n",
        "print(f\"üíæ Cache total sur Drive: {final_cache_size:.2f} GB\")\n",
        "print(f\"üñºÔ∏è Extraction d'images: {'Activ√©e' if EXTRACT_IMAGES else 'D√©sactiv√©e'}\")\n",
        "if USE_OLLAMA:\n",
        "    print(f\"ü§ñ LLM: Gemma 3 4B via Ollama (interface OpenAI)\")\n",
        "else:\n",
        "    print(\"ü§ñ LLM: D√©sactiv√©\")\n",
        "print(\"=\"*60)\n",
        "if downloaded > 0.5:\n",
        "    print(\"\\nüí° Les mod√®les sont maintenant en cache sur votre Drive.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd0fc302",
      "metadata": {
        "id": "dd0fc302"
      },
      "source": [
        "## √âtape 6 ‚Äî Fonctions utilitaires\n",
        "Extraction des r√©f√©rences, figures et conversion PDF ‚Üí Markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5a23cbb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5a23cbb",
        "outputId": "5bfbe08f-29e4-4e54-96aa-6cde13e03460"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Fonctions utilitaires d√©finies. \n"
          ]
        }
      ],
      "source": [
        "def extract_references_from_markdown(markdown_text):\n",
        "    \"\"\"Extrait la section r√©f√©rences/bibliographie du Markdown avec patterns tr√®s am√©lior√©s.\"\"\"\n",
        "    references = {\n",
        "        \"references_text\": \"\",\n",
        "        \"references_list\": [],\n",
        "        \"reference_count\": 0,\n",
        "    }\n",
        "\n",
        "    # Normaliser le texte (supprimer les sauts de ligne multiples excessifs)\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', markdown_text)\n",
        "\n",
        "    # === √âTAPE 1: Trouver la section r√©f√©rences ===\n",
        "    ref_section = \"\"\n",
        "\n",
        "    # Patterns pour trouver le D√âBUT de la section r√©f√©rences\n",
        "    section_start_patterns = [\n",
        "        r\"(?i)(?:^|\\n)#{1,4}\\s*(references?|r√©f√©rences?|bibliography|bibliographie|works?\\s*cited|cited\\s*works?|literature|sources?|notes?\\s*(?:and\\s*)?references?)\\s*\\n\",\n",
        "        r\"(?i)(?:^|\\n)\\*\\*(references?|r√©f√©rences?|bibliography|bibliographie)\\*\\*\\s*\\n\",\n",
        "        r\"(?i)(?:^|\\n)(REFERENCES?|R√âF√âRENCES?|BIBLIOGRAPHY|BIBLIOGRAPHIE|WORKS\\s*CITED)\\s*\\n\",\n",
        "        r\"(?i)(?:^|\\n)_{2,}?\\s*(references?|bibliography)\\s*_{2,}?\\s*\\n\",\n",
        "    ]\n",
        "\n",
        "    section_start_pos = -1\n",
        "    for pattern in section_start_patterns:\n",
        "        match = re.search(pattern, text)\n",
        "        if match:\n",
        "            section_start_pos = match.end()\n",
        "            break\n",
        "\n",
        "    if section_start_pos > 0:\n",
        "        # Prendre tout depuis le d√©but de la section jusqu'√† la fin ou prochaine section majeure\n",
        "        remaining = text[section_start_pos:]\n",
        "\n",
        "        # Chercher la fin de la section (prochain header de m√™me niveau ou sup√©rieur, ou fin)\n",
        "        end_patterns = [\n",
        "            r\"\\n#{1,3}\\s+[A-Z]\",  # Prochain header\n",
        "            r\"\\n\\*\\*[A-Z][a-z]+\\*\\*\\s*\\n\",  # Prochain titre en gras\n",
        "            r\"\\n(?:APPENDIX|ANNEXE|ACKNOWLEDGMENT)\",  # Sections suivantes typiques\n",
        "        ]\n",
        "\n",
        "        end_pos = len(remaining)\n",
        "        for pattern in end_patterns:\n",
        "            match = re.search(pattern, remaining, re.IGNORECASE)\n",
        "            if match and match.start() < end_pos:\n",
        "                end_pos = match.start()\n",
        "\n",
        "        ref_section = remaining[:end_pos].strip()\n",
        "\n",
        "    # === √âTAPE 2: Si pas de section trouv√©e, chercher dans les derniers 30% du document ===\n",
        "    if not ref_section or len(ref_section) < 100:\n",
        "        # Les r√©f√©rences sont souvent dans les derniers 30% du document\n",
        "        last_portion = text[int(len(text) * 0.7):]\n",
        "\n",
        "        # Chercher des blocs qui ressemblent √† des r√©f√©rences\n",
        "        # Pattern: lignes commen√ßant par [n], n., ou auteur suivi d'ann√©e\n",
        "        ref_block_pattern = r\"(?:^|\\n)((?:\\[\\d+\\]|\\d+\\.|[A-Z][a-z]+,?\\s+[A-Z])[^\\n]+(?:\\n(?!\\[\\d+\\]|\\d+\\.|[A-Z][a-z]+,\\s+[A-Z])[^\\n]+)*)\"\n",
        "        blocks = re.findall(ref_block_pattern, last_portion)\n",
        "\n",
        "        if len(blocks) >= 5:  # Au moins 5 r√©f√©rences-like\n",
        "            ref_section = \"\\n\".join(blocks)\n",
        "\n",
        "    # === √âTAPE 3: Extraction des r√©f√©rences individuelles ===\n",
        "    ref_lines = []\n",
        "\n",
        "    if ref_section:\n",
        "        references[\"references_text\"] = ref_section[:5000]  # Limiter la taille\n",
        "\n",
        "        # Patterns de d√©but de r√©f√©rence (ordre de priorit√©)\n",
        "        ref_patterns = [\n",
        "            # [1] Author...\n",
        "            r\"^\\[\\d+\\]\\s*(.+?)(?=\\n\\[\\d+\\]|\\n\\n|\\Z)\",\n",
        "            # 1. Author...\n",
        "            r\"^(\\d+)\\.\\s+([A-Z][^.]+\\.\\s+.+?)(?=\\n\\d+\\.|\\n\\n|\\Z)\",\n",
        "            # Author, A. (Year)...\n",
        "            r\"^([A-Z][a-z]+(?:[-'][A-Z][a-z]+)?,\\s+[A-Z]\\.(?:\\s*[A-Z]\\.)*\\s*(?:\\(\\d{4}\\)|,?\\s*\\d{4})[^.]+\\.[^\\n]+)\",\n",
        "            # Author (Year)...\n",
        "            r\"^([A-Z][a-z]+(?:\\s+(?:and|&|et)\\s+[A-Z][a-z]+)*\\s*\\(\\d{4}\\)[^\\n]+)\",\n",
        "            # - ou ‚Ä¢ Author...\n",
        "            r\"^[-‚Ä¢‚óè]\\s+(.+?)(?=\\n[-‚Ä¢‚óè]|\\n\\n|\\Z)\",\n",
        "        ]\n",
        "\n",
        "        lines = ref_section.split('\\n')\n",
        "        current_ref = \"\"\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if current_ref and len(current_ref) > 40:\n",
        "                    ref_lines.append(current_ref)\n",
        "                current_ref = \"\"\n",
        "                continue\n",
        "\n",
        "            # V√©rifier si c'est le d√©but d'une nouvelle r√©f√©rence\n",
        "            is_new_ref = False\n",
        "\n",
        "            # Num√©rotation explicite\n",
        "            if re.match(r\"^\\[\\d+\\]\", line) or re.match(r\"^\\d+\\.\\s+[A-Z]\", line):\n",
        "                is_new_ref = True\n",
        "            # Auteur suivi d'ann√©e entre parenth√®ses ou apr√®s virgule\n",
        "            elif re.match(r\"^[A-Z][a-z]+(?:[-'][A-Z][a-z]+)?,?\\s+[A-Z]\\..*\\(\\d{4}\\)\", line):\n",
        "                is_new_ref = True\n",
        "            elif re.match(r\"^[A-Z][a-z]+,?\\s+[A-Z]\\.\\s*(?:&|and|,)\\s*[A-Z]\", line):\n",
        "                is_new_ref = True\n",
        "            # Bullet points\n",
        "            elif re.match(r\"^[-‚Ä¢‚óè‚óã]\\s+[A-Z]\", line):\n",
        "                is_new_ref = True\n",
        "            # Auteur et al.\n",
        "            elif re.match(r\"^[A-Z][a-z]+\\s+et\\s+al\\.\", line):\n",
        "                is_new_ref = True\n",
        "\n",
        "            if is_new_ref:\n",
        "                if current_ref and len(current_ref) > 40:\n",
        "                    ref_lines.append(current_ref)\n",
        "                current_ref = line\n",
        "            elif current_ref:\n",
        "                current_ref += \" \" + line\n",
        "            elif len(line) > 50 and re.search(r'\\d{4}', line):\n",
        "                # Potentielle r√©f√©rence sans marqueur clair\n",
        "                current_ref = line\n",
        "\n",
        "        # Derni√®re r√©f√©rence\n",
        "        if current_ref and len(current_ref) > 40:\n",
        "            ref_lines.append(current_ref)\n",
        "\n",
        "    # === √âTAPE 4: Extraction alternative - citations in-text ===\n",
        "    if len(ref_lines) < 3:\n",
        "        # Chercher des r√©f√©rences cit√©es dans le texte avec format complet\n",
        "        inline_patterns = [\n",
        "            # Format num√©rique avec d√©tails\n",
        "            r\"\\[\\d+\\]\\s+[A-Z][a-z]+(?:,?\\s+[A-Z]\\.)+[^.]+\\.\\s+[^.]+\\.\\s+\\d{4}\",\n",
        "            # Format auteur-ann√©e complet\n",
        "            r\"[A-Z][a-z]+,?\\s+[A-Z]\\.(?:\\s*[A-Z]\\.)*\\s*\\(\\d{4}\\)\\.\\s+[^.]+\\.[^.]+\\.\",\n",
        "        ]\n",
        "\n",
        "        for pattern in inline_patterns:\n",
        "            matches = re.findall(pattern, text)\n",
        "            for m in matches:\n",
        "                if m not in ref_lines and len(m) > 50:\n",
        "                    ref_lines.append(m.strip())\n",
        "\n",
        "    # === √âTAPE 5: Nettoyage et d√©duplication ===\n",
        "    seen = set()\n",
        "    unique_refs = []\n",
        "    for ref in ref_lines:\n",
        "        # Nettoyer\n",
        "        ref = re.sub(r'\\s+', ' ', ref).strip()\n",
        "        ref = re.sub(r'^[\\[\\d+\\]|\\d+\\.|\\-‚Ä¢‚óè]\\s*', '', ref).strip()\n",
        "\n",
        "        # D√©duplication bas√©e sur les premiers 80 caract√®res\n",
        "        key = ref[:80].lower()\n",
        "        if key not in seen and len(ref) > 40:\n",
        "            seen.add(key)\n",
        "            unique_refs.append(ref)\n",
        "\n",
        "    references[\"references_list\"] = unique_refs[:200]  # Max 200 r√©f√©rences\n",
        "    references[\"reference_count\"] = len(unique_refs)\n",
        "\n",
        "    return references\n",
        "\n",
        "\n",
        "def extract_figures_info(markdown_text, images_dict):\n",
        "    \"\"\"Extrait les informations sur les figures du document avec d√©tection am√©lior√©e.\"\"\"\n",
        "    figures = []\n",
        "    seen_labels = set()\n",
        "\n",
        "    # === PATTERNS AM√âLIOR√âS pour les figures ===\n",
        "    fig_patterns = [\n",
        "        # Figure X: Title ou Fig. X: Title\n",
        "        r\"(?i)(?:^|\\n)\\s*((?:figure|fig\\.?)\\s*(\\d+(?:\\.\\d+)?)[:\\s.]*([^\\n]{0,200}))\",\n",
        "        # Figure X ‚Äî Title (avec tiret long)\n",
        "        r\"(?i)(?:^|\\n)\\s*((?:figure|fig\\.?)\\s*(\\d+(?:\\.\\d+)?)\\s*[‚Äî‚Äì-]\\s*([^\\n]{0,200}))\",\n",
        "        # Tableau X / Table X\n",
        "        r\"(?i)(?:^|\\n)\\s*((?:table(?:au)?)\\s*(\\d+(?:\\.\\d+)?)[:\\s.]*([^\\n]{0,200}))\",\n",
        "        # Chart/Graph/Diagram\n",
        "        r\"(?i)(?:^|\\n)\\s*((?:chart|graph|diagram|graphique|sch√©ma)\\s*(\\d+(?:\\.\\d+)?)?[:\\s.]*([^\\n]{0,150}))\",\n",
        "        # L√©gendes sous images markdown\n",
        "        r\"(?:!\\[[^\\]]*\\]\\([^)]+\\))\\s*\\n\\s*\\*([^*\\n]+)\\*\",\n",
        "        r\"(?:!\\[[^\\]]*\\]\\([^)]+\\))\\s*\\n\\s*_([^_\\n]+)_\",\n",
        "    ]\n",
        "\n",
        "    for pattern in fig_patterns:\n",
        "        for match in re.finditer(pattern, markdown_text, re.MULTILINE):\n",
        "            groups = match.groups()\n",
        "            if len(groups) >= 1:\n",
        "                full_match = groups[0] if groups[0] else match.group(0)\n",
        "                fig_num = groups[1] if len(groups) > 1 and groups[1] else \"\"\n",
        "                title = groups[2].strip() if len(groups) > 2 and groups[2] else \"\"\n",
        "\n",
        "                title = re.sub(r'^[:\\s.‚Äî‚Äì-]+', '', title).strip()\n",
        "\n",
        "                label = full_match.strip()[:100]\n",
        "                if label.lower() not in seen_labels:\n",
        "                    seen_labels.add(label.lower())\n",
        "                    figures.append({\n",
        "                        \"label\": label,\n",
        "                        \"number\": fig_num,\n",
        "                        \"title\": title,\n",
        "                        \"type\": \"figure\",\n",
        "                    })\n",
        "\n",
        "    # === D√âTECTER LES IMAGES MARKDOWN ===\n",
        "    img_md_pattern = r\"!\\[([^\\]]*)\\]\\(([^)]+)\\)\"\n",
        "    for match in re.finditer(img_md_pattern, markdown_text):\n",
        "        alt_text = match.group(1).strip()\n",
        "        img_src = match.group(2).strip()\n",
        "\n",
        "        if alt_text and alt_text.lower() not in seen_labels:\n",
        "            seen_labels.add(alt_text.lower())\n",
        "            figures.append({\n",
        "                \"label\": alt_text or f\"Image: {img_src[:50]}\",\n",
        "                \"title\": alt_text,\n",
        "                \"path\": img_src,\n",
        "                \"type\": \"embedded_image\",\n",
        "            })\n",
        "\n",
        "    # === AJOUTER LES IMAGES DU DICTIONNAIRE MARKER ===\n",
        "    if images_dict:\n",
        "        for idx, img_name in enumerate(images_dict.keys(), 1):\n",
        "            img_label = str(img_name)\n",
        "            if img_label.lower() not in seen_labels:\n",
        "                seen_labels.add(img_label.lower())\n",
        "                figures.append({\n",
        "                    \"label\": img_label,\n",
        "                    \"title\": \"\",\n",
        "                    \"path\": img_label,\n",
        "                    \"type\": \"extracted_image\",\n",
        "                    \"index\": idx,\n",
        "                })\n",
        "\n",
        "    return figures\n",
        "\n",
        "\n",
        "def save_figures(images_dict, doc_folders):\n",
        "    \"\"\"Sauvegarde les figures extraites dans le dossier _FIGURES du document.\"\"\"\n",
        "    if not images_dict:\n",
        "        return []\n",
        "\n",
        "    figures_folder = doc_folders[\"figures\"]\n",
        "    saved_paths = []\n",
        "    used_names = set()\n",
        "\n",
        "    for idx, (img_name, img_data) in enumerate(images_dict.items(), 1):\n",
        "        safe_name = re.sub(r\"[^a-zA-Z0-9_-]+\", \"_\", str(img_name))\n",
        "        safe_name = re.sub(r\"_+\", \"_\", safe_name).strip(\"_\")\n",
        "        safe_name = safe_name[:100]\n",
        "        if not safe_name or len(safe_name) < 3:\n",
        "            safe_name = f\"figure_{idx:03d}\"\n",
        "\n",
        "        base_name = safe_name\n",
        "        counter = 1\n",
        "        while safe_name in used_names:\n",
        "            safe_name = f\"{base_name}_{counter}\"\n",
        "            counter += 1\n",
        "        used_names.add(safe_name)\n",
        "\n",
        "        img_path = figures_folder / f\"{safe_name}.png\"\n",
        "\n",
        "        try:\n",
        "            if isinstance(img_data, Image.Image):\n",
        "                if img_data.mode in ('RGBA', 'LA', 'P'):\n",
        "                    rgb_image = Image.new('RGB', img_data.size, (255, 255, 255))\n",
        "                    if img_data.mode == 'P':\n",
        "                        img_data = img_data.convert('RGBA')\n",
        "                    rgb_image.paste(img_data, mask=img_data.split()[-1] if img_data.mode in ('RGBA', 'LA') else None)\n",
        "                    img_data = rgb_image\n",
        "                img_data.save(img_path, \"PNG\", optimize=True)\n",
        "            elif isinstance(img_data, (bytes, bytearray)):\n",
        "                with open(img_path, \"wb\") as f:\n",
        "                    f.write(img_data)\n",
        "            elif isinstance(img_data, str):\n",
        "                if img_data.startswith(\"data:image\"):\n",
        "                    b64_data = img_data.split(\",\", 1)[1] if \",\" in img_data else img_data\n",
        "                    with open(img_path, \"wb\") as f:\n",
        "                        f.write(base64.b64decode(b64_data))\n",
        "                elif Path(img_data).exists():\n",
        "                    shutil.copy(img_data, img_path)\n",
        "                else:\n",
        "                    try:\n",
        "                        with open(img_path, \"wb\") as f:\n",
        "                            f.write(base64.b64decode(img_data))\n",
        "                    except:\n",
        "                        continue\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            saved_paths.append(str(img_path))\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è Impossible de sauvegarder {safe_name}: {str(e)[:80]}\")\n",
        "            continue\n",
        "\n",
        "    return saved_paths\n",
        "\n",
        "\n",
        "def extract_metadata_from_markdown(markdown_text):\n",
        "    \"\"\"Extrait les m√©tadonn√©es du document (titre, auteurs, abstract, etc.).\"\"\"\n",
        "    metadata = {\n",
        "        \"title\": \"\",\n",
        "        \"authors\": [],\n",
        "        \"abstract\": \"\",\n",
        "        \"keywords\": [],\n",
        "        \"date\": \"\",\n",
        "    }\n",
        "\n",
        "    title_patterns = [\n",
        "        r\"^#\\s+([^\\n]+)\",\n",
        "        r\"^\\*\\*([^\\*\\n]{10,150})\\*\\*\",\n",
        "        r\"^([A-Z][^\\n]{20,150})\\n[=]+\",\n",
        "    ]\n",
        "    for pattern in title_patterns:\n",
        "        match = re.search(pattern, markdown_text[:2000], re.MULTILINE)\n",
        "        if match:\n",
        "            metadata[\"title\"] = match.group(1).strip()\n",
        "            break\n",
        "\n",
        "    abstract_pattern = r\"(?i)(?:abstract|r√©sum√©|summary)[:\\s]*\\n?([\\s\\S]{50,1500}?)(?=\\n\\n|\\n#{1,3}|\\n\\*\\*[A-Z])\"\n",
        "    match = re.search(abstract_pattern, markdown_text[:5000])\n",
        "    if match:\n",
        "        metadata[\"abstract\"] = match.group(1).strip()\n",
        "\n",
        "    keywords_pattern = r\"(?i)(?:keywords?|mots[- ]?cl√©s?)[:\\s]*([^\\n]+)\"\n",
        "    match = re.search(keywords_pattern, markdown_text[:5000])\n",
        "    if match:\n",
        "        kw_text = match.group(1)\n",
        "        keywords = re.split(r'[,;‚Ä¢¬∑]', kw_text)\n",
        "        metadata[\"keywords\"] = [k.strip() for k in keywords if k.strip() and len(k.strip()) > 2]\n",
        "\n",
        "    date_pattern = r\"\\b((?:19|20)\\d{2})\\b\"\n",
        "    dates = re.findall(date_pattern, markdown_text[:3000])\n",
        "    if dates:\n",
        "        metadata[\"date\"] = dates[0] if dates else \"\"\n",
        "\n",
        "    return metadata\n",
        "\n",
        "\n",
        "def convert_pdf_complete(pdf_path, doc_name):\n",
        "    \"\"\"Conversion compl√®te d'un PDF avec extraction figures et r√©f√©rences.\"\"\"\n",
        "    doc_folders = get_doc_folders(doc_name)\n",
        "\n",
        "    result_data = {\n",
        "        \"doc_name\": doc_name,\n",
        "        \"doc_folder\": str(doc_folders[\"root\"]),\n",
        "        \"markdown_path\": \"\",\n",
        "        \"figures\": [],\n",
        "        \"figures_paths\": [],\n",
        "        \"references\": {},\n",
        "        \"metadata\": {},\n",
        "        \"error\": None,\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        print(f\"   üìÑ Conversion de {pdf_path.name}...\")\n",
        "\n",
        "        try:\n",
        "            result = converter(str(pdf_path))\n",
        "        except Exception as conv_error:\n",
        "            error_msg = str(conv_error).lower()\n",
        "            if \"extension\" in error_msg or \"image\" in error_msg or \"unknown\" in error_msg:\n",
        "                print(f\"   ‚ö†Ô∏è Erreur d'extraction d'images, nouvelle tentative sans images...\")\n",
        "\n",
        "                from marker.config.parser import ConfigParser\n",
        "                temp_config = {\n",
        "                    \"workers\": 2,\n",
        "                    \"extract_images\": False,\n",
        "                    \"images_as_base64\": False,\n",
        "                    \"use_llm\": False,\n",
        "                    \"force_ocr\": False,\n",
        "                    \"languages\": [\"fr\", \"en\"],\n",
        "                    \"paginate_output\": True,\n",
        "                    \"batch_size\": 4 if device == \"cuda\" else 2,\n",
        "                }\n",
        "                config_parser = ConfigParser(temp_config)\n",
        "                from marker.converters.pdf import PdfConverter\n",
        "                temp_converter = PdfConverter(\n",
        "                    config=config_parser.generate_config_dict(),\n",
        "                    artifact_dict=model_dict,\n",
        "                    llm_service=None\n",
        "                )\n",
        "                result = temp_converter(str(pdf_path))\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "        markdown_text = getattr(result, \"markdown\", \"\") or \"\"\n",
        "        images_dict = getattr(result, \"images\", {}) or {}\n",
        "\n",
        "        print(f\"   üìä Markdown: {len(markdown_text)} caract√®res\")\n",
        "        print(f\"   üñºÔ∏è Images brutes de Marker: {len(images_dict)}\")\n",
        "\n",
        "        cleaned_images = {}\n",
        "        if images_dict:\n",
        "            for key, value in images_dict.items():\n",
        "                if key and value is not None:\n",
        "                    if not any(char in str(key) for char in ['?', '*', '<', '>', '|', '\\x00']):\n",
        "                        cleaned_images[key] = value\n",
        "\n",
        "        images_dict = cleaned_images\n",
        "\n",
        "        md_path = doc_folders[\"root\"] / f\"{doc_name}.md\"\n",
        "        with open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(markdown_text)\n",
        "\n",
        "        result_data[\"markdown_path\"] = str(md_path)\n",
        "\n",
        "        # === EXTRACTIONS ===\n",
        "        result_data[\"figures_paths\"] = save_figures(images_dict, doc_folders)\n",
        "        result_data[\"figures\"] = extract_figures_info(markdown_text, images_dict)\n",
        "        result_data[\"references\"] = extract_references_from_markdown(markdown_text)\n",
        "        result_data[\"metadata\"] = extract_metadata_from_markdown(markdown_text)\n",
        "\n",
        "        # Sauvegarder les r√©f√©rences\n",
        "        if result_data[\"references\"].get(\"reference_count\", 0) > 0:\n",
        "            ref_path = doc_folders[\"references\"] / f\"{doc_name}_references.json\"\n",
        "            with open(ref_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(result_data[\"references\"], f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        # Sauvegarder les m√©tadonn√©es\n",
        "        if result_data[\"metadata\"].get(\"title\"):\n",
        "            meta_path = doc_folders[\"analyses\"] / f\"{doc_name}_metadata.json\"\n",
        "            with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(result_data[\"metadata\"], f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        return result_data\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        result_data[\"error\"] = str(e)\n",
        "        error_log = doc_folders[\"logs\"] / \"error.txt\"\n",
        "        with open(error_log, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"Erreur: {str(e)}\\nTraceback:\\n{traceback.format_exc()}\\nDate: {datetime.now().isoformat()}\")\n",
        "        return result_data\n",
        "\n",
        "\n",
        "print(\"‚úÖ Fonctions utilitaires optimis√©es d√©finies.\")\n",
        "print(\"   ‚Ä¢ Extraction de r√©f√©rences: patterns acad√©miques tr√®s am√©lior√©s\")\n",
        "print(\"   ‚Ä¢ D√©tection de figures: texte + images Marker\")\n",
        "print(\"   ‚Ä¢ M√©tadonn√©es: titre, abstract, keywords\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f235df66",
      "metadata": {
        "id": "f235df66"
      },
      "source": [
        "## √âtape 7 ‚Äî LangExtract\n",
        "Extraction structur√©e avec LangExtract (activ√© par d√©faut avec Ollama)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4261fafe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4261fafe",
        "outputId": "eda7642b-8178-449d-9149-47e9b364f935"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Ollama d√©tect√© - Utilisation de Gemma 3 4B\n",
            "‚úÖ LangExtract configur√© (Provider: ollama, Model: gemma3:4b)\n"
          ]
        }
      ],
      "source": [
        "USE_LANGEXTRACT = True\n",
        "\n",
        "# V√©rifier si Ollama est disponible (ind√©pendamment de la config Marker)\n",
        "OLLAMA_AVAILABLE = False\n",
        "try:\n",
        "    import requests\n",
        "    response = requests.get(\"http://localhost:11434/api/tags\", timeout=2)\n",
        "    if response.status_code == 200:\n",
        "        OLLAMA_AVAILABLE = True\n",
        "        print(\"‚úÖ Ollama d√©tect√© - Utilisation de Gemma 3 4B\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è Ollama non disponible - LangExtract utilisera l'API directe\")\n",
        "\n",
        "LANGEXTRACT_CONFIG = {\n",
        "    \"provider\": \"ollama\" if OLLAMA_AVAILABLE else \"openai\",\n",
        "    \"model\": \"gemma3:4b\" if OLLAMA_AVAILABLE else \"gpt-3.5-turbo\",\n",
        "    \"base_url\": \"http://localhost:11434\" if OLLAMA_AVAILABLE else None,\n",
        "}\n",
        "\n",
        "# Prompt pour l'extraction structur√©e (utilis√© avec Ollama directement si LangExtract √©choue)\n",
        "EXTRACTION_PROMPT = \"\"\"Analysez ce document acad√©mique et retournez un JSON structur√© avec:\n",
        "{\n",
        "  \"contexte\": {\"theme\": \"\", \"zone_geographique\": \"\", \"periode\": \"\"},\n",
        "  \"acteurs\": {\"institutions\": [], \"pays\": [], \"organisations\": []},\n",
        "  \"concepts_cles\": [],\n",
        "  \"donnees_chiffrees\": [],\n",
        "  \"references_principales\": []\n",
        "}\n",
        "R√©pondez UNIQUEMENT avec le JSON, sans texte avant ou apr√®s.\"\"\"\n",
        "\n",
        "def _safe_json(obj):\n",
        "    try:\n",
        "        return json.loads(json.dumps(obj))\n",
        "    except Exception:\n",
        "        return {\"raw\": str(obj)}\n",
        "\n",
        "\n",
        "def extract_with_langextract(markdown_text, doc_name, doc_folders, references_data=None, figures_data=None):\n",
        "    \"\"\"Extraction structur√©e avec LangExtract ou Ollama directement.\"\"\"\n",
        "    if not USE_LANGEXTRACT:\n",
        "        return {\"status\": \"skipped\", \"reason\": \"USE_LANGEXTRACT=False\"}\n",
        "\n",
        "    # Pr√©parer le texte enrichi (limiter la taille pour le LLM)\n",
        "    max_chars = 15000  # Limite pour √©viter les timeouts\n",
        "    enriched_text = markdown_text[:max_chars]\n",
        "\n",
        "    if references_data and references_data.get(\"reference_count\", 0) > 0:\n",
        "        enriched_text += f\"\\n\\n[R√âF√âRENCES EXTRAITES: {references_data['reference_count']}]\"\n",
        "\n",
        "    if figures_data:\n",
        "        enriched_text += f\"\\n\\n[FIGURES D√âTECT√âES: {len(figures_data)}]\"\n",
        "\n",
        "    result = None\n",
        "\n",
        "    # === TENTATIVE 1: LangExtract (nouvelle API) ===\n",
        "    try:\n",
        "        import langextract as lx\n",
        "\n",
        "        # Essayer diff√©rentes signatures d'API\n",
        "        if hasattr(lx, \"extract\"):\n",
        "            # Nouvelle API sans prompt\n",
        "            try:\n",
        "                extraction = lx.extract(enriched_text)\n",
        "                result = _safe_json(extraction)\n",
        "            except TypeError:\n",
        "                # Essayer avec d'autres param√®tres\n",
        "                try:\n",
        "                    extraction = lx.extract(enriched_text, model=LANGEXTRACT_CONFIG[\"model\"])\n",
        "                    result = _safe_json(extraction)\n",
        "                except:\n",
        "                    pass\n",
        "        elif hasattr(lx, \"LangExtract\"):\n",
        "            try:\n",
        "                extractor = lx.LangExtract()\n",
        "                extraction = extractor.extract(enriched_text)\n",
        "                result = _safe_json(extraction)\n",
        "            except:\n",
        "                pass\n",
        "    except Exception as e:\n",
        "        pass  # Continuer avec Ollama\n",
        "\n",
        "    # === TENTATIVE 2: Ollama directement ===\n",
        "    if result is None and OLLAMA_AVAILABLE:\n",
        "        try:\n",
        "            import requests\n",
        "\n",
        "            ollama_payload = {\n",
        "                \"model\": \"gemma3:4b\",\n",
        "                \"prompt\": f\"{EXTRACTION_PROMPT}\\n\\nDOCUMENT:\\n{enriched_text[:10000]}\",\n",
        "                \"stream\": False,\n",
        "                \"options\": {\"temperature\": 0.1}\n",
        "            }\n",
        "\n",
        "            resp = requests.post(\n",
        "                \"http://localhost:11434/api/generate\",\n",
        "                json=ollama_payload,\n",
        "                timeout=120\n",
        "            )\n",
        "\n",
        "            if resp.status_code == 200:\n",
        "                ollama_result = resp.json()\n",
        "                response_text = ollama_result.get(\"response\", \"\")\n",
        "\n",
        "                # Extraire le JSON de la r√©ponse\n",
        "                json_match = re.search(r'\\{[\\s\\S]*\\}', response_text)\n",
        "                if json_match:\n",
        "                    try:\n",
        "                        result = json.loads(json_match.group())\n",
        "                    except json.JSONDecodeError:\n",
        "                        result = {\"raw_response\": response_text[:500]}\n",
        "                else:\n",
        "                    result = {\"raw_response\": response_text[:500]}\n",
        "        except Exception as e:\n",
        "            result = {\"status\": \"error\", \"error\": f\"Ollama: {str(e)}\"}\n",
        "\n",
        "    # === FALLBACK: Extraction basique sans LLM ===\n",
        "    if result is None:\n",
        "        result = {\n",
        "            \"status\": \"fallback\",\n",
        "            \"note\": \"Extraction LLM indisponible, donn√©es de base uniquement\",\n",
        "            \"references_count\": references_data.get(\"reference_count\", 0) if references_data else 0,\n",
        "            \"figures_count\": len(figures_data) if figures_data else 0,\n",
        "        }\n",
        "\n",
        "    # Sauvegarder le r√©sultat\n",
        "    extraction_path = doc_folders[\"analyses\"] / f\"{doc_name}_langextract.json\"\n",
        "    with open(extraction_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(result, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "print(f\"‚úÖ LangExtract configur√©\")\n",
        "print(f\"   Provider: {LANGEXTRACT_CONFIG['provider']}\")\n",
        "print(f\"   Model: {LANGEXTRACT_CONFIG['model']}\")\n",
        "print(f\"   Fallback Ollama: {'Activ√©' if OLLAMA_AVAILABLE else 'D√©sactiv√©'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "216e2839",
      "metadata": {
        "id": "216e2839"
      },
      "source": [
        "## √âtape 8 ‚Äî Test sur un PDF (optionnel)\n",
        "Permet de valider la configuration avant le batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4b595a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4b595a6",
        "outputId": "4fb45cdd-5553-4a12-a854-216bd156f193"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ Test sur:  Against Sovereignty in Cyberspace.pdf\n",
            "‚è≥ Conversion en cours...\n",
            "\n",
            "   DEBUG: Tentative de conversion initiale pour Against Sovereignty in Cyberspace.pdf...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Recognizing Layout: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23/23 [00:12<00:00,  1.82it/s]\n",
            "Running OCR Error Detection: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 22.53it/s]\n",
            "Detecting bboxes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.59it/s]\n",
            "Recognizing Text: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 41/41 [01:19<00:00,  1.95s/it]\n",
            "Recognizing tables: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.93it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   DEBUG: Premi√®re conversion r√©ussie.\n",
            "‚úÖ Conversion r√©ussie en 104.1s! \n",
            "   üìÇ Dossier: /content/drive/MyDrive/G√©opolitique et Souverainet√© Num√©riques/ALL/ALLMD/Against Sovereignty in Cyberspace\n",
            "   üìÑ Markdown: /content/drive/MyDrive/G√©opolitique et Souverainet√© Num√©riques/ALL/ALLMD/Against Sovereignty in Cyberspace/Against Sovereignty in Cyberspace.md\n",
            "   üñºÔ∏è Figures: 0\n",
            "   üìö R√©f√©rences: 0\n"
          ]
        }
      ],
      "source": [
        "pdf_files = sorted([p for p in INPUT_DIR.iterdir() if p.suffix.lower() == \".pdf\"])\n",
        "\n",
        "if pdf_files:\n",
        "    sample_path = pdf_files[0]\n",
        "    sample_name = sample_path.stem\n",
        "    print(f\"üß™ Test sur: {sample_path.name}\")\n",
        "    print(\"‚è≥ Conversion en cours...\\n\")\n",
        "\n",
        "    test_start = time.time()\n",
        "    sample_result = convert_pdf_complete(sample_path, sample_name)\n",
        "    test_elapsed = time.time() - test_start\n",
        "\n",
        "    if sample_result.get(\"error\"):\n",
        "        print(f\"\\n‚ùå Erreur: {sample_result['error']}\")\n",
        "    else:\n",
        "        print(f\"\\n\" + \"=\"*60)\n",
        "        print(\"‚úÖ CONVERSION R√âUSSIE!\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"‚è±Ô∏è Dur√©e: {test_elapsed:.1f}s\")\n",
        "        print(f\"\\nüìÇ Dossier de sortie:\")\n",
        "        print(f\"   {sample_result['doc_folder']}\")\n",
        "\n",
        "        print(f\"\\nüìÑ MARKDOWN:\")\n",
        "        print(f\"   Fichier: {sample_result['markdown_path']}\")\n",
        "        # Afficher un extrait du contenu\n",
        "        try:\n",
        "            with open(sample_result['markdown_path'], 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "            print(f\"   Taille: {len(content):,} caract√®res\")\n",
        "            print(f\"   Lignes: {content.count(chr(10)):,}\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        print(f\"\\nüñºÔ∏è FIGURES:\")\n",
        "        figures = sample_result.get('figures', [])\n",
        "        print(f\"   D√©tect√©es dans le texte: {len(figures)}\")\n",
        "        print(f\"   Images sauvegard√©es: {len(sample_result.get('figures_paths', []))}\")\n",
        "        if figures:\n",
        "            print(\"   Exemples:\")\n",
        "            for fig in figures[:5]:\n",
        "                fig_type = fig.get('type', 'unknown')\n",
        "                label = fig.get('label', '')[:60]\n",
        "                print(f\"      [{fig_type}] {label}\")\n",
        "            if len(figures) > 5:\n",
        "                print(f\"      ... et {len(figures)-5} autres\")\n",
        "\n",
        "        print(f\"\\nüìö R√âF√âRENCES:\")\n",
        "        refs = sample_result.get('references', {})\n",
        "        ref_count = refs.get('reference_count', 0)\n",
        "        print(f\"   Nombre: {ref_count}\")\n",
        "        if ref_count > 0:\n",
        "            print(\"   Exemples:\")\n",
        "            for ref in refs.get('references_list', [])[:3]:\n",
        "                print(f\"      ‚Ä¢ {ref[:80]}...\")\n",
        "            if ref_count > 3:\n",
        "                print(f\"      ... et {ref_count-3} autres\")\n",
        "        elif refs.get('references_text'):\n",
        "            print(f\"   ‚ö†Ô∏è Section trouv√©e mais parsing √©chou√©\")\n",
        "            print(f\"   Texte brut ({len(refs['references_text'])} chars):\")\n",
        "            print(f\"      {refs['references_text'][:200]}...\")\n",
        "\n",
        "        print(f\"\\nüìã M√âTADONN√âES:\")\n",
        "        meta = sample_result.get('metadata', {})\n",
        "        if meta.get('title'):\n",
        "            print(f\"   Titre: {meta['title'][:80]}\")\n",
        "        if meta.get('abstract'):\n",
        "            print(f\"   Abstract: {meta['abstract'][:100]}...\")\n",
        "        if meta.get('keywords'):\n",
        "            print(f\"   Keywords: {', '.join(meta['keywords'][:5])}\")\n",
        "        if meta.get('date'):\n",
        "            print(f\"   Date: {meta['date']}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "        # === DIAGNOSTIC AVANC√â si pas de figures/refs ===\n",
        "        if len(figures) == 0 or ref_count == 0:\n",
        "            print(\"\\nüîç DIAGNOSTIC (extraction insuffisante):\")\n",
        "            try:\n",
        "                with open(sample_result['markdown_path'], 'r', encoding='utf-8') as f:\n",
        "                    md_content = f.read()\n",
        "\n",
        "                # Chercher des patterns de figures\n",
        "                fig_matches = re.findall(r\"(?i)(figure|fig\\.?|table|tableau)\\s*\\d\", md_content)\n",
        "                print(f\"   Mentions 'figure/table' trouv√©es: {len(fig_matches)}\")\n",
        "                if fig_matches:\n",
        "                    print(f\"      Exemples: {fig_matches[:5]}\")\n",
        "\n",
        "                # Chercher des images markdown\n",
        "                img_matches = re.findall(r\"!\\[[^\\]]*\\]\\([^)]+\\)\", md_content)\n",
        "                print(f\"   Images markdown (![]()): {len(img_matches)}\")\n",
        "\n",
        "                # Chercher des sections de r√©f√©rences\n",
        "                ref_sections = re.findall(r\"(?i)(references?|bibliography|bibliographie)\", md_content)\n",
        "                print(f\"   Mentions 'references/bibliography': {len(ref_sections)}\")\n",
        "\n",
        "                # Chercher des citations num√©rot√©es\n",
        "                citations = re.findall(r\"\\[\\d+\\]\", md_content)\n",
        "                print(f\"   Citations num√©riques [n]: {len(citations)}\")\n",
        "\n",
        "                # Afficher la fin du document (souvent les r√©f√©rences)\n",
        "                print(f\"\\n   üìú Fin du document (2000 derniers chars):\")\n",
        "                print(\"   \" + \"-\"*50)\n",
        "                print(md_content[-2000:].replace('\\n', '\\n   '))\n",
        "            except Exception as e:\n",
        "                print(f\"   Erreur diagnostic: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå Aucun PDF trouv√© dans le dossier d'entr√©e.\")\n",
        "    print(f\"   V√©rifiez le chemin: {INPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e110727a",
      "metadata": {
        "id": "e110727a"
      },
      "source": [
        "## √âtape 9 ‚Äî Pipeline complet avec reprise\n",
        "Traitement batch + logs + reprise automatique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acc4b352",
      "metadata": {
        "id": "acc4b352"
      },
      "outputs": [],
      "source": [
        "import time as time_module\n",
        "\n",
        "def process_all_documents():\n",
        "    \"\"\"Traite tous les documents PDF avec reprise automatique.\"\"\"\n",
        "\n",
        "    all_pdfs = sorted([p for p in INPUT_DIR.iterdir() if p.suffix.lower() == \".pdf\"])\n",
        "\n",
        "    if not all_pdfs:\n",
        "        print(\"‚ùå Aucun PDF trouv√© dans le dossier d'entr√©e! \")\n",
        "        print(f\"   Chemin v√©rifi√©: {INPUT_DIR}\")\n",
        "        return [], []\n",
        "\n",
        "    log_file = GLOBAL_LOGS_DIR / f\"processing_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
        "    progress_file = GLOBAL_LOGS_DIR / \"progress.json\"\n",
        "\n",
        "    def log_message(message):\n",
        "        print(message)\n",
        "        with open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(message + \"\\n\")\n",
        "\n",
        "    processed_files = set()\n",
        "    if progress_file.exists():\n",
        "        try:\n",
        "            with open(progress_file, \"r\", encoding=\"utf-8\") as f:\n",
        "                progress_data = json.load(f)\n",
        "                processed_files = set(progress_data.get(\"processed\", []))\n",
        "            log_message(f\"üìÇ Reprise:  {len(processed_files)} fichiers d√©j√† trait√©s\")\n",
        "        except Exception:\n",
        "            processed_files = set()\n",
        "\n",
        "    remaining_pdfs = [p for p in all_pdfs if p. name not in processed_files]\n",
        "\n",
        "    log_message(\"=\"*60)\n",
        "    log_message(f\"üìÑ Total PDFs: {len(all_pdfs)}\")\n",
        "    log_message(f\"‚úÖ D√©j√† trait√©s: {len(processed_files)}\")\n",
        "    log_message(f\"‚è≥ Restants: {len(remaining_pdfs)}\")\n",
        "    log_message(\"=\"*60)\n",
        "\n",
        "    if not remaining_pdfs:\n",
        "        log_message(\"‚úÖ Tous les fichiers ont d√©j√† √©t√© trait√©s!\")\n",
        "        return [], []\n",
        "\n",
        "    results = []\n",
        "    errors = []\n",
        "    start_time = time_module.time()\n",
        "\n",
        "    def save_progress():\n",
        "        with open(progress_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump({\n",
        "                \"processed\": list(processed_files),\n",
        "                \"last_update\": datetime.now().isoformat()\n",
        "            }, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    for idx, pdf_path in enumerate(remaining_pdfs, 1):\n",
        "        doc_name = pdf_path.stem\n",
        "        doc_start_time = time_module.time()\n",
        "\n",
        "        log_message(f\"\\n[{idx}/{len(remaining_pdfs)}] üöÄ Traitement:  {pdf_path.name}\")\n",
        "\n",
        "        conversion_result = convert_pdf_complete(pdf_path, doc_name)\n",
        "\n",
        "        if conversion_result.get(\"error\"):\n",
        "            errors. append({\"file\": pdf_path. name, \"error\": conversion_result[\"error\"]})\n",
        "            log_message(f\"   ‚ùå Erreur conversion: {conversion_result['error']}\")\n",
        "            processed_files.add(pdf_path.name)\n",
        "            save_progress()\n",
        "            continue\n",
        "\n",
        "        doc_folders = get_doc_folders(doc_name)\n",
        "\n",
        "        log_message(\n",
        "            f\"   üñºÔ∏è Figures: {len(conversion_result['figures'])} trouv√©es, \"\n",
        "            f\"{len(conversion_result['figures_paths'])} sauvegard√©es\"\n",
        "        )\n",
        "        log_message(\n",
        "            f\"   üìö R√©f√©rences: {conversion_result['references'].get('reference_count', 0)} extraites\"\n",
        "        )\n",
        "\n",
        "        extraction = None\n",
        "        if USE_LANGEXTRACT:\n",
        "            log_message(\"   üîç Extraction LangExtract... \")\n",
        "            with open(conversion_result['markdown_path'], 'r', encoding='utf-8') as f:\n",
        "                md_content = f.read()\n",
        "            extraction = extract_with_langextract(\n",
        "                md_content,\n",
        "                doc_name,\n",
        "                doc_folders,\n",
        "                conversion_result['references'],\n",
        "                conversion_result['figures'],\n",
        "            )\n",
        "            if extraction. get(\"status\") == \"error\":\n",
        "                log_message(f\"   ‚ö†Ô∏è LangExtract: {extraction. get('error', 'Erreur inconnue')}\")\n",
        "            else:\n",
        "                log_message(\"   ‚úÖ LangExtract termin√©\")\n",
        "\n",
        "        analysis = {\n",
        "            \"doc_name\": doc_name,\n",
        "            \"source_pdf\": str(pdf_path),\n",
        "            \"doc_folder\": str(doc_folders[\"root\"]),\n",
        "            \"processed_at\": datetime.now().isoformat(),\n",
        "            \"processing_time_seconds\": round(time_module.time() - doc_start_time, 2),\n",
        "            \"conversion\": {\n",
        "                \"markdown_path\": conversion_result['markdown_path'],\n",
        "                \"figures_count\": len(conversion_result['figures']),\n",
        "                \"figures_saved\": len(conversion_result['figures_paths']),\n",
        "                \"figures_paths\": conversion_result['figures_paths'],\n",
        "                \"references_count\": conversion_result['references'].get('reference_count', 0),\n",
        "            },\n",
        "            \"references\": conversion_result['references'],\n",
        "            \"figures\": conversion_result['figures'],\n",
        "            \"langextract\": extraction,\n",
        "        }\n",
        "\n",
        "        analysis_path = doc_folders[\"analyses\"] / f\"{doc_name}_analysis.json\"\n",
        "        with open(analysis_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(analysis, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        doc_log_path = doc_folders[\"logs\"] / \"processing. log\"\n",
        "        with open(doc_log_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f. write(f\"Trait√© le: {datetime.now().isoformat()}\\n\")\n",
        "            f.write(f\"Dur√©e: {analysis['processing_time_seconds']} secondes\\n\")\n",
        "            f.write(f\"Figures:  {len(conversion_result['figures_paths'])}\\n\")\n",
        "            f.write(f\"R√©f√©rences: {conversion_result['references'].get('reference_count', 0)}\\n\")\n",
        "\n",
        "        results.append(analysis)\n",
        "        processed_files.add(pdf_path.name)\n",
        "        save_progress()\n",
        "\n",
        "        elapsed = time_module.time() - doc_start_time\n",
        "        log_message(f\"   ‚è±Ô∏è Dur√©e: {elapsed:.1f}s\")\n",
        "\n",
        "    total_time = time_module.time() - start_time\n",
        "    log_message(\"\\n\" + \"=\"*60)\n",
        "    log_message(f\"‚úÖ Traitement termin√© en {total_time:.1f} secondes\")\n",
        "    log_message(f\"   üìÑ R√©ussis: {len(results)}\")\n",
        "    log_message(f\"   ‚ùå Erreurs: {len(errors)}\")\n",
        "    log_message(\"=\"*60)\n",
        "\n",
        "    return results, errors\n",
        "\n",
        "\n",
        "def export_all_results(results):\n",
        "    \"\"\"Exporte les r√©sultats globaux.\"\"\"\n",
        "    if not results:\n",
        "        print(\"‚ÑπÔ∏è Aucun r√©sultat √† exporter. \")\n",
        "        return\n",
        "\n",
        "    summary_path = GLOBAL_LOGS_DIR / \"_SUMMARY.json\"\n",
        "    report_md_path = GLOBAL_LOGS_DIR / \"_REPORT. md\"\n",
        "\n",
        "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    all_references = []\n",
        "    for data in results:\n",
        "        refs = data.get(\"references\", {})\n",
        "        if refs. get(\"references_list\"):\n",
        "            for ref in refs[\"references_list\"]:\n",
        "                all_references.append({\n",
        "                    \"document\": data. get(\"doc_name\", \"\"),\n",
        "                    \"reference\": ref,\n",
        "                })\n",
        "\n",
        "    if all_references:\n",
        "        biblio_path = GLOBAL_LOGS_DIR / \"_BIBLIOGRAPHIE_COMPLETE.json\"\n",
        "        with open(biblio_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json. dump(all_references, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"üìö Bibliographie:  {biblio_path} ({len(all_references)} r√©f√©rences)\")\n",
        "\n",
        "    all_figures = []\n",
        "    for data in results:\n",
        "        for fig_path in data.get(\"conversion\", {}).get(\"figures_paths\", []):\n",
        "            all_figures.append({\n",
        "                \"document\": data. get(\"doc_name\", \"\"),\n",
        "                \"path\": fig_path,\n",
        "            })\n",
        "\n",
        "    if all_figures:\n",
        "        figures_index_path = GLOBAL_LOGS_DIR / \"_INDEX_FIGURES.json\"\n",
        "        with open(figures_index_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(all_figures, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"üñºÔ∏è Index figures: {figures_index_path} ({len(all_figures)} figures)\")\n",
        "\n",
        "    md_lines = []\n",
        "    md_lines.append(\"# Rapport de traitement MFEGSN\\n\\n\")\n",
        "    md_lines.append(f\"**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M')}\\n\\n\")\n",
        "    md_lines.append(f\"- **Total documents trait√©s:** {len(results)}\\n\")\n",
        "    md_lines.append(f\"- **Total figures extraites:** {len(all_figures)}\\n\")\n",
        "    md_lines.append(f\"- **Total r√©f√©rences:** {len(all_references)}\\n\\n\")\n",
        "    md_lines.append(\"## Documents trait√©s\\n\\n\")\n",
        "\n",
        "    for data in results:\n",
        "        conv = data.get(\"conversion\", {})\n",
        "        md_lines.append(f\"### {data. get('doc_name', '')}\\n\\n\")\n",
        "        md_lines.append(f\"- **Dossier:** `{data.get('doc_folder', '')}`\\n\")\n",
        "        md_lines.append(f\"- **Figures:** {conv.get('figures_count', 0)}\\n\")\n",
        "        md_lines. append(f\"- **R√©f√©rences:** {conv.get('references_count', 0)}\\n\")\n",
        "        md_lines.append(f\"- **Dur√©e:** {data.get('processing_time_seconds', 0)}s\\n\\n\")\n",
        "\n",
        "    with open(report_md_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\". join(md_lines))\n",
        "\n",
        "    print(f\"‚úÖ R√©sultats export√©s: \")\n",
        "    print(f\"   üìä Summary: {summary_path}\")\n",
        "    print(f\"   üìù Report: {report_md_path}\")\n",
        "\n",
        "print(\"‚úÖ Fonctions de pipeline d√©finies. \")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "830ddf11",
      "metadata": {
        "id": "830ddf11"
      },
      "source": [
        "## √âtape 10 ‚Äî Lancer le traitement\n",
        "Ex√©cutez cette cellule pour lancer le traitement complet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c0f8f92",
      "metadata": {
        "id": "4c0f8f92"
      },
      "outputs": [],
      "source": [
        "# Recharger la liste des PDFs depuis INPUT_DIR pour s'assurer qu'elle est √† jour\n",
        "print(\"üìÇ V√©rification du dossier d'entr√©e...\")\n",
        "print(f\"   Chemin: {INPUT_DIR}\")\n",
        "\n",
        "# Recharger la liste des fichiers\n",
        "pdf_files = sorted([p for p in INPUT_DIR.iterdir() if p.suffix.lower() == \".pdf\"])\n",
        "print(f\"   PDFs trouv√©s: {len(pdf_files)}\")\n",
        "\n",
        "if len(pdf_files) == 0:\n",
        "    print(\"\\n‚ö†Ô∏è ATTENTION: Aucun PDF trouv√©!\")\n",
        "    print(\"   V√©rifiez que:\")\n",
        "    print(\"   1. Le chemin INPUT_DIR est correct\")\n",
        "    print(\"   2. Des fichiers PDF sont pr√©sents dans ce dossier\")\n",
        "    print(\"   3. Google Drive est bien mont√©\")\n",
        "else:\n",
        "    for i, pdf in enumerate(pdf_files[:5], 1):\n",
        "        print(f\"   {i}. {pdf.name}\")\n",
        "    if len(pdf_files) > 5:\n",
        "        print(f\"   ... et {len(pdf_files) - 5} autres\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üöÄ D√©marrage du pipeline...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "results, errors = process_all_documents()\n",
        "\n",
        "if results:\n",
        "    export_all_results(results)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ PIPELINE TERMIN√â\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if errors:\n",
        "    print(f\"\\n‚ö†Ô∏è {len(errors)} erreur(s) rencontr√©e(s):\")\n",
        "    for err in errors[:5]:\n",
        "        print(f\"   - {err['file']}: {err['error'][:50]}...\")\n",
        "    if len(errors) > 5:\n",
        "        print(f\"   ... et {len(errors) - 5} autres erreurs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "utils001",
      "metadata": {
        "id": "utils001"
      },
      "source": [
        "## Utilitaires ‚Äî Gestion du cache\n",
        "Cellules optionnelles pour g√©rer le cache des mod√®les sur Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "utils002",
      "metadata": {
        "id": "utils002"
      },
      "outputs": [],
      "source": [
        "# === AFFICHER LA TAILLE DU CACHE ===\n",
        "def show_cache_info():\n",
        "    \"\"\"Affiche les informations sur le cache Drive.\"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"üíæ INFORMATIONS CACHE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for name, path in [(\"HuggingFace\", HF_CACHE_DRIVE),\n",
        "                       (\"Torch\", TORCH_CACHE_DRIVE),\n",
        "                       (\"Datalab\", DATALAB_CACHE_DRIVE)]:\n",
        "        size = get_dir_size(path)\n",
        "        print(f\"üìÇ {name}: {size:.2f} GB\")\n",
        "\n",
        "    total = get_dir_size(DRIVE_CACHE_DIR)\n",
        "    print(f\"\\nüì¶ Total: {total:.2f} GB\")\n",
        "    print(f\"üìç Emplacement: {DRIVE_CACHE_DIR}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# Appeler la fonction pour afficher les infos\n",
        "show_cache_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "utils003",
      "metadata": {
        "id": "utils003"
      },
      "outputs": [],
      "source": [
        "# === SUPPRIMER LE CACHE (si n√©cessaire) ===\n",
        "# ‚ö†Ô∏è ATTENTION:  Cela supprimera tous les mod√®les en cache!\n",
        "# D√©commentez les lignes ci-dessous pour ex√©cuter.\n",
        "\n",
        "# import shutil\n",
        "#\n",
        "# if DRIVE_CACHE_DIR. exists():\n",
        "#     print(f\"‚ö†Ô∏è Suppression du cache:  {DRIVE_CACHE_DIR}\")\n",
        "#     shutil.rmtree(DRIVE_CACHE_DIR)\n",
        "#     print(\"‚úÖ Cache supprim√©.  Les mod√®les seront ret√©l√©charg√©s au prochain lancement.\")\n",
        "# else:\n",
        "#     print(\"‚ÑπÔ∏è Aucun cache √† supprimer.\")\n",
        "\n",
        "print(\"‚ÑπÔ∏è D√©commentez le code ci-dessus pour supprimer le cache.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
