{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b54619af",
   "metadata": {},
   "source": [
    "# MFEGSN ‚Äî Pipeline Colab (Marker + LangExtract)\n",
    "\n",
    "Notebook optimis√© pour une ex√©cution **pas √† pas** sur Google Colab.\n",
    "\n",
    "**üöÄ Optimisation:** Les mod√®les (~4GB) sont sauvegard√©s sur Google Drive apr√®s le premier t√©l√©chargement.\n",
    "Les sessions suivantes chargeront les mod√®les depuis Drive en quelques secondes.\n",
    "\n",
    "**Ordre recommand√©:** √âtapes 1 ‚Üí 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014f8fe4",
   "metadata": {},
   "source": [
    "## √âtape 1 ‚Äî Monter Google Drive\n",
    "**IMPORTANT:** Ex√©cutez cette cellule EN PREMIER pour permettre le cache des mod√®les."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dce4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Cr√©er le dossier de cache sur Drive\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# === CONFIGURATION DU CACHE ===\n",
    "DRIVE_CACHE_DIR = Path(\"/content/drive/MyDrive/.mfegsn_cache\")\n",
    "DRIVE_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Sous-dossiers pour chaque type de cache\n",
    "HF_CACHE_DRIVE = DRIVE_CACHE_DIR / \"huggingface\"\n",
    "TORCH_CACHE_DRIVE = DRIVE_CACHE_DIR / \"torch\"\n",
    "DATALAB_CACHE_DRIVE = DRIVE_CACHE_DIR / \"datalab\"\n",
    "\n",
    "for cache_dir in [HF_CACHE_DRIVE, TORCH_CACHE_DRIVE, DATALAB_CACHE_DRIVE]:\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configurer les variables d'environnement pour utiliser le cache Drive\n",
    "os. environ[\"HF_HOME\"] = str(HF_CACHE_DRIVE)\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = str(HF_CACHE_DRIVE)\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = str(HF_CACHE_DRIVE)\n",
    "os.environ[\"TORCH_HOME\"] = str(TORCH_CACHE_DRIVE)\n",
    "os.environ[\"XDG_CACHE_HOME\"] = str(DRIVE_CACHE_DIR)\n",
    "\n",
    "# V√©rifier la taille du cache existant\n",
    "def get_dir_size(path):\n",
    "    total = 0\n",
    "    if path.exists():\n",
    "        for f in path.rglob(\"*\"):\n",
    "            if f.is_file():\n",
    "                total += f.stat().st_size\n",
    "    return total / 1e9\n",
    "\n",
    "cache_size = get_dir_size(DRIVE_CACHE_DIR)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ Google Drive mont√©! \")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìÇ Cache Drive:  {DRIVE_CACHE_DIR}\")\n",
    "print(f\"üíæ Taille du cache: {cache_size:.2f} GB\")\n",
    "if cache_size > 3:\n",
    "    print(\"üöÄ Mod√®les d√©j√† en cache!  Le chargement sera rapide.\")\n",
    "else:\n",
    "    print(\"üì• Premier lancement:  les mod√®les seront t√©l√©charg√©s et mis en cache.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c063508",
   "metadata": {},
   "source": [
    "## √âtape 2 ‚Äî Installer les d√©pendances\n",
    "Installation des packages Python n√©cessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7a1e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©pendances syst√®me\n",
    "!apt-get update -qq\n",
    "! apt-get install -y zstd -qq\n",
    "\n",
    "# D√©pendances Python\n",
    "!python -m pip install -q --upgrade pip\n",
    "!python -m pip install -q marker-pdf[full] langextract google-generativeai pillow\n",
    "\n",
    "# Installer hf_transfer pour des t√©l√©chargements plus rapides (sans casser les d√©pendances)\n",
    "!pip install -q hf_transfer --no-deps\n",
    "\n",
    "# Activer hf_transfer\n",
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "# V√©rifier les versions\n",
    "print(\"\\nüì¶ Versions install√©es:\")\n",
    "!pip show huggingface_hub 2>/dev/null | grep Version\n",
    "! pip show transformers 2>/dev/null | grep Version\n",
    "!pip show marker-pdf 2>/dev/null | grep Version\n",
    "\n",
    "print(\"\\n‚úÖ D√©pendances install√©es.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9950b81",
   "metadata": {},
   "source": [
    "## √âtape 3 ‚Äî Configurer les dossiers de travail\n",
    "Modifiez les chemins selon votre Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749433f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# === MODIFIEZ CES CHEMINS ===\n",
    "INPUT_DIR = Path(\"/content/drive/MyDrive/G√©opolitique et Souverainet√© Num√©riques/ALL/ALLPDF\")\n",
    "OUTPUT_DIR = Path(\"/content/drive/MyDrive/G√©opolitique et Souverainet√© Num√©riques/ALL/ALLMD\")\n",
    "\n",
    "assert INPUT_DIR.exists(), f\"‚ùå Dossier introuvable: {INPUT_DIR}\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dossier global pour les logs du pipeline\n",
    "GLOBAL_LOGS_DIR = OUTPUT_DIR / \"_PIPELINE_LOGS\"\n",
    "GLOBAL_LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def get_doc_folders(doc_name):\n",
    "    \"\"\"Cr√©e et retourne les dossiers pour un document donn√©.\"\"\"\n",
    "    doc_dir = OUTPUT_DIR / doc_name\n",
    "    doc_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    folders = {\n",
    "        \"root\": doc_dir,\n",
    "        \"figures\": doc_dir / \"_FIGURES\",\n",
    "        \"references\": doc_dir / \"_REFERENCES\",\n",
    "        \"analyses\": doc_dir / \"_ANALYSES\",\n",
    "        \"logs\": doc_dir / \"_LOGS\",\n",
    "    }\n",
    "    \n",
    "    for folder in folders.values():\n",
    "        folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    return folders\n",
    "\n",
    "pdf_files = sorted([p for p in INPUT_DIR.iterdir() if p.suffix.lower() == \".pdf\"])\n",
    "print(\"=\"*60)\n",
    "print(\"üìÅ CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìÇ Entr√©e: {INPUT_DIR}\")\n",
    "print(f\"üìÇ Sortie: {OUTPUT_DIR}\")\n",
    "print(f\"üìÑ PDFs: {len(pdf_files)}\")\n",
    "print(f\"üìÇ Structure par fichier:  <nom_fichier>/\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ _ANALYSES/\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ _FIGURES/\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ _LOGS/\")\n",
    "print(f\"   ‚îî‚îÄ‚îÄ _REFERENCES/\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5c801b",
   "metadata": {},
   "source": [
    "## √âtape 4 ‚Äî Ollama + Gemma 3 4B\n",
    "Installation et d√©marrage de Gemma 3 4B via Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352143f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_OLLAMA = True  # Activ√© par d√©faut pour Gemma local\n",
    "\n",
    "ollama_process = None\n",
    "\n",
    "if USE_OLLAMA:\n",
    "    import subprocess\n",
    "    import time\n",
    "    import os\n",
    "    \n",
    "    # Installer requests si n√©cessaire\n",
    "    try:\n",
    "        import requests\n",
    "    except ImportError:\n",
    "        !pip install -q requests\n",
    "        import requests\n",
    "\n",
    "    # Installer Ollama\n",
    "    print(\"üì¶ Installation d'Ollama...\")\n",
    "    !curl -fsSL https://ollama.com/install.sh 2>/dev/null | sh 2>&1 | tail -n 3\n",
    "\n",
    "    # D√©marrer Ollama en arri√®re-plan\n",
    "    ollama_process = subprocess.Popen(\n",
    "        [\"ollama\", \"serve\"],\n",
    "        stdout=subprocess.DEVNULL,\n",
    "        stderr=subprocess.DEVNULL\n",
    "    )\n",
    "    print(\"‚è≥ D√©marrage du serveur Ollama...\")\n",
    "    \n",
    "    # Attendre que le serveur soit pr√™t (avec timeout de 30 secondes)\n",
    "    server_ready = False\n",
    "    for i in range(30):\n",
    "        try:\n",
    "            response = requests.get(\"http://localhost:11434/api/tags\", timeout=2)\n",
    "            if response.status_code == 200:\n",
    "                print(\"‚úÖ Serveur Ollama pr√™t!\")\n",
    "                server_ready = True\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "        time.sleep(1)\n",
    "    \n",
    "    if not server_ready:\n",
    "        print(\"‚ö†Ô∏è Le serveur Ollama n'a pas d√©marr√© √† temps\")\n",
    "        print(\"   Essayez de r√©ex√©cuter cette cellule.\")\n",
    "\n",
    "    # T√©l√©charger Gemma 3 4B (SANS espace dans le nom!)\n",
    "    print(\"\\nüì• T√©l√©chargement de Gemma 3 4B (‚âà3GB)...\")\n",
    "    print(\"   Cela peut prendre plusieurs minutes...\")\n",
    "    !ollama pull gemma3:4b\n",
    "    \n",
    "    # V√©rification\n",
    "    result = subprocess.run([\"ollama\", \"list\"], capture_output=True, text=True)\n",
    "    if \"gemma3:4b\" in result.stdout or \"gemma3\" in result.stdout:\n",
    "        print(\"\\n‚úÖ Gemma 3 4B t√©l√©charg√©!\")\n",
    "        \n",
    "        # Pr√©chauffer le mod√®le avec une requ√™te test\n",
    "        print(\"üî• Pr√©chauffage du mod√®le...\")\n",
    "        test_result = subprocess.run(\n",
    "            [\"ollama\", \"run\", \"gemma3:4b\", \"R√©ponds uniquement: OK\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if test_result.returncode == 0:\n",
    "            print(\"‚úÖ Gemma 3 4B pr√™t et op√©rationnel!\")\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"ü§ñ OLLAMA CONFIGUR√â\")\n",
    "            print(\"=\"*60)\n",
    "            print(\"Mod√®le: gemma3:4b\")\n",
    "            print(\"Serveur: http://localhost:11434\")\n",
    "            print(\"=\"*60)\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Avertissement: Le test du mod√®le a √©chou√©\")\n",
    "            print(\"   Le mod√®le devrait quand m√™me fonctionner.\")\n",
    "    else:\n",
    "        print(\"‚ùå Erreur: Gemma 3 4B non trouv√©\")\n",
    "        print(\"Mod√®les disponibles:\")\n",
    "        print(result.stdout if result.stdout else \"Aucun mod√®le\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Ollama d√©sactiv√©. Mettez USE_OLLAMA = True pour l'activer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ae817d",
   "metadata": {},
   "source": [
    "## √âtape 5 ‚Äî Configurer Marker\n",
    "Chargement des mod√®les Marker (~4GB) depuis le cache Drive.\n",
    "\n",
    "**‚ö° Premier lancement:** 10-20 minutes (t√©l√©chargement + sauvegarde sur Drive)\n",
    "\n",
    "**üöÄ Lancements suivants:** 1-2 minutes (chargement depuis Drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0f4c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import shutil\n",
    "import base64\n",
    "import logging\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# === CONFIGURATION DES LOGS ===\n",
    "# D√©sactiver les warnings non critiques\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Configurer le logging pour r√©duire la verbosit√©\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "# R√©duire drastiquement les logs des biblioth√®ques tierces\n",
    "for logger_name in ['transformers', 'torch', 'PIL', 'urllib3', 'filelock', \n",
    "                     'huggingface_hub', 'marker', 'datasets']:\n",
    "    logging.getLogger(logger_name).setLevel(logging.ERROR)\n",
    "\n",
    "# Variables d'environnement pour r√©duire les logs\n",
    "os.environ['TRANSFORMERS_VERBOSITY'] = 'error'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '0'  # Garder les barres de progression\n",
    "\n",
    "# S'assurer que le cache Drive est bien configur√©\n",
    "os.environ[\"HF_HOME\"] = str(HF_CACHE_DRIVE)\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = str(HF_CACHE_DRIVE)\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = str(HF_CACHE_DRIVE)\n",
    "os.environ[\"TORCH_HOME\"] = str(TORCH_CACHE_DRIVE)\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "# === FORCER LE T√âL√âCHARGEMENT DES MOD√àLES SURYA DEPUIS HUGGING FACE ===\n",
    "# R√©sout les erreurs de connexion aux serveurs Datalab\n",
    "# Les mod√®les sont h√©berg√©s officiellement par Vik Paruchuri sur HF\n",
    "\n",
    "# Mod√®les Surya √† t√©l√©charger depuis Hugging Face\n",
    "os.environ[\"SURYA_LAYOUT_MODEL\"] = \"vikp/surya_layout\"\n",
    "os.environ[\"SURYA_REC_MODEL\"] = \"vikp/surya_rec\"\n",
    "os.environ[\"SURYA_DET_MODEL\"] = \"vikp/surya_det\"\n",
    "os.environ[\"SURYA_ORDER_MODEL\"] = \"vikp/surya_order\"\n",
    "os.environ[\"SURYA_TABLE_REC_MODEL\"] = \"vikp/surya_tablerec\"\n",
    "\n",
    "# Alternative: forcer l'utilisation de huggingface_hub pour tous les t√©l√©chargements\n",
    "os.environ[\"SURYA_DOWNLOAD_BACKEND\"] = \"huggingface\"\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"=\"*60)\n",
    "print(\"üñ•Ô∏è CONFIGURATION MAT√âRIELLE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# V√©rifier le cache existant\n",
    "def get_dir_size(path):\n",
    "    total = 0\n",
    "    if path.exists():\n",
    "        for f in path.rglob(\"*\"):\n",
    "            if f.is_file():\n",
    "                try:\n",
    "                    total += f.stat().st_size\n",
    "                except:\n",
    "                    pass\n",
    "    return total / 1e9\n",
    "\n",
    "initial_cache_size = get_dir_size(DRIVE_CACHE_DIR)\n",
    "print(f\"\\nüíæ Cache Drive: {initial_cache_size:.2f} GB\")\n",
    "\n",
    "if initial_cache_size > 3:\n",
    "    print(\"üöÄ Mod√®les trouv√©s en cache! Chargement rapide...\")\n",
    "else:\n",
    "    print(\"üì• Premier t√©l√©chargement des mod√®les Marker (~4GB)\")\n",
    "    print(\"   ‚è±Ô∏è Dur√©e estim√©e: 10-20 minutes\")\n",
    "    print(\"   üíæ Les mod√®les seront sauvegard√©s sur Drive\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Chronom√®tre\n",
    "start_time = time.time()\n",
    "\n",
    "# Charger les mod√®les avec messages de progression\n",
    "print(\"\\nüì¶ Chargement des mod√®les Marker...\")\n",
    "if initial_cache_size > 3:\n",
    "    print(\"   ‚ö° Chargement depuis le cache Drive...\")\n",
    "else:\n",
    "    print(\"   üì• T√©l√©chargement en cours (barres de progression ci-dessous)...\")\n",
    "\n",
    "from marker.converters.pdf import PdfConverter\n",
    "from marker.models import create_model_dict\n",
    "from marker.config.parser import ConfigParser\n",
    "\n",
    "# Configuration Marker avec Gemma local\n",
    "marker_config = {\n",
    "    \"workers\": 2,\n",
    "    \"extract_images\": True,\n",
    "    \"images_as_base64\": False,\n",
    "    \"use_llm\": USE_OLLAMA,\n",
    "    \"llm_provider\": \"ollama\" if USE_OLLAMA else None,\n",
    "    \"llm_model\": \"gemma3:4b\" if USE_OLLAMA else None,\n",
    "    \"force_ocr\": False,\n",
    "    \"languages\": [\"fr\", \"en\"],\n",
    "    \"paginate_output\": True,\n",
    "    \"batch_size\": 4 if device == \"cuda\" else 2,\n",
    "}\n",
    "\n",
    "print(\"\\n‚öôÔ∏è Cr√©ation du dictionnaire de mod√®les...\")\n",
    "model_dict = create_model_dict()\n",
    "\n",
    "print(\"‚úì Configuration du convertisseur...\")\n",
    "config_parser = ConfigParser(marker_config)\n",
    "converter = PdfConverter(\n",
    "    config=config_parser.generate_config_dict(),\n",
    "    artifact_dict=model_dict,\n",
    ")\n",
    "\n",
    "# R√©sum√©\n",
    "elapsed = time.time() - start_time\n",
    "final_cache_size = get_dir_size(DRIVE_CACHE_DIR)\n",
    "downloaded = final_cache_size - initial_cache_size\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ MARKER CONFIGUR√â AVEC SUCC√àS!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚è±Ô∏è Dur√©e: {elapsed/60:.1f} minutes\")\n",
    "print(f\"üì¶ T√©l√©charg√© cette session: {max(0, downloaded):.2f} GB\")\n",
    "print(f\"üíæ Cache total sur Drive: {final_cache_size:.2f} GB\")\n",
    "print(f\"‚öôÔ∏è Workers: {marker_config['workers']}, Batch: {marker_config['batch_size']}\")\n",
    "if USE_OLLAMA:\n",
    "    print(f\"ü§ñ LLM: Gemma 3 4B via Ollama\")\n",
    "print(\"=\"*60)\n",
    "if downloaded > 0.5:\n",
    "    print(\"\\nüí° Les mod√®les sont maintenant en cache sur votre Drive.\")\n",
    "    print(\"   Les prochaines sessions chargeront en 1-2 minutes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0fc302",
   "metadata": {},
   "source": [
    "## √âtape 6 ‚Äî Fonctions utilitaires\n",
    "Extraction des r√©f√©rences, figures et conversion PDF ‚Üí Markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a23cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_references_from_markdown(markdown_text):\n",
    "    \"\"\"Extrait la section r√©f√©rences/bibliographie du Markdown.\"\"\"\n",
    "    references = {\n",
    "        \"references_text\": \"\",\n",
    "        \"references_list\": [],\n",
    "        \"reference_count\": 0,\n",
    "    }\n",
    "\n",
    "    ref_patterns = [\n",
    "        r\"(? i)(? :^|\\n)#{1,3}\\s*(references|r√©f√©rences|bibliography|bibliographie|works\\s*cited|sources?)\\s*[\\s: ]*\\n([\\s\\S]*?)(? =\\n#{1,3}\\s|\\Z)\",\n",
    "        r\"(?i)(?:^|\\n)\\*\\*(references|r√©f√©rences|bibliography|bibliographie)\\*\\*\\s*[\\s: ]*\\n([\\s\\S]*?)(?=\\n\\*\\*|\\n#{1,3}|\\Z)\",\n",
    "    ]\n",
    "\n",
    "    for pattern in ref_patterns:\n",
    "        match = re.search(pattern, markdown_text, re.MULTILINE)\n",
    "        if match:\n",
    "            ref_section = match.group(2).strip()\n",
    "            references[\"references_text\"] = ref_section\n",
    "\n",
    "            ref_lines = []\n",
    "            lines = ref_section.split(\"\\n\")\n",
    "            current_ref = \"\"\n",
    "\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    if current_ref:\n",
    "                        ref_lines.append(current_ref. strip())\n",
    "                        current_ref = \"\"\n",
    "                    continue\n",
    "\n",
    "                if re.match(r\"^(\\[\\d+\\]|\\d+\\.|[-‚Ä¢]|\\([A-Z])\", line):\n",
    "                    if current_ref:\n",
    "                        ref_lines.append(current_ref.strip())\n",
    "                    current_ref = line\n",
    "                else:\n",
    "                    current_ref += \" \" + line\n",
    "\n",
    "            if current_ref:\n",
    "                ref_lines. append(current_ref.strip())\n",
    "\n",
    "            references[\"references_list\"] = [r for r in ref_lines if len(r) > 20]\n",
    "            references[\"reference_count\"] = len(references[\"references_list\"])\n",
    "            break\n",
    "\n",
    "    return references\n",
    "\n",
    "\n",
    "def extract_figures_info(markdown_text, images_dict):\n",
    "    \"\"\"Extrait les informations sur les figures du document.\"\"\"\n",
    "    figures = []\n",
    "    fig_pattern = r\"(?i)(figure|fig\\.)\\s*(\\d+)?\\s*[:]? \\s*(. {0,120})\"\n",
    "\n",
    "    for match in re.finditer(fig_pattern, markdown_text):\n",
    "        title = (match.group(3) or \"\").strip()\n",
    "        figures.append({\n",
    "            \"label\": match.group(0).strip(),\n",
    "            \"title\": title,\n",
    "        })\n",
    "\n",
    "    if images_dict:\n",
    "        for img_name in images_dict. keys():\n",
    "            existing = any(f.get(\"path\") == img_name for f in figures)\n",
    "            if not existing:\n",
    "                figures.append({\n",
    "                    \"label\": str(img_name),\n",
    "                    \"title\": \"\",\n",
    "                    \"path\": str(img_name),\n",
    "                })\n",
    "\n",
    "    return figures\n",
    "\n",
    "\n",
    "def save_figures(images_dict, doc_folders):\n",
    "    \"\"\"Sauvegarde les figures extraites dans le dossier _FIGURES du document.\"\"\"\n",
    "    if not images_dict:\n",
    "        return []\n",
    "\n",
    "    figures_folder = doc_folders[\"figures\"]\n",
    "    saved_paths = []\n",
    "\n",
    "    for img_name, img_data in images_dict.items():\n",
    "        safe_name = re.sub(r\"[^a-zA-Z0-9_-]+\", \"_\", str(img_name))\n",
    "        img_path = figures_folder / f\"{safe_name}.png\"\n",
    "\n",
    "        try:\n",
    "            if isinstance(img_data, Image.Image):\n",
    "                img_data.save(img_path)\n",
    "            elif isinstance(img_data, (bytes, bytearray)):\n",
    "                with open(img_path, \"wb\") as f:\n",
    "                    f.write(img_data)\n",
    "            elif isinstance(img_data, str):\n",
    "                if img_data.startswith(\"data: image\"):\n",
    "                    b64_data = img_data.split(\",\", 1)[1]\n",
    "                    with open(img_path, \"wb\") as f:\n",
    "                        f.write(base64.b64decode(b64_data))\n",
    "                elif Path(img_data).exists():\n",
    "                    shutil.copy(img_data, img_path)\n",
    "                else:\n",
    "                    with open(img_path, \"wb\") as f:\n",
    "                        f.write(base64.b64decode(img_data))\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            saved_paths. append(str(img_path))\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return saved_paths\n",
    "\n",
    "\n",
    "def convert_pdf_complete(pdf_path, doc_name):\n",
    "    \"\"\"Conversion compl√®te d'un PDF avec extraction figures et r√©f√©rences.\"\"\"\n",
    "    doc_folders = get_doc_folders(doc_name)\n",
    "    \n",
    "    result_data = {\n",
    "        \"doc_name\": doc_name,\n",
    "        \"doc_folder\": str(doc_folders[\"root\"]),\n",
    "        \"markdown_path\": \"\",\n",
    "        \"figures\": [],\n",
    "        \"figures_paths\": [],\n",
    "        \"references\": {},\n",
    "        \"error\": None,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        result = converter(str(pdf_path))\n",
    "        markdown_text = getattr(result, \"markdown\", \"\") or \"\"\n",
    "        images_dict = getattr(result, \"images\", {}) or {}\n",
    "\n",
    "        md_path = doc_folders[\"root\"] / f\"{doc_name}.md\"\n",
    "        with open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(markdown_text)\n",
    "\n",
    "        result_data[\"markdown_path\"] = str(md_path)\n",
    "        result_data[\"figures_paths\"] = save_figures(images_dict, doc_folders)\n",
    "        result_data[\"figures\"] = extract_figures_info(markdown_text, images_dict)\n",
    "\n",
    "        result_data[\"references\"] = extract_references_from_markdown(markdown_text)\n",
    "        if result_data[\"references\"]. get(\"reference_count\", 0) > 0:\n",
    "            ref_path = doc_folders[\"references\"] / f\"{doc_name}_references.json\"\n",
    "            with open(ref_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(result_data[\"references\"], f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        return result_data\n",
    "    except Exception as e:\n",
    "        result_data[\"error\"] = str(e)\n",
    "        error_log = doc_folders[\"logs\"] / \"error. txt\"\n",
    "        with open(error_log, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"Erreur:  {str(e)}\\nDate: {datetime.now().isoformat()}\")\n",
    "        return result_data\n",
    "\n",
    "print(\"‚úÖ Fonctions utilitaires d√©finies. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f235df66",
   "metadata": {},
   "source": [
    "## √âtape 7 ‚Äî LangExtract\n",
    "Extraction structur√©e avec LangExtract (activ√© par d√©faut avec Ollama)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4261fafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_LANGEXTRACT = True\n",
    "\n",
    "LANGEXTRACT_CONFIG = {\n",
    "    \"provider\": \"ollama\" if USE_OLLAMA else \"openai\",\n",
    "    \"model\": \"gemma3:4b\" if USE_OLLAMA else \"gpt-3.5-turbo\",\n",
    "    \"base_url\": \"http://localhost:11434\" if USE_OLLAMA else None,\n",
    "}\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"Vous √™tes un assistant d'analyse pour des documents en sciences sociales.\n",
    "Retournez un JSON structur√© avec les sections suivantes:\n",
    "\n",
    "1.  CONTEXTE\n",
    "- Th√®me principal\n",
    "- Zone g√©ographique\n",
    "- P√©riode\n",
    "\n",
    "2. ACTEURS\n",
    "- Institutions\n",
    "- Pays\n",
    "- Organisations\n",
    "\n",
    "3. CONCEPTS CL√âS\n",
    "- Mots-cl√©s\n",
    "- Concepts\n",
    "\n",
    "4. DONN√âES\n",
    "- Chiffres cl√©s (si disponibles)\n",
    "\n",
    "5. R√âF√âRENCES\n",
    "- Principales r√©f√©rences cit√©es\n",
    "\n",
    "6. FIGURES ET TABLEAUX\n",
    "- Liste des figures mentionn√©es\n",
    "\n",
    "R√©pondez uniquement avec un JSON valide.\"\"\"\n",
    "\n",
    "def _safe_json(obj):\n",
    "    try:\n",
    "        return json.loads(json.dumps(obj))\n",
    "    except Exception:\n",
    "        return {\"raw\": str(obj)}\n",
    "\n",
    "\n",
    "def extract_with_langextract(markdown_text, doc_name, doc_folders, references_data=None, figures_data=None):\n",
    "    \"\"\"Extraction structur√©e avec LangExtract.\"\"\"\n",
    "    if not USE_LANGEXTRACT:\n",
    "        return {\"status\": \"skipped\", \"reason\": \"USE_LANGEXTRACT=False\"}\n",
    "\n",
    "    enriched_text = markdown_text\n",
    "\n",
    "    if references_data and references_data.get(\"reference_count\", 0) > 0:\n",
    "        enriched_text += \"\\n\\n## R√âF√âRENCES\\n\"\n",
    "        enriched_text += f\"Nombre de r√©f√©rences: {references_data['reference_count']}\\n\"\n",
    "        for i, ref in enumerate(references_data. get(\"references_list\", [])[:20], 1):\n",
    "            enriched_text += f\"[{i}] {ref}\\n\"\n",
    "\n",
    "    if figures_data:\n",
    "        enriched_text += \"\\n\\n## FIGURES IDENTIFI√âES\\n\"\n",
    "        enriched_text += f\"Nombre de figures:  {len(figures_data)}\\n\"\n",
    "        for fig in figures_data[: 10]:\n",
    "            enriched_text += f\"- {fig. get('label', '')} {fig.get('title', '')}\\n\"\n",
    "\n",
    "    try:\n",
    "        import langextract as lx\n",
    "        \n",
    "        extractor_kwargs = {\n",
    "            \"prompt\": PROMPT_TEMPLATE,\n",
    "        }\n",
    "        \n",
    "        if USE_OLLAMA and LANGEXTRACT_CONFIG[\"base_url\"]:\n",
    "            extractor_kwargs[\"provider\"] = \"ollama\"\n",
    "            extractor_kwargs[\"model\"] = LANGEXTRACT_CONFIG[\"model\"]\n",
    "            extractor_kwargs[\"base_url\"] = LANGEXTRACT_CONFIG[\"base_url\"]\n",
    "        \n",
    "        if hasattr(lx, \"extract\"):\n",
    "            extraction = lx.extract(enriched_text, **extractor_kwargs)\n",
    "        elif hasattr(lx, \"LangExtract\"):\n",
    "            extractor = lx.LangExtract(**extractor_kwargs)\n",
    "            extraction = extractor. extract(enriched_text)\n",
    "        else:\n",
    "            return {\"status\": \"error\", \"error\": \"API LangExtract introuvable\"}\n",
    "\n",
    "        result = _safe_json(extraction)\n",
    "        \n",
    "        extraction_path = doc_folders[\"analyses\"] / f\"{doc_name}_langextract.json\"\n",
    "        with open(extraction_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return {\"status\":  \"error\", \"error\":  str(e)}\n",
    "\n",
    "print(f\"‚úÖ LangExtract configur√© (Provider: {LANGEXTRACT_CONFIG['provider']}, Model: {LANGEXTRACT_CONFIG['model']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216e2839",
   "metadata": {},
   "source": [
    "## √âtape 8 ‚Äî Test sur un PDF (optionnel)\n",
    "Permet de valider la configuration avant le batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b595a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files = sorted([p for p in INPUT_DIR.iterdir() if p.suffix.lower() == \".pdf\"])\n",
    "\n",
    "if pdf_files:\n",
    "    sample_path = pdf_files[0]\n",
    "    sample_name = sample_path.stem\n",
    "    print(f\"üß™ Test sur:  {sample_path.name}\")\n",
    "    print(\"‚è≥ Conversion en cours...\\n\")\n",
    "    \n",
    "    test_start = time.time()\n",
    "    sample_result = convert_pdf_complete(sample_path, sample_name)\n",
    "    test_elapsed = time.time() - test_start\n",
    "    \n",
    "    if sample_result. get(\"error\"):\n",
    "        print(f\"‚ùå Erreur:  {sample_result['error']}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Conversion r√©ussie en {test_elapsed:.1f}s! \")\n",
    "        print(f\"   üìÇ Dossier: {sample_result['doc_folder']}\")\n",
    "        print(f\"   üìÑ Markdown: {sample_result['markdown_path']}\")\n",
    "        print(f\"   üñºÔ∏è Figures: {len(sample_result['figures_paths'])}\")\n",
    "        print(f\"   üìö R√©f√©rences: {sample_result['references']. get('reference_count', 0)}\")\n",
    "else:\n",
    "    print(\"‚ùå Aucun PDF trouv√© dans le dossier d'entr√©e.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e110727a",
   "metadata": {},
   "source": [
    "## √âtape 9 ‚Äî Pipeline complet avec reprise\n",
    "Traitement batch + logs + reprise automatique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc4b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as time_module\n",
    "\n",
    "def process_all_documents():\n",
    "    \"\"\"Traite tous les documents PDF avec reprise automatique.\"\"\"\n",
    "    \n",
    "    all_pdfs = sorted([p for p in INPUT_DIR.iterdir() if p.suffix.lower() == \".pdf\"])\n",
    "    \n",
    "    if not all_pdfs:\n",
    "        print(\"‚ùå Aucun PDF trouv√© dans le dossier d'entr√©e! \")\n",
    "        print(f\"   Chemin v√©rifi√©: {INPUT_DIR}\")\n",
    "        return [], []\n",
    "    \n",
    "    log_file = GLOBAL_LOGS_DIR / f\"processing_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "    progress_file = GLOBAL_LOGS_DIR / \"progress.json\"\n",
    "\n",
    "    def log_message(message):\n",
    "        print(message)\n",
    "        with open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(message + \"\\n\")\n",
    "\n",
    "    processed_files = set()\n",
    "    if progress_file.exists():\n",
    "        try:\n",
    "            with open(progress_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                progress_data = json.load(f)\n",
    "                processed_files = set(progress_data.get(\"processed\", []))\n",
    "            log_message(f\"üìÇ Reprise:  {len(processed_files)} fichiers d√©j√† trait√©s\")\n",
    "        except Exception:\n",
    "            processed_files = set()\n",
    "\n",
    "    remaining_pdfs = [p for p in all_pdfs if p. name not in processed_files]\n",
    "\n",
    "    log_message(\"=\"*60)\n",
    "    log_message(f\"üìÑ Total PDFs: {len(all_pdfs)}\")\n",
    "    log_message(f\"‚úÖ D√©j√† trait√©s: {len(processed_files)}\")\n",
    "    log_message(f\"‚è≥ Restants: {len(remaining_pdfs)}\")\n",
    "    log_message(\"=\"*60)\n",
    "\n",
    "    if not remaining_pdfs:\n",
    "        log_message(\"‚úÖ Tous les fichiers ont d√©j√† √©t√© trait√©s!\")\n",
    "        return [], []\n",
    "\n",
    "    results = []\n",
    "    errors = []\n",
    "    start_time = time_module.time()\n",
    "\n",
    "    def save_progress():\n",
    "        with open(progress_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"processed\": list(processed_files),\n",
    "                \"last_update\": datetime.now().isoformat()\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    for idx, pdf_path in enumerate(remaining_pdfs, 1):\n",
    "        doc_name = pdf_path.stem\n",
    "        doc_start_time = time_module.time()\n",
    "        \n",
    "        log_message(f\"\\n[{idx}/{len(remaining_pdfs)}] üöÄ Traitement:  {pdf_path.name}\")\n",
    "\n",
    "        conversion_result = convert_pdf_complete(pdf_path, doc_name)\n",
    "        \n",
    "        if conversion_result.get(\"error\"):\n",
    "            errors. append({\"file\": pdf_path. name, \"error\": conversion_result[\"error\"]})\n",
    "            log_message(f\"   ‚ùå Erreur conversion: {conversion_result['error']}\")\n",
    "            processed_files.add(pdf_path.name)\n",
    "            save_progress()\n",
    "            continue\n",
    "\n",
    "        doc_folders = get_doc_folders(doc_name)\n",
    "        \n",
    "        log_message(\n",
    "            f\"   üñºÔ∏è Figures: {len(conversion_result['figures'])} trouv√©es, \"\n",
    "            f\"{len(conversion_result['figures_paths'])} sauvegard√©es\"\n",
    "        )\n",
    "        log_message(\n",
    "            f\"   üìö R√©f√©rences: {conversion_result['references'].get('reference_count', 0)} extraites\"\n",
    "        )\n",
    "\n",
    "        extraction = None\n",
    "        if USE_LANGEXTRACT:\n",
    "            log_message(\"   üîç Extraction LangExtract... \")\n",
    "            with open(conversion_result['markdown_path'], 'r', encoding='utf-8') as f:\n",
    "                md_content = f.read()\n",
    "            extraction = extract_with_langextract(\n",
    "                md_content,\n",
    "                doc_name,\n",
    "                doc_folders,\n",
    "                conversion_result['references'],\n",
    "                conversion_result['figures'],\n",
    "            )\n",
    "            if extraction. get(\"status\") == \"error\":\n",
    "                log_message(f\"   ‚ö†Ô∏è LangExtract: {extraction. get('error', 'Erreur inconnue')}\")\n",
    "            else:\n",
    "                log_message(\"   ‚úÖ LangExtract termin√©\")\n",
    "\n",
    "        analysis = {\n",
    "            \"doc_name\": doc_name,\n",
    "            \"source_pdf\": str(pdf_path),\n",
    "            \"doc_folder\": str(doc_folders[\"root\"]),\n",
    "            \"processed_at\": datetime.now().isoformat(),\n",
    "            \"processing_time_seconds\": round(time_module.time() - doc_start_time, 2),\n",
    "            \"conversion\": {\n",
    "                \"markdown_path\": conversion_result['markdown_path'],\n",
    "                \"figures_count\": len(conversion_result['figures']),\n",
    "                \"figures_saved\": len(conversion_result['figures_paths']),\n",
    "                \"figures_paths\": conversion_result['figures_paths'],\n",
    "                \"references_count\": conversion_result['references'].get('reference_count', 0),\n",
    "            },\n",
    "            \"references\": conversion_result['references'],\n",
    "            \"figures\": conversion_result['figures'],\n",
    "            \"langextract\": extraction,\n",
    "        }\n",
    "\n",
    "        analysis_path = doc_folders[\"analyses\"] / f\"{doc_name}_analysis.json\"\n",
    "        with open(analysis_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(analysis, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        doc_log_path = doc_folders[\"logs\"] / \"processing. log\"\n",
    "        with open(doc_log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f. write(f\"Trait√© le: {datetime.now().isoformat()}\\n\")\n",
    "            f.write(f\"Dur√©e: {analysis['processing_time_seconds']} secondes\\n\")\n",
    "            f.write(f\"Figures:  {len(conversion_result['figures_paths'])}\\n\")\n",
    "            f.write(f\"R√©f√©rences: {conversion_result['references'].get('reference_count', 0)}\\n\")\n",
    "\n",
    "        results.append(analysis)\n",
    "        processed_files.add(pdf_path.name)\n",
    "        save_progress()\n",
    "        \n",
    "        elapsed = time_module.time() - doc_start_time\n",
    "        log_message(f\"   ‚è±Ô∏è Dur√©e: {elapsed:.1f}s\")\n",
    "\n",
    "    total_time = time_module.time() - start_time\n",
    "    log_message(\"\\n\" + \"=\"*60)\n",
    "    log_message(f\"‚úÖ Traitement termin√© en {total_time:.1f} secondes\")\n",
    "    log_message(f\"   üìÑ R√©ussis: {len(results)}\")\n",
    "    log_message(f\"   ‚ùå Erreurs: {len(errors)}\")\n",
    "    log_message(\"=\"*60)\n",
    "\n",
    "    return results, errors\n",
    "\n",
    "\n",
    "def export_all_results(results):\n",
    "    \"\"\"Exporte les r√©sultats globaux.\"\"\"\n",
    "    if not results:\n",
    "        print(\"‚ÑπÔ∏è Aucun r√©sultat √† exporter. \")\n",
    "        return\n",
    "    \n",
    "    summary_path = GLOBAL_LOGS_DIR / \"_SUMMARY.json\"\n",
    "    report_md_path = GLOBAL_LOGS_DIR / \"_REPORT. md\"\n",
    "\n",
    "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    all_references = []\n",
    "    for data in results:\n",
    "        refs = data.get(\"references\", {})\n",
    "        if refs. get(\"references_list\"):\n",
    "            for ref in refs[\"references_list\"]:\n",
    "                all_references.append({\n",
    "                    \"document\": data. get(\"doc_name\", \"\"),\n",
    "                    \"reference\": ref,\n",
    "                })\n",
    "\n",
    "    if all_references:\n",
    "        biblio_path = GLOBAL_LOGS_DIR / \"_BIBLIOGRAPHIE_COMPLETE.json\"\n",
    "        with open(biblio_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json. dump(all_references, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"üìö Bibliographie:  {biblio_path} ({len(all_references)} r√©f√©rences)\")\n",
    "\n",
    "    all_figures = []\n",
    "    for data in results:\n",
    "        for fig_path in data.get(\"conversion\", {}).get(\"figures_paths\", []):\n",
    "            all_figures.append({\n",
    "                \"document\": data. get(\"doc_name\", \"\"),\n",
    "                \"path\": fig_path,\n",
    "            })\n",
    "\n",
    "    if all_figures:\n",
    "        figures_index_path = GLOBAL_LOGS_DIR / \"_INDEX_FIGURES.json\"\n",
    "        with open(figures_index_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(all_figures, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"üñºÔ∏è Index figures: {figures_index_path} ({len(all_figures)} figures)\")\n",
    "\n",
    "    md_lines = []\n",
    "    md_lines.append(\"# Rapport de traitement MFEGSN\\n\\n\")\n",
    "    md_lines.append(f\"**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M')}\\n\\n\")\n",
    "    md_lines.append(f\"- **Total documents trait√©s:** {len(results)}\\n\")\n",
    "    md_lines.append(f\"- **Total figures extraites:** {len(all_figures)}\\n\")\n",
    "    md_lines.append(f\"- **Total r√©f√©rences:** {len(all_references)}\\n\\n\")\n",
    "    md_lines.append(\"## Documents trait√©s\\n\\n\")\n",
    "\n",
    "    for data in results:\n",
    "        conv = data.get(\"conversion\", {})\n",
    "        md_lines.append(f\"### {data. get('doc_name', '')}\\n\\n\")\n",
    "        md_lines.append(f\"- **Dossier:** `{data.get('doc_folder', '')}`\\n\")\n",
    "        md_lines.append(f\"- **Figures:** {conv.get('figures_count', 0)}\\n\")\n",
    "        md_lines. append(f\"- **R√©f√©rences:** {conv.get('references_count', 0)}\\n\")\n",
    "        md_lines.append(f\"- **Dur√©e:** {data.get('processing_time_seconds', 0)}s\\n\\n\")\n",
    "\n",
    "    with open(report_md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\". join(md_lines))\n",
    "\n",
    "    print(f\"‚úÖ R√©sultats export√©s: \")\n",
    "    print(f\"   üìä Summary: {summary_path}\")\n",
    "    print(f\"   üìù Report: {report_md_path}\")\n",
    "\n",
    "print(\"‚úÖ Fonctions de pipeline d√©finies. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830ddf11",
   "metadata": {},
   "source": [
    "## √âtape 10 ‚Äî Lancer le traitement\n",
    "Ex√©cutez cette cellule pour lancer le traitement complet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0f8f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recharger la liste des PDFs depuis INPUT_DIR pour s'assurer qu'elle est √† jour\n",
    "print(\"üìÇ V√©rification du dossier d'entr√©e...\")\n",
    "print(f\"   Chemin: {INPUT_DIR}\")\n",
    "\n",
    "# Recharger la liste des fichiers\n",
    "pdf_files = sorted([p for p in INPUT_DIR.iterdir() if p.suffix.lower() == \".pdf\"])\n",
    "print(f\"   PDFs trouv√©s: {len(pdf_files)}\")\n",
    "\n",
    "if len(pdf_files) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è ATTENTION: Aucun PDF trouv√©!\")\n",
    "    print(\"   V√©rifiez que:\")\n",
    "    print(\"   1. Le chemin INPUT_DIR est correct\")\n",
    "    print(\"   2. Des fichiers PDF sont pr√©sents dans ce dossier\")\n",
    "    print(\"   3. Google Drive est bien mont√©\")\n",
    "else:\n",
    "    for i, pdf in enumerate(pdf_files[:5], 1):\n",
    "        print(f\"   {i}. {pdf.name}\")\n",
    "    if len(pdf_files) > 5:\n",
    "        print(f\"   ... et {len(pdf_files) - 5} autres\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ D√©marrage du pipeline...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results, errors = process_all_documents()\n",
    "\n",
    "if results:\n",
    "    export_all_results(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ PIPELINE TERMIN√â\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if errors:\n",
    "    print(f\"\\n‚ö†Ô∏è {len(errors)} erreur(s) rencontr√©e(s):\")\n",
    "    for err in errors[:5]:\n",
    "        print(f\"   - {err['file']}: {err['error'][:50]}...\")\n",
    "    if len(errors) > 5:\n",
    "        print(f\"   ... et {len(errors) - 5} autres erreurs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utils001",
   "metadata": {},
   "source": [
    "## Utilitaires ‚Äî Gestion du cache\n",
    "Cellules optionnelles pour g√©rer le cache des mod√®les sur Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utils002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === AFFICHER LA TAILLE DU CACHE ===\n",
    "def show_cache_info():\n",
    "    \"\"\"Affiche les informations sur le cache Drive.\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"üíæ INFORMATIONS CACHE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for name, path in [(\"HuggingFace\", HF_CACHE_DRIVE), \n",
    "                       (\"Torch\", TORCH_CACHE_DRIVE), \n",
    "                       (\"Datalab\", DATALAB_CACHE_DRIVE)]:\n",
    "        size = get_dir_size(path)\n",
    "        print(f\"üìÇ {name}: {size:.2f} GB\")\n",
    "    \n",
    "    total = get_dir_size(DRIVE_CACHE_DIR)\n",
    "    print(f\"\\nüì¶ Total: {total:.2f} GB\")\n",
    "    print(f\"üìç Emplacement: {DRIVE_CACHE_DIR}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Appeler la fonction pour afficher les infos\n",
    "show_cache_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utils003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SUPPRIMER LE CACHE (si n√©cessaire) ===\n",
    "# ‚ö†Ô∏è ATTENTION:  Cela supprimera tous les mod√®les en cache!\n",
    "# D√©commentez les lignes ci-dessous pour ex√©cuter.\n",
    "\n",
    "# import shutil\n",
    "# \n",
    "# if DRIVE_CACHE_DIR. exists():\n",
    "#     print(f\"‚ö†Ô∏è Suppression du cache:  {DRIVE_CACHE_DIR}\")\n",
    "#     shutil.rmtree(DRIVE_CACHE_DIR)\n",
    "#     print(\"‚úÖ Cache supprim√©.  Les mod√®les seront ret√©l√©charg√©s au prochain lancement.\")\n",
    "# else:\n",
    "#     print(\"‚ÑπÔ∏è Aucun cache √† supprimer.\")\n",
    "\n",
    "print(\"‚ÑπÔ∏è D√©commentez le code ci-dessus pour supprimer le cache.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}