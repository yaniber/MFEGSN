# ============================================
# NOTEBOOK COLAB COMPLET :  Marker + LangExtract
# Traitement illimitÃ© avec extraction figures & rÃ©fÃ©rences
# Version corrigÃ©e avec 2 workers et zstd
# ============================================

# ============================================
# Ã‰TAPE 1 : Installation des dÃ©pendances systÃ¨me + Python
# ============================================

# Installer zstd (requis pour Ollama)
!apt-get update -qq
!apt-get install -y zstd -qq

# Installer les packages Python
!pip install marker-pdf[full] langextract -q
!pip install google-generativeai pillow -q

# Installation d'Ollama (maintenant que zstd est installÃ©)
!curl -fsSL https://ollama.com/install.sh | sh

print("âœ… Toutes les dÃ©pendances installÃ©es !")

# ============================================
# Ã‰TAPE 2 :  Montage du Google Drive
# ============================================

from google. colab import drive
drive.mount('/content/drive')

# ============================================
# Ã‰TAPE 3 : Configuration des dossiers
# ============================================

import os

# === MODIFIEZ CES CHEMINS SELON VOTRE DRIVE ===
input_folder = "/content/drive/MyDrive/GÃ©opolitique et SouverainetÃ© NumÃ©riques/ALL/ALLPDF"
output_folder = "/content/drive/MyDrive/GÃ©opolitique et SouverainetÃ© NumÃ©riques/ALL/ALLMD"

# Sous-dossiers pour organisation
figures_folder = os.path.join(output_folder, "_FIGURES")
references_folder = os.path.join(output_folder, "_REFERENCES")
analyses_folder = os.path.join(output_folder, "_ANALYSES")
logs_folder = os.path.join(output_folder, "_LOGS")

# CrÃ©er tous les dossiers
for folder in [output_folder, figures_folder, references_folder, analyses_folder, logs_folder]:
    os.makedirs(folder, exist_ok=True)

# Compter les PDFs
pdf_files = [f for f in os.listdir(input_folder) if f.lower().endswith('.pdf')]
print(f"{'='*60}")
print(f"ğŸ“ CONFIGURATION")
print(f"{'='*60}")
print(f"ğŸ“‚ Dossier d'entrÃ©e  : {input_folder}")
print(f"ğŸ“‚ Dossier de sortie : {output_folder}")
print(f"ğŸ“„ Nombre total de PDFs : {len(pdf_files)}")
print(f"ğŸ–¼ï¸  Figures extraites dans : {figures_folder}")
print(f"ğŸ“š RÃ©fÃ©rences extraites dans : {references_folder}")
print(f"{'='*60}")

# ============================================
# Ã‰TAPE 4 : DÃ©marrer Ollama + Gemma 3 4B
# ============================================

import subprocess
import time

# DÃ©marrer Ollama en arriÃ¨re-plan
ollama_process = subprocess.Popen(
    ['ollama', 'serve'],
    stdout=subprocess.DEVNULL,
    stderr=subprocess.DEVNULL
)
print("â³ DÃ©marrage d'Ollama...")
time.sleep(10)

# TÃ©lÃ©charger Gemma 3 4B
print("ğŸ“¥ TÃ©lÃ©chargement de Gemma 3 4B (environ 3GB)...")
!ollama pull gemma3:4b

# VÃ©rifier que le modÃ¨le est disponible
! ollama list

print("âœ… Gemma 3 4B prÃªt !")

# ============================================
# Ã‰TAPE 5 : Configuration de Marker avec 2 WORKERS + extraction complÃ¨te
# ============================================

import torch
from marker.converters.pdf import PdfConverter
from marker.models import create_model_dict
from marker.config. parser import ConfigParser
import shutil
import re
import json
from PIL import Image
from datetime import datetime

# VÃ©rifier GPU
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸ–¥ï¸  Device : {device}")
if device == "cuda":
    print(f"   GPU :  {torch.cuda.get_device_name(0)}")
    print(f"   VRAM : {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

# Configuration Marker avec 2 WORKERS
marker_config = {
    # === WORKERS ===
    "workers": 2,  # 2 workers pour parallÃ©lisation

    # === Extraction des images/figures ===
    "extract_images": True,
    "images_as_base64": False,

    # === QualitÃ© de conversion ===
    "use_llm": False,
    "force_ocr": False,

    # === Langues ===
    "languages": ["fr", "en"],

    # === Pagination ===
    "paginate_output": True,

    # === Performance ===
    "batch_size": 4 if device == "cuda" else 2,
}

print("ğŸ“¥ Chargement des modÃ¨les Marker (premiÃ¨re fois = tÃ©lÃ©chargement)...")
print(f"âš™ï¸  Configuration : {marker_config['workers']} workers, batch_size={marker_config['batch_size']}")

# CrÃ©er le dictionnaire de modÃ¨les
model_dict = create_model_dict()

# CrÃ©er le convertisseur avec la configuration
config_parser = ConfigParser(marker_config)
converter = PdfConverter(
    config=config_parser. generate_config_dict(),
    artifact_dict=model_dict,
)

print("âœ… Marker configurÃ© avec 2 workers !")

# ============================================
# Ã‰TAPE 6 :  Fonctions d'extraction avancÃ©es
# ============================================

def extract_references_from_markdown(markdown_text):
    """
    Extrait la section rÃ©fÃ©rences/bibliographie du Markdown
    """
    references = {
        'references_text': '',
        'references_list': [],
        'reference_count': 0
    }

    # Patterns pour identifier la section rÃ©fÃ©rences
    ref_patterns = [
        r'(? : ^|\n)#{1,3}\s*(? :References? |RÃ©fÃ©rences?|Bibliography|Bibliographie|Works?\s*Cited|Sources?|Ouvrages?\s*citÃ©s?)[\s: ]*\n([\s\S]*?)(?=\n#{1,3}\s|\Z)',
        r'(?:^|\n)\*\*(? :References?|RÃ©fÃ©rences?|Bibliography|Bibliographie)\*\*[\s:]*\n([\s\S]*?)(?=\n\*\*|\n#{1,3}|\Z)',
    ]

    for pattern in ref_patterns:
        match = re.search(pattern, markdown_text, re.IGNORECASE | re.MULTILINE)
        if match:
            ref_section = match.group(1).strip()
            references['references_text'] = ref_section

            # Extraire les rÃ©fÃ©rences individuelles
            ref_lines = []
            lines = ref_section.split('\n')
            current_ref = ""

            for line in lines:
                line = line.strip()
                if not line:
                    if current_ref:
                        ref_lines.append(current_ref. strip())
                        current_ref = ""
                    continue

                # Nouvelle rÃ©fÃ©rence si commence par marqueur
                if re.match(r'^(\[\d+\]|\d+\.|[-â€¢]|\([A-Z])', line):
                    if current_ref:
                        ref_lines.append(current_ref.strip())
                    current_ref = line
                else:
                    current_ref += " " + line

            if current_ref:
                ref_lines.append(current_ref. strip())

            references['references_list'] = [r for r in ref_lines if len(r) > 20]
            references['reference_count'] = len(references['references_list'])
            break

    return references


def extract_figures_info(markdown_text, images_dict):
    """
    Extrait les informations sur les figures du document
    """
    figures = []

    # Trouver les rÃ©fÃ©rences aux figures dans le texte
    fig_pattern = r'!\[([^\]]*)\]\(([^)]+)\)|(? :Figure|Fig\. ?\s*|Illustration)\s*(\d+)[:\s]*([^\n]+)?'

    for match in re.finditer(fig_pattern, markdown_text, re.IGNORECASE):
        fig_info = {
            'alt_text': match.group(1) if match.group(1) else '',
            'path': match.group(2) if match.group(2) else '',
            'number': match.group(3) if match.group(3) else '',
            'caption': match.group(4).strip() if match.group(4) else '',
            'position': match.start()
        }
        figures. append(fig_info)

    # Ajouter les images du dictionnaire Marker
    if images_dict:
        for img_name, img_data in images_dict.items():
            existing = any(f['path'] == img_name for f in figures)
            if not existing:
                figures.append({
                    'alt_text': img_name,
                    'path':  img_name,
                    'number': '',
                    'caption': '',
                    'from_marker': True
                })

    return figures


def save_figures(images_dict, doc_name, figures_base_folder):
    """
    Sauvegarde les figures extraites dans un dossier dÃ©diÃ©
    """
    saved_paths = []

    if not images_dict:
        return saved_paths

    # CrÃ©er un sous-dossier pour ce document
    doc_figures_folder = os.path.join(figures_base_folder, doc_name)
    os.makedirs(doc_figures_folder, exist_ok=True)

    for img_name, img_data in images_dict.items():
        try:
            if isinstance(img_data, Image.Image):
                img_path = os.path.join(doc_figures_folder, f"{img_name}.png")
                img_data.save(img_path, "PNG")
                saved_paths. append(img_path)
            elif isinstance(img_data, bytes):
                img_path = os.path.join(doc_figures_folder, f"{img_name}.png")
                with open(img_path, 'wb') as f:
                    f. write(img_data)
                saved_paths.append(img_path)
            elif isinstance(img_data, str) and os.path.exists(img_data):
                dest_path = os.path.join(doc_figures_folder, os.path.basename(img_data))
                shutil.copy2(img_data, dest_path)
                saved_paths.append(dest_path)
        except Exception as e:
            print(f"    âš ï¸ Erreur sauvegarde figure {img_name}: {e}")

    return saved_paths


def convert_pdf_complete(pdf_path, doc_name):
    """
    Conversion complÃ¨te d'un PDF avec extraction figures et rÃ©fÃ©rences
    """
    result_data = {
        'markdown':  None,
        'metadata': {},
        'figures': [],
        'figures_paths': [],
        'references': {},
        'page_count': 0,
        'error': None
    }

    try:
        # Conversion avec Marker
        result = converter(pdf_path)

        result_data['markdown'] = result. markdown
        result_data['metadata'] = result.metadata if hasattr(result, 'metadata') else {}

        # Nombre de pages
        if hasattr(result, 'pages'):
            result_data['page_count'] = len(result. pages)
        elif hasattr(result, 'document') and hasattr(result.document, 'pages'):
            result_data['page_count'] = len(result. document.pages)

        # Extraction des images
        images_dict = {}
        if hasattr(result, 'images') and result.images:
            images_dict = result.images

        # Sauvegarder les figures
        result_data['figures_paths'] = save_figures(images_dict, doc_name, figures_folder)

        # Extraire les infos sur les figures
        result_data['figures'] = extract_figures_info(result.markdown, images_dict)

        # Extraire les rÃ©fÃ©rences
        result_data['references'] = extract_references_from_markdown(result.markdown)

        # Sauvegarder les rÃ©fÃ©rences dans un fichier sÃ©parÃ©
        if result_data['references']['reference_count'] > 0:
            ref_path = os.path.join(references_folder, f"{doc_name}_references.json")
            with open(ref_path, 'w', encoding='utf-8') as f:
                json.dump(result_data['references'], f, ensure_ascii=False, indent=2)

    except Exception as e:
        result_data['error'] = str(e)
        import traceback
        result_data['traceback'] = traceback.format_exc()

    return result_data

# ============================================
# Ã‰TAPE 7 : Configuration LangExtract avec Gemma 3 4B
# ============================================

import langextract as lx

MODEL_ID = "gemma3:4b"
MODEL_URL = "http://localhost:11434"
TIMEOUT = 600  # 10 minutes pour les trÃ¨s longs documents

# Prompt enrichi pour sciences sociales avec figures et rÃ©fÃ©rences
PROMPT_COMPLET = """
Analyser ce document acadÃ©mique (article ou ouvrage) et extraire TOUTES les informations suivantes de maniÃ¨re exhaustive :

## 1. MÃ‰TADONNÃ‰ES BIBLIOGRAPHIQUES
- Titre complet
- Sous-titre (si prÃ©sent)
- Auteur(s) avec affiliations
- Date de publication
- Ã‰diteur / Revue
- ISBN / DOI / ISSN (si mentionnÃ©)
- Type :  article, chapitre, ouvrage, rapport, thÃ¨se, working paper

## 2. STRUCTURE DU DOCUMENT
- Table des matiÃ¨res / plan
- Nombre de parties/chapitres
- PrÃ©sence d'annexes

## 3. PROBLÃ‰MATIQUE ET CADRE THÃ‰ORIQUE
- Question de recherche principale
- Questions secondaires
- HypothÃ¨ses formulÃ©es
- Cadre thÃ©orique / paradigme
- Auteurs et thÃ©ories de rÃ©fÃ©rence
- Concepts clÃ©s (avec dÃ©finitions si fournies)
- Champ disciplinaire

## 4. MÃ‰THODOLOGIE
- Approche :  qualitative / quantitative / mixte
- MÃ©thodes de collecte :  entretiens, archives, enquÃªte, observation, etc.
- Terrain d'Ã©tude gÃ©ographique
- PÃ©riode temporelle couverte
- Ã‰chantillon / corpus
- Outils d'analyse utilisÃ©s
- Limites mÃ©thodologiques mentionnÃ©es

## 5. ARGUMENTS ET RÃ‰SULTATS PRINCIPAUX
- ThÃ¨se centrale / argument principal
- Arguments secondaires (liste numÃ©rotÃ©e)
- RÃ©sultats empiriques majeurs
- DonnÃ©es chiffrÃ©es importantes
- Ã‰tudes de cas dÃ©taillÃ©es
- Comparaisons internationales (si applicable)

## 6. FIGURES ET TABLEAUX
- Liste des figures mentionnÃ©es avec leurs titres
- Tableaux de donnÃ©es importants
- Cartes ou schÃ©mas

## 7. CONCLUSIONS ET IMPLICATIONS
- Conclusions principales
- Contributions Ã  la littÃ©rature
- Implications politiques / pratiques
- Limites reconnues
- Agenda de recherche futur

## 8. RÃ‰FÃ‰RENCES CLÃ‰S
- Auteurs les plus citÃ©s
- Ouvrages fondamentaux mentionnÃ©s
- DÃ©bats acadÃ©miques rÃ©fÃ©rencÃ©s

## 9. MOTS-CLÃ‰S ET THÃ‰MATIQUES
- Mots-clÃ©s (si fournis)
- ThÃ©matiques principales
- Sous-thÃ©matiques

RÃ©ponds en JSON structurÃ© avec ces sections.
"""

EXAMPLES_COMPLET = [
    {
        "input":  """La souverainetÃ© numÃ©rique europÃ©enne :  enjeux et perspectives

        Cet ouvrage analyse les dÃ©fis de l'autonomie stratÃ©gique numÃ©rique de l'Union europÃ©enne
        face aux gÃ©ants technologiques amÃ©ricains et chinois. Ã€ travers une Ã©tude comparative
        de 5 pays membres et 40 entretiens avec des dÃ©cideurs, nous montrons que.. .""",
        "output": {
            "metadonnees": {
                "titre":  "La souverainetÃ© numÃ©rique europÃ©enne : enjeux et perspectives",
                "type": "ouvrage",
                "champ_disciplinaire": "Science politique, Relations internationales"
            },
            "problematique": {
                "question_principale": "Quels sont les dÃ©fis de l'autonomie stratÃ©gique numÃ©rique de l'UE ?",
                "concepts_cles": ["souverainetÃ© numÃ©rique", "autonomie stratÃ©gique", "gÃ©ants technologiques"],
                "cadre_theorique": "Ã‰tudes de sÃ©curitÃ©, Ã‰conomie politique internationale"
            },
            "methodologie": {
                "approche": "mixte",
                "methodes":  ["Ã©tude comparative", "entretiens semi-directifs"],
                "terrain": "5 pays membres de l'UE",
                "echantillon": "40 dÃ©cideurs"
            },
            "resultats": {
                "these_principale": "",
                "arguments":  [],
                "donnees_chiffrees": []
            },
            "figures":  [],
            "conclusions": {
                "principales": [],
                "implications_politiques": []
            },
            "references_cles": [],
            "mots_cles": ["souverainetÃ© numÃ©rique", "Europe", "technologie", "gÃ©opolitique"]
        }
    }
]


def extract_with_langextract(markdown_text, doc_name, references_data=None, figures_data=None):
    """
    Extraction structurÃ©e avec LangExtract + Gemma 3 4B
    """
    MAX_CHARS = 80000

    # Enrichir le texte avec les rÃ©fÃ©rences et figures
    enriched_text = markdown_text

    if references_data and references_data. get('reference_count', 0) > 0:
        enriched_text += "\n\n## RÃ‰FÃ‰RENCES EXTRAITES\n"
        enriched_text += f"Nombre de rÃ©fÃ©rences : {references_data['reference_count']}\n"
        for i, ref in enumerate(references_data. get('references_list', [])[:20], 1):
            enriched_text += f"{i}.  {ref}\n"

    if figures_data:
        enriched_text += f"\n\n## FIGURES IDENTIFIÃ‰ES\nNombre de figures : {len(figures_data)}\n"
        for fig in figures_data[: 10]:
            if fig.get('caption'):
                enriched_text += f"- {fig. get('caption')}\n"

    # Tronquer si nÃ©cessaire
    if len(enriched_text) > MAX_CHARS:
        print(f"    ğŸ“ Document long ({len(enriched_text)} car.) â†’ troncature intelligente")
        part1 = enriched_text[:MAX_CHARS // 2]
        part2 = enriched_text[-(MAX_CHARS // 2):]
        enriched_text = part1 + "\n\n[...  CONTENU INTERMÃ‰DIAIRE OMIS ... ]\n\n" + part2

    try:
        result = lx.extract(
            text_or_documents=enriched_text,
            prompt_description=PROMPT_COMPLET,
            examples=EXAMPLES_COMPLET,
            model_id=MODEL_ID,
            model_url=MODEL_URL,
            timeout=TIMEOUT,
            fence_output=False,
            use_schema_constraints=False
        )
        return result

    except Exception as e:
        print(f"    âŒ Erreur LangExtract:  {e}")
        return {"error": str(e)}

# ============================================
# Ã‰TAPE 8 : Pipeline complet SANS LIMITE
# ============================================

from tqdm import tqdm
import traceback

def process_all_documents():
    """
    Pipeline complet pour TOUS les documents sans limite
    Avec reprise automatique en cas d'interruption
    """
    log_file = os.path.join(logs_folder, f"processing_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")
    progress_file = os.path.join(logs_folder, "progress.json")

    # Charger la progression existante
    processed_files = set()
    if os.path.exists(progress_file):
        with open(progress_file, 'r') as f:
            progress_data = json.load(f)
            processed_files = set(progress_data.get('processed', []))
        print(f"ğŸ“‚ Reprise :  {len(processed_files)} fichiers dÃ©jÃ  traitÃ©s")

    # Liste des PDFs Ã  traiter
    all_pdfs = [f for f in os.listdir(input_folder) if f.lower().endswith('.pdf')]
    remaining_pdfs = [f for f in all_pdfs if f not in processed_files]

    print(f"\n{'='*60}")
    print(f"ğŸš€ DÃ‰MARRAGE DU PIPELINE COMPLET")
    print(f"{'='*60}")
    print(f"ğŸ“„ Total PDFs :  {len(all_pdfs)}")
    print(f"âœ… DÃ©jÃ  traitÃ©s : {len(processed_files)}")
    print(f"â³ Restants : {len(remaining_pdfs)}")
    print(f"âš™ï¸  Marker :  2 workers")
    print(f"ğŸ¤– LLM : {MODEL_ID}")
    print(f"{'='*60}\n")

    results = {}
    errors = []

    def log_message(msg):
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        full_msg = f"[{timestamp}] {msg}"
        print(full_msg)
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(full_msg + "\n")

    def save_progress():
        with open(progress_file, 'w') as f:
            json.dump({
                'processed': list(processed_files),
                'last_update': datetime.now().isoformat(),
                'total':  len(all_pdfs)
            }, f, indent=2)

    # Traitement de chaque PDF
    for i, filename in enumerate(tqdm(remaining_pdfs, desc="ğŸ“„ Traitement")):
        pdf_path = os.path.join(input_folder, filename)
        doc_name = os.path.splitext(filename)[0]

        log_message(f"[{i+1}/{len(remaining_pdfs)}] Traitement :  {filename}")

        try:
            # === Ã‰TAPE A : Conversion PDF â†’ Markdown + Figures + RÃ©fÃ©rences ===
            log_message(f"  ğŸ“ Conversion Marker (2 workers)...")
            conversion_result = convert_pdf_complete(pdf_path, doc_name)

            if conversion_result['error']:
                raise Exception(f"Erreur Marker:  {conversion_result['error']}")

            markdown_content = conversion_result['markdown']
            log_message(f"  âœ… Markdown :  {len(markdown_content)} caractÃ¨res")
            log_message(f"  ğŸ–¼ï¸  Figures : {len(conversion_result['figures'])} trouvÃ©es, {len(conversion_result['figures_paths'])} sauvegardÃ©es")
            log_message(f"  ğŸ“š RÃ©fÃ©rences : {conversion_result['references']. get('reference_count', 0)} extraites")

            # Sauvegarder le Markdown
            md_path = os.path.join(output_folder, f"{doc_name}.md")
            with open(md_path, 'w', encoding='utf-8') as f:
                f.write(markdown_content)

            # === Ã‰TAPE B : Extraction LangExtract ===
            log_message(f"  ğŸ” Extraction LangExtract avec Gemma 3 4B...")
            extraction = extract_with_langextract(
                markdown_content,
                doc_name,
                conversion_result['references'],
                conversion_result['figures']
            )

            # === Ã‰TAPE C : Compilation des rÃ©sultats ===
            result_data = {
                'filename': filename,
                'processed_at': datetime.now().isoformat(),
                'conversion':  {
                    'markdown_length': len(markdown_content),
                    'page_count': conversion_result['page_count'],
                    'figures_count': len(conversion_result['figures']),
                    'figures_saved': len(conversion_result['figures_paths']),
                    'figures_paths': conversion_result['figures_paths'],
                    'references_count':  conversion_result['references']. get('reference_count', 0)
                },
                'extraction':  extraction,
                'references': conversion_result['references'],
                'figures': conversion_result['figures']
            }

            # Sauvegarder l'analyse JSON
            analysis_path = os.path.join(analyses_folder, f"{doc_name}_analysis.json")
            with open(analysis_path, 'w', encoding='utf-8') as f:
                json.dump(result_data, f, ensure_ascii=False, indent=2)

            results[filename] = result_data
            processed_files.add(filename)
            save_progress()

            log_message(f"  âœ… TerminÃ© : {analysis_path}")

        except Exception as e:
            error_info = {
                'file': filename,
                'error':  str(e),
                'traceback': traceback.format_exc(),
                'timestamp': datetime. now().isoformat()
            }
            errors.append(error_info)
            log_message(f"  âŒ ERREUR : {e}")

            # Sauvegarder les erreurs
            errors_path = os.path.join(logs_folder, "errors.json")
            with open(errors_path, 'w', encoding='utf-8') as f:
                json.dump(errors, f, ensure_ascii=False, indent=2)

            continue

    # === RÃ‰SUMÃ‰ FINAL ===
    print(f"\n{'='*60}")
    print(f"âœ… PIPELINE TERMINÃ‰")
    print(f"{'='*60}")
    print(f"ğŸ“Š Documents traitÃ©s avec succÃ¨s : {len(results)}")
    print(f"âŒ Erreurs : {len(errors)}")
    print(f"ğŸ“‚ RÃ©sultats dans : {output_folder}")
    print(f"ğŸ“‹ Log complet : {log_file}")

    if errors:
        print(f"\nâš ï¸ Fichiers en erreur :")
        for err in errors[: 10]:
            print(f"   - {err['file']}: {err['error'][: 50]}...")
        if len(errors) > 10:
            print(f"   ...  et {len(errors) - 10} autres erreurs")

    return results, errors

# ============================================
# Ã‰TAPE 9 :  Exports consolidÃ©s
# ============================================

import pandas as pd

def export_all_results():
    """
    Compile et exporte tous les rÃ©sultats en formats multiples
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“¦ EXPORT DES RÃ‰SULTATS CONSOLIDÃ‰S")
    print(f"{'='*60}")

    # Charger toutes les analyses
    all_analyses = {}
    analysis_files = [f for f in os.listdir(analyses_folder) if f.endswith('_analysis.json')]

    for filename in tqdm(analysis_files, desc="Chargement"):
        filepath = os.path.join(analyses_folder, filename)
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
            all_analyses[data['filename']] = data

    print(f"ğŸ“„ {len(all_analyses)} analyses chargÃ©es")

    # === 1. JSON CONSOLIDÃ‰ COMPLET ===
    consolidated_path = os.path.join(output_folder, "_CORPUS_COMPLET.json")
    with open(consolidated_path, 'w', encoding='utf-8') as f:
        json.dump(all_analyses, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“„ JSON complet :  {consolidated_path}")

    # === 2. CSV SYNTHÃ‰TIQUE POUR EXCEL ===
    rows = []
    for filename, data in all_analyses.items():
        extraction = data.get('extraction', {})
        conversion = data.get('conversion', {})

        row = {
            'fichier': filename,
            'date_analyse': data.get('processed_at', ''),
            'pages':  conversion.get('page_count', 0),
            'longueur_md': conversion.get('markdown_length', 0),
            'nb_figures': conversion.get('figures_count', 0),
            'nb_references': conversion.get('references_count', 0),
        }

        if isinstance(extraction, dict) and 'error' not in extraction:
            meta = extraction.get('metadonnees', {})
            if isinstance(meta, dict):
                row['titre'] = meta.get('titre', '')
                row['type_document'] = meta.get('type', '')
                row['auteurs'] = meta.get('auteurs', '') if isinstance(meta.get('auteurs'), str) else '; '.join(meta.get('auteurs', []))

            prob = extraction.get('problematique', {})
            if isinstance(prob, dict):
                row['question_recherche'] = prob.get('question_principale', prob.get('question_recherche', ''))
                concepts = prob.get('concepts_cles', [])
                row['concepts_cles'] = '; '.join(concepts) if isinstance(concepts, list) else str(concepts)
                row['cadre_theorique'] = prob. get('cadre_theorique', '')

            methodo = extraction.get('methodologie', {})
            if isinstance(methodo, dict):
                row['approche'] = methodo.get('approche', '')
                methodes = methodo.get('methodes', [])
                row['methodes'] = '; '.join(methodes) if isinstance(methodes, list) else str(methodes)
                row['terrain'] = methodo.get('terrain', '')

            res = extraction.get('resultats', {})
            if isinstance(res, dict):
                row['these_principale'] = res.get('these_principale', res.get('these_centrale', ''))

            mots = extraction.get('mots_cles', [])
            row['mots_cles'] = '; '.join(mots) if isinstance(mots, list) else str(mots)

        rows.append(row)

    df = pd. DataFrame(rows)
    csv_path = os.path.join(output_folder, "_CORPUS_TABLEAU.csv")
    df.to_csv(csv_path, index=False, encoding='utf-8-sig')
    print(f"ğŸ“Š CSV tableau :  {csv_path}")

    # === 3. BIBLIOGRAPHIE CONSOLIDÃ‰E ===
    all_references = []
    for filename, data in all_analyses.items():
        refs = data.get('references', {})
        if refs. get('references_list'):
            for ref in refs['references_list']:
                all_references.append({
                    'source_document': filename,
                    'reference': ref
                })

    if all_references:
        biblio_path = os.path. join(references_folder, "_BIBLIOGRAPHIE_COMPLETE.json")
        with open(biblio_path, 'w', encoding='utf-8') as f:
            json.dump(all_references, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“š Bibliographie :  {biblio_path} ({len(all_references)} rÃ©fÃ©rences)")

    # === 4. INDEX DES FIGURES ===
    all_figures = []
    for filename, data in all_analyses.items():
        conv = data.get('conversion', {})
        for fig_path in conv.get('figures_paths', []):
            all_figures.append({
                'source_document': filename,
                'figure_path': fig_path
            })

    if all_figures:
        figures_index_path = os.path.join(figures_folder, "_INDEX_FIGURES.json")
        with open(figures_index_path, 'w', encoding='utf-8') as f:
            json. dump(all_figures, f, ensure_ascii=False, indent=2)
        print(f"ğŸ–¼ï¸  Index figures : {figures_index_path} ({len(all_figures)} figures)")

    # === 5. SYNTHÃˆSE MARKDOWN LISIBLE ===
    md_content = f"""# SynthÃ¨se du corpus de recherche

*GÃ©nÃ©rÃ©e le {datetime.now().strftime('%Y-%m-%d %H:%M')}*

## Statistiques gÃ©nÃ©rales

| MÃ©trique | Valeur |
|----------|--------|
| Documents analysÃ©s | {len(all_analyses)} |
| Total figures extraites | {len(all_figures)} |
| Total rÃ©fÃ©rences | {len(all_references)} |

---

"""

    for filename, data in list(all_analyses.items()):
        extraction = data.get('extraction', {})
        conversion = data.get('conversion', {})

        md_content += f"## ğŸ“„ {filename}\n\n"
        md_content += f"- **Pages** : {conversion.get('page_count', 'N/A')}\n"
        md_content += f"- **Figures** : {conversion.get('figures_count', 0)}\n"
        md_content += f"- **RÃ©fÃ©rences** : {conversion.get('references_count', 0)}\n\n"

        if isinstance(extraction, dict) and 'error' not in extraction:
            res = extraction.get('resultats', {})
            if isinstance(res, dict) and res.get('these_principale'):
                md_content += f"**ThÃ¨se principale** : {res['these_principale']}\n\n"

            prob = extraction.get('problematique', {})
            if isinstance(prob, dict) and prob.get('question_principale'):
                md_content += f"**Question de recherche** :  {prob['question_principale']}\n\n"

        md_content += "---\n\n"

    md_summary_path = os.path.join(output_folder, "_SYNTHESE_CORPUS.md")
    with open(md_summary_path, 'w', encoding='utf-8') as f:
        f.write(md_content)
    print(f"ğŸ“– SynthÃ¨se MD : {md_summary_path}")

    print(f"\nâœ… Export terminÃ© !")
    return df

# ============================================
# Ã‰TAPE 10 :  EXÃ‰CUTION
# ============================================

print(f"""
{'='*60}
ğŸ“‹ INSTRUCTIONS D'EXÃ‰CUTION
{'='*60}

1. ExÃ©cutez les cellules 1 Ã  9 dans l'ordre

2. Lancez le traitement complet :

   results, errors = process_all_documents()

3. Exportez les rÃ©sultats consolidÃ©s :

   df = export_all_results()
   display(df)

4. Structure des fichiers gÃ©nÃ©rÃ©s :

   {output_folder}/
   â”œâ”€â”€ *. md                          # Markdown de chaque document
   â”œâ”€â”€ _CORPUS_COMPLET.json          # Toutes les analyses
   â”œâ”€â”€ _CORPUS_TABLEAU.csv           # Pour Excel/Sheets
   â”œâ”€â”€ _SYNTHESE_CORPUS.md           # RÃ©sumÃ© lisible
   â”œâ”€â”€ _FIGURES/
   â”‚   â”œâ”€â”€ document1/
   â”‚   â”‚   â”œâ”€â”€ figure1.png
   â”‚   â”‚   â””â”€â”€ figure2.png
   â”‚   â””â”€â”€ _INDEX_FIGURES.json
   â”œâ”€â”€ _REFERENCES/
   â”‚   â”œâ”€â”€ doc1_references.json
   â”‚   â””â”€â”€ _BIBLIOGRAPHIE_COMPLETE.json
   â”œâ”€â”€ _ANALYSES/
   â”‚   â””â”€â”€ *_analysis.json
   â””â”€â”€ _LOGS/
       â”œâ”€â”€ progress.json              # Pour reprise
       â””â”€â”€ processing_log_*.txt

ğŸ’¡ En cas d'interruption, relancez simplement :
   results, errors = process_all_documents()
   Le script reprendra lÃ  oÃ¹ il s'est arrÃªtÃ© !
""")

# === DÃ‰COMMENTER POUR LANCER ===
# results, errors = process_all_documents()
# df = export_all_results()
# display(df)
