{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b54619af",
   "metadata": {
    "id": "b54619af"
   },
   "source": [
    "# MFEGSN ‚Äî Pipeline Colab (Marker + LangExtract)\n",
    "\n",
    "Notebook optimis√© pour une ex√©cution **pas √† pas** sur Google Colab.\n",
    "\n",
    "**üöÄ Optimisation:** Les mod√®les (~4GB) sont sauvegard√©s sur Google Drive apr√®s le premier t√©l√©chargement.\n",
    "Les sessions suivantes chargeront les mod√®les depuis Drive en quelques secondes.\n",
    "\n",
    "**Ordre recommand√©:** √âtapes 1 ‚Üí 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014f8fe4",
   "metadata": {
    "id": "014f8fe4"
   },
   "source": [
    "## √âtape 1 ‚Äî Monter Google Drive\n",
    "**IMPORTANT:** Ex√©cutez cette cellule EN PREMIER pour permettre le cache des mod√®les."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82dce4c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "82dce4c1",
    "outputId": "a0ec9951-f2f0-4a90-8312-2132ae67c838"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "============================================================\n",
      "‚úÖ Google Drive mont√©! \n",
      "============================================================\n",
      "üìÇ Cache Drive:  /content/drive/MyDrive/.mfegsn_cache\n",
      "üíæ Taille du cache: 3.53 GB\n",
      "üöÄ Mod√®les d√©j√† en cache!  Le chargement sera rapide.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Cr√©er le dossier de cache sur Drive\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# === CONFIGURATION DU CACHE ===\n",
    "DRIVE_CACHE_DIR = Path(\"/content/drive/MyDrive/.mfegsn_cache\")\n",
    "DRIVE_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Sous-dossiers pour chaque type de cache\n",
    "HF_CACHE_DRIVE = DRIVE_CACHE_DIR / \"huggingface\"\n",
    "TORCH_CACHE_DRIVE = DRIVE_CACHE_DIR / \"torch\"\n",
    "DATALAB_CACHE_DRIVE = DRIVE_CACHE_DIR / \"datalab\"\n",
    "\n",
    "for cache_dir in [HF_CACHE_DRIVE, TORCH_CACHE_DRIVE, DATALAB_CACHE_DRIVE]:\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configurer les variables d'environnement pour utiliser le cache Drive\n",
    "os. environ[\"HF_HOME\"] = str(HF_CACHE_DRIVE)\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = str(HF_CACHE_DRIVE)\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = str(HF_CACHE_DRIVE)\n",
    "os.environ[\"TORCH_HOME\"] = str(TORCH_CACHE_DRIVE)\n",
    "os.environ[\"XDG_CACHE_HOME\"] = str(DRIVE_CACHE_DIR)\n",
    "\n",
    "# V√©rifier la taille du cache existant\n",
    "def get_dir_size(path):\n",
    "    total = 0\n",
    "    if path.exists():\n",
    "        for f in path.rglob(\"*\"):\n",
    "            if f.is_file():\n",
    "                total += f.stat().st_size\n",
    "    return total / 1e9\n",
    "\n",
    "cache_size = get_dir_size(DRIVE_CACHE_DIR)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ Google Drive mont√©! \")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìÇ Cache Drive:  {DRIVE_CACHE_DIR}\")\n",
    "print(f\"üíæ Taille du cache: {cache_size:.2f} GB\")\n",
    "if cache_size > 3:\n",
    "    print(\"üöÄ Mod√®les d√©j√† en cache!  Le chargement sera rapide.\")\n",
    "else:\n",
    "    print(\"üì• Premier lancement:  les mod√®les seront t√©l√©charg√©s et mis en cache.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c063508",
   "metadata": {
    "id": "3c063508"
   },
   "source": [
    "## √âtape 2 ‚Äî Installer les d√©pendances\n",
    "Installation des packages Python n√©cessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7a1e27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6a7a1e27",
    "outputId": "7ccb3668-75ae-434b-e37f-72a4250dc68f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "Selecting previously unselected package zstd.\n",
      "(Reading database ... 121689 files and directories currently installed.)\n",
      "Preparing to unpack .../zstd_1.4.8+dfsg-3build1_amd64.deb ...\n",
      "Unpacking zstd (1.4.8+dfsg-3build1) ...\n",
      "Setting up zstd (1.4.8+dfsg-3build1) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "\n",
      "üì¶ Versions install√©es:\n",
      "Version: 0.36.0\n",
      "Version: 4.57.6\n",
      "Version: 1.10.1\n",
      "\n",
      "‚úÖ D√©pendances install√©es.\n"
     ]
    }
   ],
   "source": [
    "# D√©pendances syst√®me\n",
    "!apt-get update -qq\n",
    "!apt-get install -y zstd -qq\n",
    "\n",
    "# D√©pendances Python\n",
    "!python -m pip install -q --upgrade pip\n",
    "!python -m pip install -q marker-pdf[full] langextract google-generativeai pillow pymupdf\n",
    "\n",
    "# Installer hf_transfer pour des t√©l√©chargements plus rapides (sans casser les d√©pendances)\n",
    "!pip install -q hf_transfer --no-deps\n",
    "\n",
    "# Activer hf_transfer\n",
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "# V√©rifier les versions\n",
    "print(\"\\nüì¶ Versions install√©es:\")\n",
    "!pip show huggingface_hub 2>/dev/null | grep Version\n",
    "!pip show transformers 2>/dev/null | grep Version\n",
    "!pip show marker-pdf 2>/dev/null | grep Version\n",
    "!pip show pymupdf 2>/dev/null | grep Version\n",
    "\n",
    "print(\"\\n‚úÖ D√©pendances install√©es.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9950b81",
   "metadata": {
    "id": "e9950b81"
   },
   "source": [
    "## √âtape 3 ‚Äî Configurer les dossiers de travail\n",
    "Modifiez les chemins selon votre Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "749433f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "749433f3",
    "outputId": "e12cafc3-cba0-4ebd-f2bd-2ae020671ea4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìÅ CONFIGURATION\n",
      "============================================================\n",
      "üìÇ Entr√©e: /content/drive/MyDrive/G√©opolitique et Souverainet√© Num√©riques/ALL/ALLPDF\n",
      "üìÇ Sortie: /content/drive/MyDrive/G√©opolitique et Souverainet√© Num√©riques/ALL/ALLMD\n",
      "üìÑ PDFs: 54\n",
      "üìÇ Structure par fichier:  <nom_fichier>/\n",
      "   ‚îú‚îÄ‚îÄ _ANALYSES/\n",
      "   ‚îú‚îÄ‚îÄ _FIGURES/\n",
      "   ‚îú‚îÄ‚îÄ _LOGS/\n",
      "   ‚îî‚îÄ‚îÄ _REFERENCES/\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# === MODIFIEZ CES CHEMINS ===\n",
    "INPUT_DIR = Path(\"/content/drive/MyDrive/G√©opolitique et Souverainet√© Num√©riques/ALL/ALLPDF\")\n",
    "OUTPUT_DIR = Path(\"/content/drive/MyDrive/G√©opolitique et Souverainet√© Num√©riques/ALL/ALLMD\")\n",
    "\n",
    "assert INPUT_DIR.exists(), f\"‚ùå Dossier introuvable: {INPUT_DIR}\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dossier global pour les logs du pipeline\n",
    "GLOBAL_LOGS_DIR = OUTPUT_DIR / \"_PIPELINE_LOGS\"\n",
    "GLOBAL_LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def get_doc_folders(doc_name):\n",
    "    \"\"\"Cr√©e et retourne les dossiers pour un document donn√©.\"\"\"\n",
    "    doc_dir = OUTPUT_DIR / doc_name\n",
    "    doc_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    folders = {\n",
    "        \"root\": doc_dir,\n",
    "        \"figures\": doc_dir / \"_FIGURES\",\n",
    "        \"references\": doc_dir / \"_REFERENCES\",\n",
    "        \"analyses\": doc_dir / \"_ANALYSES\",\n",
    "        \"logs\": doc_dir / \"_LOGS\",\n",
    "    }\n",
    "\n",
    "    for folder in folders.values():\n",
    "        folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    return folders\n",
    "\n",
    "pdf_files = sorted([p for p in INPUT_DIR.iterdir() if p.suffix.lower() == \".pdf\"])\n",
    "print(\"=\"*60)\n",
    "print(\"üìÅ CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìÇ Entr√©e: {INPUT_DIR}\")\n",
    "print(f\"üìÇ Sortie: {OUTPUT_DIR}\")\n",
    "print(f\"üìÑ PDFs: {len(pdf_files)}\")\n",
    "print(f\"üìÇ Structure par fichier:  <nom_fichier>/\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ _ANALYSES/\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ _FIGURES/\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ _LOGS/\")\n",
    "print(f\"   ‚îî‚îÄ‚îÄ _REFERENCES/\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa74e5d1",
   "metadata": {},
   "source": [
    "## √âtape 4 ‚Äî Renommer les fichiers PDF (Optionnel)\n",
    "Cette √©tape permet de renommer en lot les fichiers PDF en rempla√ßant une cha√Æne de caract√®res par une autre.\n",
    "\n",
    "**Utile pour:** Nettoyer les noms de fichiers, supprimer des pr√©fixes/suffixes ind√©sirables, corriger des caract√®res\n",
    "\n",
    "**‚ö†Ô∏è Cette √©tape est optionnelle.** Mettez `RENAME_FILES = False` pour la sauter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e447e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION DU RENOMMAGE ===\n",
    "RENAME_FILES = True  # Mettre √† False pour sauter cette √©tape\n",
    "\n",
    "# === R√àGLES DE REMPLACEMENT ===\n",
    "# Ajoutez autant de r√®gles que n√©cessaire\n",
    "# Format: (\"cha√Æne √† chercher\", \"cha√Æne de remplacement\")\n",
    "RENAME_RULES = [\n",
    "    (\"_ocr\", \"\"),           # Exemple: supprimer \"_ocr\" des noms\n",
    "    # (\"ancien\", \"nouveau\"),  # Ajoutez d'autres r√®gles ici\n",
    "    # (\" - \", \"_\"),           # Remplacer \" - \" par \"_\"\n",
    "    # (\"  \", \" \"),            # Supprimer les doubles espaces\n",
    "]\n",
    "\n",
    "# === OPTIONS AVANC√âES ===\n",
    "RENAME_PREVIEW_ONLY = True   # True = afficher sans renommer, False = renommer r√©ellement\n",
    "RENAME_TARGET_DIR = INPUT_DIR  # Dossier cible (INPUT_DIR par d√©faut)\n",
    "\n",
    "if RENAME_FILES:\n",
    "    from pathlib import Path\n",
    "    import shutil\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"üìù RENOMMAGE DES FICHIERS PDF\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if RENAME_PREVIEW_ONLY:\n",
    "        print(\"‚ö†Ô∏è MODE APER√áU: Aucun fichier ne sera renomm√©\")\n",
    "        print(\"   Mettez RENAME_PREVIEW_ONLY = False pour appliquer les changements\")\n",
    "    else:\n",
    "        print(\"üîÑ MODE ACTIF: Les fichiers seront renomm√©s!\")\n",
    "    \n",
    "    print(f\"\\nüìÇ Dossier: {RENAME_TARGET_DIR}\")\n",
    "    print(f\"\\nüìã R√®gles de remplacement:\")\n",
    "    for old, new in RENAME_RULES:\n",
    "        display_new = f'\"{new}\"' if new else \"(supprim√©)\"\n",
    "        print(f'   \"{old}\" ‚Üí {display_new}')\n",
    "    \n",
    "    # Lister les fichiers PDF\n",
    "    pdf_files = sorted([p for p in RENAME_TARGET_DIR.iterdir() if p.suffix.lower() == \".pdf\"])\n",
    "    print(f\"\\nüìÑ {len(pdf_files)} fichiers PDF trouv√©s\")\n",
    "    \n",
    "    # Calculer les nouveaux noms\n",
    "    rename_plan = []\n",
    "    skipped = 0\n",
    "    \n",
    "    for pdf_path in pdf_files:\n",
    "        old_name = pdf_path.stem\n",
    "        new_name = old_name\n",
    "        \n",
    "        # Appliquer toutes les r√®gles de remplacement\n",
    "        for search_str, replace_str in RENAME_RULES:\n",
    "            if search_str:  # Ignorer les r√®gles vides\n",
    "                new_name = new_name.replace(search_str, replace_str)\n",
    "        \n",
    "        # Nettoyer les espaces multiples et les espaces en d√©but/fin\n",
    "        new_name = \" \".join(new_name.split()).strip()\n",
    "        \n",
    "        # V√©rifier si le nom a chang√©\n",
    "        if new_name != old_name:\n",
    "            new_path = pdf_path.parent / f\"{new_name}.pdf\"\n",
    "            rename_plan.append({\n",
    "                \"old_path\": pdf_path,\n",
    "                \"new_path\": new_path,\n",
    "                \"old_name\": old_name,\n",
    "                \"new_name\": new_name,\n",
    "            })\n",
    "        else:\n",
    "            skipped += 1\n",
    "    \n",
    "    print(f\"\\nüìä R√©sultat de l'analyse:\")\n",
    "    print(f\"   üîÑ √Ä renommer: {len(rename_plan)}\")\n",
    "    print(f\"   ‚è≠Ô∏è Inchang√©s: {skipped}\")\n",
    "    \n",
    "    if rename_plan:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"üìã APER√áU DES CHANGEMENTS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for idx, item in enumerate(rename_plan, 1):\n",
    "            print(f\"\\n{idx}. {item['old_name']}\")\n",
    "            print(f\"   ‚Üí {item['new_name']}\")\n",
    "        \n",
    "        # Appliquer les changements si pas en mode aper√ßu\n",
    "        if not RENAME_PREVIEW_ONLY:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(\"üîÑ APPLICATION DES CHANGEMENTS\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            success_count = 0\n",
    "            error_count = 0\n",
    "            \n",
    "            for item in rename_plan:\n",
    "                try:\n",
    "                    # V√©rifier si le fichier destination existe d√©j√†\n",
    "                    if item['new_path'].exists():\n",
    "                        print(f\"‚ö†Ô∏è {item['old_name']} ‚Üí Fichier destination existe d√©j√†!\")\n",
    "                        error_count += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Renommer le fichier\n",
    "                    item['old_path'].rename(item['new_path'])\n",
    "                    print(f\"‚úÖ {item['old_name']} ‚Üí {item['new_name']}\")\n",
    "                    success_count += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå {item['old_name']} ‚Üí Erreur: {str(e)[:50]}\")\n",
    "                    error_count += 1\n",
    "            \n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(\"üìä R√âSUM√â DU RENOMMAGE\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"‚úÖ Renomm√©s: {success_count}\")\n",
    "            print(f\"‚ùå Erreurs: {error_count}\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            # Mettre √† jour la liste des PDFs\n",
    "            pdf_files = sorted([p for p in RENAME_TARGET_DIR.iterdir() if p.suffix.lower() == \".pdf\"])\n",
    "            print(f\"\\nüìÑ Nouveaux fichiers: {len(pdf_files)}\")\n",
    "        else:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(\"üí° Pour appliquer ces changements:\")\n",
    "            print(\"   1. Mettez RENAME_PREVIEW_ONLY = False\")\n",
    "            print(\"   2. R√©ex√©cutez cette cellule\")\n",
    "            print(\"=\"*60)\n",
    "    else:\n",
    "        print(\"\\n‚úÖ Aucun fichier √† renommer (aucune r√®gle ne s'applique)\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Renommage d√©sactiv√©. Mettez RENAME_FILES = True pour l'activer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5c801b",
   "metadata": {
    "id": "8a5c801b"
   },
   "source": [
    "## √âtape 5 ‚Äî Ollama + Gemma 3 4B\n",
    "Installation et d√©marrage de Gemma 3 4B via Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "352143f1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "352143f1",
    "outputId": "9c4caa0b-fc5d-4f3d-f2e7-1d37284e1efd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Installation d'Ollama...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n",
      "‚è≥ D√©marrage du serveur Ollama...\n",
      "‚úÖ Serveur Ollama pr√™t!\n",
      "\n",
      "üì• T√©l√©chargement de Gemma 3 4B (‚âà3GB)...\n",
      "   Cela peut prendre plusieurs minutes...\n",
      "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\n",
      "\n",
      "‚úÖ Gemma 3 4B t√©l√©charg√©!\n",
      "üî• Pr√©chauffage du mod√®le...\n",
      "‚úÖ Gemma 3 4B pr√™t et op√©rationnel!\n",
      "\n",
      "============================================================\n",
      "ü§ñ OLLAMA CONFIGUR√â\n",
      "============================================================\n",
      "Mod√®le: gemma3:4b\n",
      "Serveur: http://localhost:11434\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "USE_OLLAMA = True  # Activ√© par d√©faut pour Gemma local\n",
    "\n",
    "ollama_process = None\n",
    "\n",
    "if USE_OLLAMA:\n",
    "    import subprocess\n",
    "    import time\n",
    "    import os\n",
    "\n",
    "    # Installer requests si n√©cessaire\n",
    "    try:\n",
    "        import requests\n",
    "    except ImportError:\n",
    "        !pip install -q requests\n",
    "        import requests\n",
    "\n",
    "    # Installer Ollama\n",
    "    print(\"üì¶ Installation d'Ollama...\")\n",
    "    !curl -fsSL https://ollama.com/install.sh 2>/dev/null | sh 2>&1 | tail -n 3\n",
    "\n",
    "    # D√©marrer Ollama en arri√®re-plan\n",
    "    ollama_process = subprocess.Popen(\n",
    "        [\"ollama\", \"serve\"],\n",
    "        stdout=subprocess.DEVNULL,\n",
    "        stderr=subprocess.DEVNULL\n",
    "    )\n",
    "    print(\"‚è≥ D√©marrage du serveur Ollama...\")\n",
    "\n",
    "    # Attendre que le serveur soit pr√™t (avec timeout de 30 secondes)\n",
    "    server_ready = False\n",
    "    for i in range(30):\n",
    "        try:\n",
    "            response = requests.get(\"http://localhost:11434/api/tags\", timeout=2)\n",
    "            if response.status_code == 200:\n",
    "                print(\"‚úÖ Serveur Ollama pr√™t!\")\n",
    "                server_ready = True\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "        time.sleep(1)\n",
    "\n",
    "    if not server_ready:\n",
    "        print(\"‚ö†Ô∏è Le serveur Ollama n'a pas d√©marr√© √† temps\")\n",
    "        print(\"   Essayez de r√©ex√©cuter cette cellule.\")\n",
    "\n",
    "    # T√©l√©charger Gemma 3 4B (SANS espace dans le nom!)\n",
    "    print(\"\\nüì• T√©l√©chargement de Gemma 3 4B (‚âà3GB)...\")\n",
    "    print(\"   Cela peut prendre plusieurs minutes...\")\n",
    "    !ollama pull gemma3:4b\n",
    "\n",
    "    # V√©rification\n",
    "    result = subprocess.run([\"ollama\", \"list\"], capture_output=True, text=True)\n",
    "    if \"gemma3:4b\" in result.stdout or \"gemma3\" in result.stdout:\n",
    "        print(\"\\n‚úÖ Gemma 3 4B t√©l√©charg√©!\")\n",
    "\n",
    "        # Pr√©chauffer le mod√®le avec une requ√™te test\n",
    "        print(\"üî• Pr√©chauffage du mod√®le...\")\n",
    "        test_result = subprocess.run(\n",
    "            [\"ollama\", \"run\", \"gemma3:4b\", \"R√©ponds uniquement: OK\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=30\n",
    "        )\n",
    "\n",
    "        if test_result.returncode == 0:\n",
    "            print(\"‚úÖ Gemma 3 4B pr√™t et op√©rationnel!\")\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"ü§ñ OLLAMA CONFIGUR√â\")\n",
    "            print(\"=\"*60)\n",
    "            print(\"Mod√®le: gemma3:4b\")\n",
    "            print(\"Serveur: http://localhost:11434\")\n",
    "            print(\"=\"*60)\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Avertissement: Le test du mod√®le a √©chou√©\")\n",
    "            print(\"   Le mod√®le devrait quand m√™me fonctionner.\")\n",
    "    else:\n",
    "        print(\"‚ùå Erreur: Gemma 3 4B non trouv√©\")\n",
    "        print(\"Mod√®les disponibles:\")\n",
    "        print(result.stdout if result.stdout else \"Aucun mod√®le\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Ollama d√©sactiv√©. Mettez USE_OLLAMA = True pour l'activer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f282b1f7",
   "metadata": {},
   "source": [
    "## √âtape 6 ‚Äî OCR des PDFs avec OCRmyPDF (Optionnel)\n",
    "Cette √©tape permet d'appliquer l'OCR sur tous les PDFs avant de les traiter avec Marker.\n",
    "\n",
    "**Utile pour:** PDFs scann√©s, documents sans texte s√©lectionnable\n",
    "\n",
    "**R√©sultat:** Les PDFs originaux sont remplac√©s par leur version OCRis√©e dans `ALLPDF`\n",
    "\n",
    "**‚ö†Ô∏è Cette √©tape est optionnelle.** Si vos PDFs contiennent d√©j√† du texte s√©lectionnable, vous pouvez la sauter.\n",
    "\n",
    "**Note:** Utilise OCRmyPDF + Tesseract (compatible Colab, pas besoin de Docker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eba85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION OCR ===\n",
    "USE_OCR = True  # Mettre √† False pour sauter cette √©tape\n",
    "OCR_LANGUAGES = [\"fra\", \"eng\", \"ara\"]  # Langues OCR (fra=fran√ßais, eng=anglais, ara=arabe)\n",
    "\n",
    "if USE_OCR:\n",
    "    import subprocess\n",
    "    import os\n",
    "    import shutil\n",
    "    import tempfile\n",
    "    from pathlib import Path\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"üîç OCR DES PDFS AVEC OCRMYPDF + TESSERACT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Installation des d√©pendances OCR\n",
    "    print(\"\\nüì¶ Installation des outils OCR...\")\n",
    "    \n",
    "    # Installer Tesseract, unpaper et les langues\n",
    "    !sudo apt-get update -qq\n",
    "    !sudo apt-get install -y -qq tesseract-ocr unpaper ghostscript\n",
    "    \n",
    "    # Installer les packs de langues Tesseract\n",
    "    lang_packages = \" \".join([f\"tesseract-ocr-{lang}\" for lang in OCR_LANGUAGES])\n",
    "    !sudo apt-get install -y -qq {lang_packages}\n",
    "    \n",
    "    # Installer OCRmyPDF et ses d√©pendances\n",
    "    !pip install -q ocrmypdf\n",
    "    \n",
    "    # V√©rifier l'installation\n",
    "    tesseract_check = subprocess.run([\"tesseract\", \"--version\"], capture_output=True, text=True)\n",
    "    if tesseract_check.returncode == 0:\n",
    "        version_line = tesseract_check.stdout.split('\\n')[0]\n",
    "        print(f\"‚úÖ Tesseract install√©: {version_line}\")\n",
    "    else:\n",
    "        print(\"‚ùå Erreur d'installation de Tesseract\")\n",
    "    \n",
    "    # Lister les langues disponibles\n",
    "    langs_check = subprocess.run([\"tesseract\", \"--list-langs\"], capture_output=True, text=True)\n",
    "    available_langs = langs_check.stdout.strip().split('\\n')[1:]  # Skip header\n",
    "    print(f\"üìù Langues disponibles: {', '.join(available_langs)}\")\n",
    "    \n",
    "    # V√©rifier que nos langues sont install√©es\n",
    "    missing_langs = [lang for lang in OCR_LANGUAGES if lang not in available_langs]\n",
    "    if missing_langs:\n",
    "        print(f\"‚ö†Ô∏è Langues manquantes: {missing_langs}\")\n",
    "    \n",
    "    # Importer ocrmypdf\n",
    "    import ocrmypdf\n",
    "    print(f\"‚úÖ OCRmyPDF version: {ocrmypdf.__version__}\")\n",
    "    \n",
    "    # D√©tecter le nombre de CPU disponibles\n",
    "    import multiprocessing\n",
    "    num_cpus = multiprocessing.cpu_count()\n",
    "    print(f\"üñ•Ô∏è CPUs disponibles: {num_cpus}\")\n",
    "    \n",
    "    # Configurer le dossier (source = destination)\n",
    "    PDF_DIR = Path(\"/content/drive/MyDrive/G√©opolitique et Souverainet√© Num√©riques/ALL/ALLPDF\")\n",
    "    \n",
    "    # Dossier temporaire pour les fichiers OCRis√©s\n",
    "    TEMP_OCR_DIR = Path(\"/content/temp_ocr\")\n",
    "    TEMP_OCR_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Langues pour OCRmyPDF (format: fra+eng+ara)\n",
    "    ocr_lang_str = \"+\".join(OCR_LANGUAGES)\n",
    "    \n",
    "    print(f\"\\nüìÅ Dossier PDF: {PDF_DIR}\")\n",
    "    print(f\"üîÑ Mode: Remplacement des originaux par version OCR\")\n",
    "    print(f\"üåç Langues OCR: {ocr_lang_str}\")\n",
    "    \n",
    "    # Lister les PDFs\n",
    "    pdf_files = sorted([p for p in PDF_DIR.iterdir() if p.suffix.lower() == \".pdf\"])\n",
    "    print(f\"\\nüìÑ {len(pdf_files)} PDFs √† traiter\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    success_count = 0\n",
    "    already_ocr_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    for idx, pdf_path in enumerate(pdf_files, 1):\n",
    "        pdf_name = pdf_path.stem\n",
    "        temp_output = TEMP_OCR_DIR / f\"{pdf_name}.pdf\"\n",
    "        \n",
    "        print(f\"üîç [{idx}/{len(pdf_files)}] {pdf_name}...\", end=\" \", flush=True)\n",
    "        \n",
    "        try:\n",
    "            # Appliquer OCR avec OCRmyPDF vers fichier temporaire\n",
    "            result = ocrmypdf.ocr(\n",
    "                pdf_path,\n",
    "                temp_output,\n",
    "                language=ocr_lang_str,\n",
    "                skip_text=True,  # Ne pas r√©-OCR les pages avec du texte\n",
    "                optimize=0,      # Pas d'optimisation (plus rapide)\n",
    "                progress_bar=False,\n",
    "                jobs=num_cpus,   # Utiliser tous les CPUs\n",
    "                tesseract_timeout=180,  # Timeout par page\n",
    "                tesseract_non_ocr_timeout=30\n",
    "            )\n",
    "            \n",
    "            if result == ocrmypdf.ExitCode.ok:\n",
    "                # Remplacer l'original par la version OCR\n",
    "                shutil.move(str(temp_output), str(pdf_path))\n",
    "                print(\"‚úÖ (remplac√©)\")\n",
    "                success_count += 1\n",
    "            elif result == ocrmypdf.ExitCode.already_done_ocr:\n",
    "                # D√©j√† OCRis√©, supprimer le temp si cr√©√©\n",
    "                if temp_output.exists():\n",
    "                    temp_output.unlink()\n",
    "                print(\"üìã (d√©j√† OCRis√©)\")\n",
    "                already_ocr_count += 1\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è (code: {result}, conserv√©)\")\n",
    "                if temp_output.exists():\n",
    "                    temp_output.unlink()\n",
    "                already_ocr_count += 1\n",
    "                \n",
    "        except ocrmypdf.exceptions.PriorOcrFoundError:\n",
    "            # Le PDF a d√©j√† du texte OCR\n",
    "            if temp_output.exists():\n",
    "                temp_output.unlink()\n",
    "            print(\"üìã (texte existant)\")\n",
    "            already_ocr_count += 1\n",
    "        except ocrmypdf.exceptions.EncryptedPdfError:\n",
    "            print(\"üîí (PDF prot√©g√©)\")\n",
    "            error_count += 1\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)[:60]\n",
    "            print(f\"‚ùå {error_msg}\")\n",
    "            error_count += 1\n",
    "            # Nettoyer le fichier temp en cas d'erreur\n",
    "            if temp_output.exists():\n",
    "                temp_output.unlink()\n",
    "    \n",
    "    # Nettoyer le dossier temporaire\n",
    "    shutil.rmtree(TEMP_OCR_DIR, ignore_errors=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä R√âSUM√â OCR\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"‚úÖ OCR appliqu√© et remplac√©: {success_count}\")\n",
    "    print(f\"üìã D√©j√† OCRis√© (inchang√©): {already_ocr_count}\")\n",
    "    print(f\"‚ùå Erreurs: {error_count}\")\n",
    "    print(f\"üìÇ Dossier: {PDF_DIR}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è OCR d√©sactiv√©. Mettez USE_OCR = True pour l'activer.\")\n",
    "    print(\"   Les PDFs seront utilis√©s tels quels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ae817d",
   "metadata": {
    "id": "04ae817d"
   },
   "source": [
    "## √âtape 7 ‚Äî Configurer Marker\n",
    "Chargement des mod√®les Marker (~4GB) depuis le cache Drive.\n",
    "\n",
    "**‚ö° Premier lancement:** 10-20 minutes (t√©l√©chargement + sauvegarde sur Drive)\n",
    "\n",
    "**üöÄ Lancements suivants:** 1-2 minutes (chargement depuis Drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0f4c3b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ca0f4c3b",
    "outputId": "65415dac-6c19-4a09-8e44-e84604507857"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üñ•Ô∏è CONFIGURATION MAT√âRIELLE\n",
      "============================================================\n",
      "Device: cuda\n",
      "GPU: Tesla T4\n",
      "VRAM: 15.8 GB\n",
      "\n",
      "üíæ Cache Drive: 3.53 GB\n",
      "üöÄ Mod√®les trouv√©s en cache! Chargement rapide...\n",
      "============================================================\n",
      "\n",
      "üì¶ Chargement des mod√®les Marker...\n",
      "   ‚ö° Chargement depuis le cache Drive...\n",
      "‚ö†Ô∏è Impossible d'importer OpenAILLMService. Le LLM sera d√©sactiv√© pour Marker.\n",
      "\n",
      "‚öôÔ∏è Cr√©ation du dictionnaire de mod√®les...\n",
      "‚úì Configuration du convertisseur...\n",
      "\n",
      "============================================================\n",
      "‚úÖ MARKER CONFIGUR√â AVEC SUCC√àS!\n",
      "============================================================\n",
      "‚è±Ô∏è Dur√©e: 0.4 minutes\n",
      "üì¶ T√©l√©charg√© cette session: 0.00 GB\n",
      "üíæ Cache total sur Drive: 3.53 GB\n",
      "üñºÔ∏è Extraction d'images: D√©sactiv√©e\n",
      "ü§ñ LLM: D√©sactiv√©\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import shutil\n",
    "import base64\n",
    "import logging\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# === OPTION: EXTRACTION D'IMAGES ===\n",
    "EXTRACT_IMAGES = True  # ACTIV√â pour extraire les images/figures\n",
    "\n",
    "# === CONFIGURATION DES LOGS ===\n",
    "# D√©sactiver les warnings non critiques\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Configurer le logging pour r√©duire la verbosit√©\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "# R√©duire drastiquement les logs des biblioth√®ques tierces\n",
    "for logger_name in ['transformers', 'torch', 'PIL', 'urllib3', 'filelock',\n",
    "                     'huggingface_hub', 'marker', 'datasets']:\n",
    "    logging.getLogger(logger_name).setLevel(logging.ERROR)\n",
    "\n",
    "# Variables d'environnement pour r√©duire les logs\n",
    "os.environ['TRANSFORMERS_VERBOSITY'] = 'error'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '0'  # Garder les barres de progression\n",
    "\n",
    "# S'assurer que le cache Drive est bien configur√©\n",
    "os.environ[\"HF_HOME\"] = str(HF_CACHE_DRIVE)\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = str(HF_CACHE_DRIVE)\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = str(HF_CACHE_DRIVE)\n",
    "os.environ[\"TORCH_HOME\"] = str(TORCH_CACHE_DRIVE)\n",
    "# Forcer HF Transfer\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "# === FORCER LE T√âL√âCHARGEMENT DES MOD√àLES SURYA DEPUIS HUGGING FACE ===\n",
    "os.environ[\"SURYA_LAYOUT_MODEL\"] = \"vikp/surya_layout\"\n",
    "os.environ[\"SURYA_REC_MODEL\"] = \"vikp/surya_rec\"\n",
    "os.environ[\"SURYA_DET_MODEL\"] = \"vikp/surya_det\"\n",
    "os.environ[\"SURYA_ORDER_MODEL\"] = \"vikp/surya_order\"\n",
    "os.environ[\"SURYA_TABLE_REC_MODEL\"] = \"vikp/surya_tablerec\"\n",
    "os.environ[\"SURYA_DOWNLOAD_BACKEND\"] = \"huggingface\"\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"=\"*60)\n",
    "print(\"üñ•Ô∏è CONFIGURATION MAT√âRIELLE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# V√©rifier le cache existant\n",
    "def get_dir_size(path):\n",
    "    total = 0\n",
    "    if path.exists():\n",
    "        for f in path.rglob(\"*\"):\n",
    "            if f.is_file():\n",
    "                try:\n",
    "                    total += f.stat().st_size\n",
    "                except:\n",
    "                    pass\n",
    "    return total / 1e9\n",
    "\n",
    "initial_cache_size = get_dir_size(DRIVE_CACHE_DIR)\n",
    "print(f\"\\nüíæ Cache Drive: {initial_cache_size:.2f} GB\")\n",
    "\n",
    "if initial_cache_size > 3:\n",
    "    print(\"üöÄ Mod√®les trouv√©s en cache! Chargement rapide...\")\n",
    "else:\n",
    "    print(\"üì• Premier t√©l√©chargement des mod√®les Marker (~4GB)\")\n",
    "    print(\"   ‚è±Ô∏è Dur√©e estim√©e: 10-20 minutes\")\n",
    "    print(\"   üíæ Les mod√®les seront sauvegard√©s sur Drive\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Chronom√®tre\n",
    "start_time = time.time()\n",
    "\n",
    "# Charger les mod√®les avec messages de progression\n",
    "print(\"\\nüì¶ Chargement des mod√®les Marker...\")\n",
    "if initial_cache_size > 3:\n",
    "    print(\"   ‚ö° Chargement depuis le cache Drive...\")\n",
    "else:\n",
    "    print(\"   üì• T√©l√©chargement en cours...\")\n",
    "\n",
    "from marker.converters.pdf import PdfConverter\n",
    "from marker.models import create_model_dict\n",
    "from marker.config.parser import ConfigParser\n",
    "\n",
    "# Tenter d'importer le service LLM appropri√©\n",
    "LLM_SERVICE_CLS = None\n",
    "USE_OLLAMA = True\n",
    "\n",
    "if USE_OLLAMA:\n",
    "    try:\n",
    "        from marker.llm.openai import OpenAILLMService\n",
    "        LLM_SERVICE_CLS = OpenAILLMService\n",
    "        print(\"‚úÖ Service LLM: OpenAILLMService (compatible Ollama)\")\n",
    "    except ImportError:\n",
    "        try:\n",
    "            from marker.services.openai import OpenAILLMService\n",
    "            LLM_SERVICE_CLS = OpenAILLMService\n",
    "            print(\"‚úÖ Service LLM: OpenAILLMService (via path alternatif)\")\n",
    "        except ImportError:\n",
    "            print(\"‚ö†Ô∏è Impossible d'importer OpenAILLMService. Le LLM sera d√©sactiv√© pour Marker.\")\n",
    "            USE_OLLAMA = False\n",
    "\n",
    "# Configuration pour Ollama via API OpenAI\n",
    "if USE_OLLAMA:\n",
    "    os.environ[\"MARKER_LLM_PROVIDER\"] = \"openai\"\n",
    "    os.environ[\"OPENAI_BASE_URL\"] = \"http://localhost:11434/v1\"\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"ollama\"\n",
    "    os.environ[\"OPENAI_MODEL\"] = \"gemma3:4b\"\n",
    "\n",
    "# Configuration Marker OPTIMIS√âE pour extraction compl√®te\n",
    "marker_config = {\n",
    "    \"workers\": 2,\n",
    "    \"extract_images\": EXTRACT_IMAGES,\n",
    "    \"images_as_base64\": True,  # R√©cup√©rer les images en base64 pour √©viter les erreurs de fichiers\n",
    "    \"use_llm\": USE_OLLAMA,\n",
    "    \"llm_provider\": \"openai\" if USE_OLLAMA else None,\n",
    "    \"llm_model\": \"gemma3:4b\" if USE_OLLAMA else None,\n",
    "    \"force_ocr\": False,\n",
    "    \"languages\": [\"fr\", \"en\"],\n",
    "    \"paginate_output\": True,\n",
    "    \"batch_size\": 4 if device == \"cuda\" else 2,\n",
    "    # Options suppl√©mentaires pour am√©liorer l'extraction\n",
    "    \"output_format\": \"markdown\",\n",
    "}\n",
    "\n",
    "print(\"\\n‚öôÔ∏è Cr√©ation du dictionnaire de mod√®les...\")\n",
    "model_dict = create_model_dict()\n",
    "\n",
    "print(\"‚úì Configuration du convertisseur...\")\n",
    "config_parser = ConfigParser(marker_config)\n",
    "\n",
    "converter = PdfConverter(\n",
    "    config=config_parser.generate_config_dict(),\n",
    "    artifact_dict=model_dict\n",
    ")\n",
    "\n",
    "# R√©sum√©\n",
    "elapsed = time.time() - start_time\n",
    "final_cache_size = get_dir_size(DRIVE_CACHE_DIR)\n",
    "downloaded = final_cache_size - initial_cache_size\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ MARKER CONFIGUR√â AVEC SUCC√àS!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚è±Ô∏è Dur√©e: {elapsed/60:.1f} minutes\")\n",
    "print(f\"üì¶ T√©l√©charg√© cette session: {max(0, downloaded):.2f} GB\")\n",
    "print(f\"üíæ Cache total sur Drive: {final_cache_size:.2f} GB\")\n",
    "print(f\"üñºÔ∏è Extraction d'images: {'Activ√©e' if EXTRACT_IMAGES else 'D√©sactiv√©e'}\")\n",
    "if USE_OLLAMA:\n",
    "    print(f\"ü§ñ LLM: Gemma 3 4B via Ollama (interface OpenAI)\")\n",
    "else:\n",
    "    print(\"ü§ñ LLM: D√©sactiv√©\")\n",
    "print(\"=\"*60)\n",
    "if downloaded > 0.5:\n",
    "    print(\"\\nüí° Les mod√®les sont maintenant en cache sur votre Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0fc302",
   "metadata": {
    "id": "dd0fc302"
   },
   "source": [
    "## √âtape 8 ‚Äî Fonctions utilitaires\n",
    "Extraction des r√©f√©rences, figures et conversion PDF ‚Üí Markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a23cbb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e5a23cbb",
    "outputId": "5bfbe08f-29e4-4e54-96aa-6cde13e03460"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fonctions utilitaires d√©finies. \n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import fitz  # PyMuPDF pour compter les pages\n",
    "\n",
    "def get_pdf_page_count(pdf_path):\n",
    "    \"\"\"Retourne le nombre de pages d'un PDF.\"\"\"\n",
    "    try:\n",
    "        with fitz.open(str(pdf_path)) as doc:\n",
    "            return len(doc)\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Lib√®re la m√©moire GPU et RAM.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def create_converter_for_large_pdf(page_count):\n",
    "    \"\"\"Cr√©e un convertisseur optimis√© pour les gros PDFs.\"\"\"\n",
    "    # Ajuster les param√®tres selon la taille\n",
    "    if page_count > 300:\n",
    "        batch = 1\n",
    "        extract_img = False\n",
    "        use_llm = False\n",
    "    elif page_count > 200:\n",
    "        batch = 2\n",
    "        extract_img = False\n",
    "        use_llm = False\n",
    "    elif page_count > 100:\n",
    "        batch = 2 if device == \"cuda\" else 1\n",
    "        extract_img = EXTRACT_IMAGES\n",
    "        use_llm = False\n",
    "    else:\n",
    "        return None  # Utiliser le convertisseur par d√©faut\n",
    "\n",
    "    config = {\n",
    "        \"workers\": 1,  # R√©duire les workers\n",
    "        \"extract_images\": extract_img,\n",
    "        \"images_as_base64\": extract_img,\n",
    "        \"use_llm\": use_llm,\n",
    "        \"force_ocr\": False,\n",
    "        \"languages\": [\"fr\", \"en\"],\n",
    "        \"paginate_output\": True,\n",
    "        \"batch_size\": batch,\n",
    "        \"output_format\": \"markdown\",\n",
    "    }\n",
    "\n",
    "    config_parser = ConfigParser(config)\n",
    "    return PdfConverter(\n",
    "        config=config_parser.generate_config_dict(),\n",
    "        artifact_dict=model_dict\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_references_from_markdown(markdown_text):\n",
    "    \"\"\"Extrait la section r√©f√©rences/bibliographie du Markdown avec patterns tr√®s am√©lior√©s.\"\"\"\n",
    "    references = {\n",
    "        \"references_text\": \"\",\n",
    "        \"references_list\": [],\n",
    "        \"reference_count\": 0,\n",
    "    }\n",
    "\n",
    "    # Normaliser le texte (supprimer les sauts de ligne multiples excessifs)\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', markdown_text)\n",
    "\n",
    "    # === √âTAPE 1: Trouver la section r√©f√©rences ===\n",
    "    ref_section = \"\"\n",
    "\n",
    "    # Patterns pour trouver le D√âBUT de la section r√©f√©rences\n",
    "    section_start_patterns = [\n",
    "        r\"(?i)(?:^|\\n)#{1,4}\\s*(references?|r√©f√©rences?|bibliography|bibliographie|works?\\s*cited|cited\\s*works?|literature|sources?|notes?\\s*(?:and\\s*)?references?)\\s*\\n\",\n",
    "        r\"(?i)(?:^|\\n)\\*\\*(references?|r√©f√©rences?|bibliography|bibliographie)\\*\\*\\s*\\n\",\n",
    "        r\"(?i)(?:^|\\n)(REFERENCES?|R√âF√âRENCES?|BIBLIOGRAPHY|BIBLIOGRAPHIE|WORKS\\s*CITED)\\s*\\n\",\n",
    "        r\"(?i)(?:^|\\n)_{2,}?\\s*(references?|bibliography)\\s*_{2,}?\\s*\\n\",\n",
    "    ]\n",
    "\n",
    "    section_start_pos = -1\n",
    "    for pattern in section_start_patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            section_start_pos = match.end()\n",
    "            break\n",
    "\n",
    "    if section_start_pos > 0:\n",
    "        remaining = text[section_start_pos:]\n",
    "        end_patterns = [\n",
    "            r\"\\n#{1,3}\\s+[A-Z]\",\n",
    "            r\"\\n\\*\\*[A-Z][a-z]+\\*\\*\\s*\\n\",\n",
    "            r\"\\n(?:APPENDIX|ANNEXE|ACKNOWLEDGMENT)\",\n",
    "        ]\n",
    "\n",
    "        end_pos = len(remaining)\n",
    "        for pattern in end_patterns:\n",
    "            match = re.search(pattern, remaining, re.IGNORECASE)\n",
    "            if match and match.start() < end_pos:\n",
    "                end_pos = match.start()\n",
    "\n",
    "        ref_section = remaining[:end_pos].strip()\n",
    "\n",
    "    # === √âTAPE 2: Si pas de section trouv√©e, chercher dans les derniers 30% ===\n",
    "    if not ref_section or len(ref_section) < 100:\n",
    "        last_portion = text[int(len(text) * 0.7):]\n",
    "        ref_block_pattern = r\"(?:^|\\n)((?:\\[\\d+\\]|\\d+\\.|[A-Z][a-z]+,?\\s+[A-Z])[^\\n]+(?:\\n(?!\\[\\d+\\]|\\d+\\.|[A-Z][a-z]+,\\s+[A-Z])[^\\n]+)*)\"\n",
    "        blocks = re.findall(ref_block_pattern, last_portion)\n",
    "\n",
    "        if len(blocks) >= 5:\n",
    "            ref_section = \"\\n\".join(blocks)\n",
    "\n",
    "    # === √âTAPE 3: Extraction des r√©f√©rences individuelles ===\n",
    "    ref_lines = []\n",
    "\n",
    "    if ref_section:\n",
    "        references[\"references_text\"] = ref_section[:5000]\n",
    "\n",
    "        lines = ref_section.split('\\n')\n",
    "        current_ref = \"\"\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if current_ref and len(current_ref) > 40:\n",
    "                    ref_lines.append(current_ref)\n",
    "                current_ref = \"\"\n",
    "                continue\n",
    "\n",
    "            is_new_ref = False\n",
    "            if re.match(r\"^\\[\\d+\\]\", line) or re.match(r\"^\\d+\\.\\s+[A-Z]\", line):\n",
    "                is_new_ref = True\n",
    "            elif re.match(r\"^[A-Z][a-z]+(?:[-'][A-Z][a-z]+)?,?\\s+[A-Z]\\..*\\(\\d{4}\\)\", line):\n",
    "                is_new_ref = True\n",
    "            elif re.match(r\"^[A-Z][a-z]+,?\\s+[A-Z]\\.\\s*(?:&|and|,)\\s*[A-Z]\", line):\n",
    "                is_new_ref = True\n",
    "            elif re.match(r\"^[-‚Ä¢‚óè‚óã]\\s+[A-Z]\", line):\n",
    "                is_new_ref = True\n",
    "            elif re.match(r\"^[A-Z][a-z]+\\s+et\\s+al\\.\", line):\n",
    "                is_new_ref = True\n",
    "\n",
    "            if is_new_ref:\n",
    "                if current_ref and len(current_ref) > 40:\n",
    "                    ref_lines.append(current_ref)\n",
    "                current_ref = line\n",
    "            elif current_ref:\n",
    "                current_ref += \" \" + line\n",
    "            elif len(line) > 50 and re.search(r'\\d{4}', line):\n",
    "                current_ref = line\n",
    "\n",
    "        if current_ref and len(current_ref) > 40:\n",
    "            ref_lines.append(current_ref)\n",
    "\n",
    "    # === √âTAPE 4: Extraction alternative ===\n",
    "    if len(ref_lines) < 3:\n",
    "        inline_patterns = [\n",
    "            r\"\\[\\d+\\]\\s+[A-Z][a-z]+(?:,?\\s+[A-Z]\\.)+[^.]+\\.\\s+[^.]+\\.\\s+\\d{4}\",\n",
    "            r\"[A-Z][a-z]+,?\\s+[A-Z]\\.(?:\\s*[A-Z]\\.)*\\s*\\(\\d{4}\\)\\.\\s+[^.]+\\.[^.]+\\.\",\n",
    "        ]\n",
    "\n",
    "        for pattern in inline_patterns:\n",
    "            matches = re.findall(pattern, text)\n",
    "            for m in matches:\n",
    "                if m not in ref_lines and len(m) > 50:\n",
    "                    ref_lines.append(m.strip())\n",
    "\n",
    "    # === √âTAPE 5: Nettoyage et d√©duplication ===\n",
    "    seen = set()\n",
    "    unique_refs = []\n",
    "    for ref in ref_lines:\n",
    "        ref = re.sub(r'\\s+', ' ', ref).strip()\n",
    "        ref = re.sub(r'^[\\[\\d+\\]|\\d+\\.|\\-‚Ä¢‚óè]\\s*', '', ref).strip()\n",
    "        key = ref[:80].lower()\n",
    "        if key not in seen and len(ref) > 40:\n",
    "            seen.add(key)\n",
    "            unique_refs.append(ref)\n",
    "\n",
    "    references[\"references_list\"] = unique_refs[:200]\n",
    "    references[\"reference_count\"] = len(unique_refs)\n",
    "\n",
    "    return references\n",
    "\n",
    "\n",
    "def extract_figures_info(markdown_text, images_dict):\n",
    "    \"\"\"Extrait les informations sur les figures du document.\"\"\"\n",
    "    figures = []\n",
    "    seen_labels = set()\n",
    "\n",
    "    fig_patterns = [\n",
    "        r\"(?i)(?:^|\\n)\\s*((?:figure|fig\\.?)\\s*(\\d+(?:\\.\\d+)?)[:\\s.]*([^\\n]{0,200}))\",\n",
    "        r\"(?i)(?:^|\\n)\\s*((?:figure|fig\\.?)\\s*(\\d+(?:\\.\\d+)?)\\s*[‚Äî‚Äì-]\\s*([^\\n]{0,200}))\",\n",
    "        r\"(?i)(?:^|\\n)\\s*((?:table(?:au)?)\\s*(\\d+(?:\\.\\d+)?)[:\\s.]*([^\\n]{0,200}))\",\n",
    "        r\"(?i)(?:^|\\n)\\s*((?:chart|graph|diagram|graphique|sch√©ma)\\s*(\\d+(?:\\.\\d+)?)?[:\\s.]*([^\\n]{0,150}))\",\n",
    "    ]\n",
    "\n",
    "    for pattern in fig_patterns:\n",
    "        for match in re.finditer(pattern, markdown_text, re.MULTILINE):\n",
    "            groups = match.groups()\n",
    "            if len(groups) >= 1:\n",
    "                full_match = groups[0] if groups[0] else match.group(0)\n",
    "                fig_num = groups[1] if len(groups) > 1 and groups[1] else \"\"\n",
    "                title = groups[2].strip() if len(groups) > 2 and groups[2] else \"\"\n",
    "                title = re.sub(r'^[:\\s.‚Äî‚Äì-]+', '', title).strip()\n",
    "                label = full_match.strip()[:100]\n",
    "                if label.lower() not in seen_labels:\n",
    "                    seen_labels.add(label.lower())\n",
    "                    figures.append({\n",
    "                        \"label\": label,\n",
    "                        \"number\": fig_num,\n",
    "                        \"title\": title,\n",
    "                        \"type\": \"figure\",\n",
    "                    })\n",
    "\n",
    "    img_md_pattern = r\"!\\[([^\\]]*)\\]\\(([^)]+)\\)\"\n",
    "    for match in re.finditer(img_md_pattern, markdown_text):\n",
    "        alt_text = match.group(1).strip()\n",
    "        img_src = match.group(2).strip()\n",
    "        if alt_text and alt_text.lower() not in seen_labels:\n",
    "            seen_labels.add(alt_text.lower())\n",
    "            figures.append({\n",
    "                \"label\": alt_text or f\"Image: {img_src[:50]}\",\n",
    "                \"title\": alt_text,\n",
    "                \"path\": img_src,\n",
    "                \"type\": \"embedded_image\",\n",
    "            })\n",
    "\n",
    "    if images_dict:\n",
    "        for idx, img_name in enumerate(images_dict.keys(), 1):\n",
    "            img_label = str(img_name)\n",
    "            if img_label.lower() not in seen_labels:\n",
    "                seen_labels.add(img_label.lower())\n",
    "                figures.append({\n",
    "                    \"label\": img_label,\n",
    "                    \"title\": \"\",\n",
    "                    \"path\": img_label,\n",
    "                    \"type\": \"extracted_image\",\n",
    "                    \"index\": idx,\n",
    "                })\n",
    "\n",
    "    return figures\n",
    "\n",
    "\n",
    "def save_figures(images_dict, doc_folders):\n",
    "    \"\"\"Sauvegarde les figures extraites.\"\"\"\n",
    "    if not images_dict:\n",
    "        return []\n",
    "\n",
    "    figures_folder = doc_folders[\"figures\"]\n",
    "    saved_paths = []\n",
    "    used_names = set()\n",
    "\n",
    "    for idx, (img_name, img_data) in enumerate(images_dict.items(), 1):\n",
    "        safe_name = re.sub(r\"[^a-zA-Z0-9_-]+\", \"_\", str(img_name))\n",
    "        safe_name = re.sub(r\"_+\", \"_\", safe_name).strip(\"_\")[:100]\n",
    "        if not safe_name or len(safe_name) < 3:\n",
    "            safe_name = f\"figure_{idx:03d}\"\n",
    "\n",
    "        base_name = safe_name\n",
    "        counter = 1\n",
    "        while safe_name in used_names:\n",
    "            safe_name = f\"{base_name}_{counter}\"\n",
    "            counter += 1\n",
    "        used_names.add(safe_name)\n",
    "\n",
    "        img_path = figures_folder / f\"{safe_name}.png\"\n",
    "\n",
    "        try:\n",
    "            if isinstance(img_data, Image.Image):\n",
    "                if img_data.mode in ('RGBA', 'LA', 'P'):\n",
    "                    rgb_image = Image.new('RGB', img_data.size, (255, 255, 255))\n",
    "                    if img_data.mode == 'P':\n",
    "                        img_data = img_data.convert('RGBA')\n",
    "                    rgb_image.paste(img_data, mask=img_data.split()[-1] if img_data.mode in ('RGBA', 'LA') else None)\n",
    "                    img_data = rgb_image\n",
    "                img_data.save(img_path, \"PNG\", optimize=True)\n",
    "            elif isinstance(img_data, (bytes, bytearray)):\n",
    "                with open(img_path, \"wb\") as f:\n",
    "                    f.write(img_data)\n",
    "            elif isinstance(img_data, str):\n",
    "                if img_data.startswith(\"data:image\"):\n",
    "                    b64_data = img_data.split(\",\", 1)[1] if \",\" in img_data else img_data\n",
    "                    with open(img_path, \"wb\") as f:\n",
    "                        f.write(base64.b64decode(b64_data))\n",
    "                elif Path(img_data).exists():\n",
    "                    shutil.copy(img_data, img_path)\n",
    "                else:\n",
    "                    try:\n",
    "                        with open(img_path, \"wb\") as f:\n",
    "                            f.write(base64.b64decode(img_data))\n",
    "                    except:\n",
    "                        continue\n",
    "            else:\n",
    "                continue\n",
    "            saved_paths.append(str(img_path))\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    return saved_paths\n",
    "\n",
    "\n",
    "def extract_metadata_from_markdown(markdown_text):\n",
    "    \"\"\"Extrait les m√©tadonn√©es du document.\"\"\"\n",
    "    metadata = {\"title\": \"\", \"authors\": [], \"abstract\": \"\", \"keywords\": [], \"date\": \"\"}\n",
    "\n",
    "    title_patterns = [\n",
    "        r\"^#\\s+([^\\n]+)\",\n",
    "        r\"^\\*\\*([^\\*\\n]{10,150})\\*\\*\",\n",
    "        r\"^([A-Z][^\\n]{20,150})\\n[=]+\",\n",
    "    ]\n",
    "    for pattern in title_patterns:\n",
    "        match = re.search(pattern, markdown_text[:2000], re.MULTILINE)\n",
    "        if match:\n",
    "            metadata[\"title\"] = match.group(1).strip()\n",
    "            break\n",
    "\n",
    "    abstract_pattern = r\"(?i)(?:abstract|r√©sum√©|summary)[:\\s]*\\n?([\\s\\S]{50,1500}?)(?=\\n\\n|\\n#{1,3}|\\n\\*\\*[A-Z])\"\n",
    "    match = re.search(abstract_pattern, markdown_text[:5000])\n",
    "    if match:\n",
    "        metadata[\"abstract\"] = match.group(1).strip()\n",
    "\n",
    "    keywords_pattern = r\"(?i)(?:keywords?|mots[- ]?cl√©s?)[:\\s]*([^\\n]+)\"\n",
    "    match = re.search(keywords_pattern, markdown_text[:5000])\n",
    "    if match:\n",
    "        kw_text = match.group(1)\n",
    "        keywords = re.split(r'[,;‚Ä¢¬∑]', kw_text)\n",
    "        metadata[\"keywords\"] = [k.strip() for k in keywords if k.strip() and len(k.strip()) > 2]\n",
    "\n",
    "    dates = re.findall(r\"\\b((?:19|20)\\d{2})\\b\", markdown_text[:3000])\n",
    "    if dates:\n",
    "        metadata[\"date\"] = dates[0]\n",
    "\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def convert_pdf_complete(pdf_path, doc_name):\n",
    "    \"\"\"Conversion compl√®te d'un PDF avec gestion m√©moire optimis√©e.\"\"\"\n",
    "    doc_folders = get_doc_folders(doc_name)\n",
    "\n",
    "    result_data = {\n",
    "        \"doc_name\": doc_name,\n",
    "        \"doc_folder\": str(doc_folders[\"root\"]),\n",
    "        \"markdown_path\": \"\",\n",
    "        \"figures\": [],\n",
    "        \"figures_paths\": [],\n",
    "        \"references\": {},\n",
    "        \"metadata\": {},\n",
    "        \"page_count\": 0,\n",
    "        \"error\": None,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Compter les pages pour adapter le traitement\n",
    "        page_count = get_pdf_page_count(pdf_path)\n",
    "        result_data[\"page_count\"] = page_count\n",
    "\n",
    "        # Choisir le convertisseur adapt√©\n",
    "        if page_count > 100:\n",
    "            print(f\"   ‚ö†Ô∏è Document volumineux ({page_count} pages) - Mode optimis√© m√©moire\")\n",
    "            current_converter = create_converter_for_large_pdf(page_count)\n",
    "            if current_converter is None:\n",
    "                current_converter = converter\n",
    "        else:\n",
    "            current_converter = converter\n",
    "\n",
    "        print(f\"   üìÑ Conversion de {pdf_path.name} ({page_count} pages)...\")\n",
    "\n",
    "        # Lib√©rer la m√©moire avant conversion\n",
    "        clear_memory()\n",
    "\n",
    "        try:\n",
    "            result = current_converter(str(pdf_path))\n",
    "        except Exception as conv_error:\n",
    "            error_msg = str(conv_error).lower()\n",
    "            if \"extension\" in error_msg or \"image\" in error_msg or \"unknown\" in error_msg or \"memory\" in error_msg:\n",
    "                print(f\"   ‚ö†Ô∏è Erreur, nouvelle tentative en mode minimal...\")\n",
    "                clear_memory()\n",
    "\n",
    "                temp_config = {\n",
    "                    \"workers\": 1,\n",
    "                    \"extract_images\": False,\n",
    "                    \"images_as_base64\": False,\n",
    "                    \"use_llm\": False,\n",
    "                    \"force_ocr\": False,\n",
    "                    \"languages\": [\"fr\", \"en\"],\n",
    "                    \"paginate_output\": True,\n",
    "                    \"batch_size\": 1,\n",
    "                }\n",
    "                config_parser = ConfigParser(temp_config)\n",
    "                temp_converter = PdfConverter(\n",
    "                    config=config_parser.generate_config_dict(),\n",
    "                    artifact_dict=model_dict,\n",
    "                    llm_service=None\n",
    "                )\n",
    "                result = temp_converter(str(pdf_path))\n",
    "                del temp_converter\n",
    "                clear_memory()\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        markdown_text = getattr(result, \"markdown\", \"\") or \"\"\n",
    "        images_dict = getattr(result, \"images\", {}) or {}\n",
    "\n",
    "        print(f\"   üìä Markdown: {len(markdown_text)} caract√®res\")\n",
    "        print(f\"   üñºÔ∏è Images brutes de Marker: {len(images_dict)}\")\n",
    "\n",
    "        # Nettoyer les images\n",
    "        cleaned_images = {}\n",
    "        if images_dict:\n",
    "            for key, value in images_dict.items():\n",
    "                if key and value is not None:\n",
    "                    if not any(char in str(key) for char in ['?', '*', '<', '>', '|', '\\x00']):\n",
    "                        cleaned_images[key] = value\n",
    "\n",
    "        images_dict = cleaned_images\n",
    "\n",
    "        # Sauvegarder le Markdown\n",
    "        md_path = doc_folders[\"root\"] / f\"{doc_name}.md\"\n",
    "        with open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(markdown_text)\n",
    "\n",
    "        result_data[\"markdown_path\"] = str(md_path)\n",
    "\n",
    "        # Extractions\n",
    "        result_data[\"figures_paths\"] = save_figures(images_dict, doc_folders)\n",
    "        result_data[\"figures\"] = extract_figures_info(markdown_text, images_dict)\n",
    "        result_data[\"references\"] = extract_references_from_markdown(markdown_text)\n",
    "        result_data[\"metadata\"] = extract_metadata_from_markdown(markdown_text)\n",
    "\n",
    "        # Lib√©rer les variables lourdes\n",
    "        del markdown_text, images_dict, cleaned_images, result\n",
    "        clear_memory()\n",
    "\n",
    "        # Sauvegarder les r√©sultats\n",
    "        if result_data[\"references\"].get(\"reference_count\", 0) > 0:\n",
    "            ref_path = doc_folders[\"references\"] / f\"{doc_name}_references.json\"\n",
    "            with open(ref_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(result_data[\"references\"], f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        if result_data[\"metadata\"].get(\"title\"):\n",
    "            meta_path = doc_folders[\"analyses\"] / f\"{doc_name}_metadata.json\"\n",
    "            with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(result_data[\"metadata\"], f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        return result_data\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        result_data[\"error\"] = str(e)\n",
    "        error_log = doc_folders[\"logs\"] / \"error.txt\"\n",
    "        with open(error_log, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"Erreur: {str(e)}\\nTraceback:\\n{traceback.format_exc()}\\nDate: {datetime.now().isoformat()}\")\n",
    "        clear_memory()\n",
    "        return result_data\n",
    "\n",
    "\n",
    "print(\"‚úÖ Fonctions utilitaires optimis√©es d√©finies.\")\n",
    "print(\"   ‚Ä¢ Gestion m√©moire: lib√©ration automatique GPU/RAM\")\n",
    "print(\"   ‚Ä¢ Gros fichiers (>100 pages): mode optimis√© automatique\")\n",
    "print(\"   ‚Ä¢ Extraction de r√©f√©rences: patterns acad√©miques am√©lior√©s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f235df66",
   "metadata": {
    "id": "f235df66"
   },
   "source": [
    "## √âtape 9 ‚Äî LangExtract\n",
    "Extraction structur√©e avec LangExtract (activ√© par d√©faut avec Ollama)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4261fafe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4261fafe",
    "outputId": "eda7642b-8178-449d-9149-47e9b364f935"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ollama d√©tect√© - Utilisation de Gemma 3 4B\n",
      "‚úÖ LangExtract configur√© (Provider: ollama, Model: gemma3:4b)\n"
     ]
    }
   ],
   "source": [
    "USE_LANGEXTRACT = True\n",
    "\n",
    "# V√©rifier si Ollama est disponible (ind√©pendamment de la config Marker)\n",
    "OLLAMA_AVAILABLE = False\n",
    "try:\n",
    "    import requests\n",
    "    response = requests.get(\"http://localhost:11434/api/tags\", timeout=2)\n",
    "    if response.status_code == 200:\n",
    "        OLLAMA_AVAILABLE = True\n",
    "        print(\"‚úÖ Ollama d√©tect√© - Utilisation de Gemma 3 4B\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Ollama non disponible - LangExtract utilisera l'API directe\")\n",
    "\n",
    "LANGEXTRACT_CONFIG = {\n",
    "    \"provider\": \"ollama\" if OLLAMA_AVAILABLE else \"openai\",\n",
    "    \"model\": \"gemma3:4b\" if OLLAMA_AVAILABLE else \"gpt-3.5-turbo\",\n",
    "    \"base_url\": \"http://localhost:11434\" if OLLAMA_AVAILABLE else None,\n",
    "}\n",
    "\n",
    "# Prompt pour l'extraction structur√©e (utilis√© avec Ollama directement si LangExtract √©choue)\n",
    "EXTRACTION_PROMPT = \"\"\"Analysez ce document acad√©mique et retournez un JSON structur√© avec:\n",
    "{\n",
    "  \"contexte\": {\"theme\": \"\", \"zone_geographique\": \"\", \"periode\": \"\"},\n",
    "  \"acteurs\": {\"institutions\": [], \"pays\": [], \"organisations\": []},\n",
    "  \"concepts_cles\": [],\n",
    "  \"donnees_chiffrees\": [],\n",
    "  \"references_principales\": []\n",
    "}\n",
    "R√©pondez UNIQUEMENT avec le JSON, sans texte avant ou apr√®s.\"\"\"\n",
    "\n",
    "def _safe_json(obj):\n",
    "    try:\n",
    "        return json.loads(json.dumps(obj))\n",
    "    except Exception:\n",
    "        return {\"raw\": str(obj)}\n",
    "\n",
    "\n",
    "def extract_with_langextract(markdown_text, doc_name, doc_folders, references_data=None, figures_data=None):\n",
    "    \"\"\"Extraction structur√©e avec LangExtract ou Ollama directement.\"\"\"\n",
    "    if not USE_LANGEXTRACT:\n",
    "        return {\"status\": \"skipped\", \"reason\": \"USE_LANGEXTRACT=False\"}\n",
    "\n",
    "    # Pr√©parer le texte enrichi (limiter la taille pour le LLM)\n",
    "    max_chars = 15000  # Limite pour √©viter les timeouts\n",
    "    enriched_text = markdown_text[:max_chars]\n",
    "\n",
    "    if references_data and references_data.get(\"reference_count\", 0) > 0:\n",
    "        enriched_text += f\"\\n\\n[R√âF√âRENCES EXTRAITES: {references_data['reference_count']}]\"\n",
    "\n",
    "    if figures_data:\n",
    "        enriched_text += f\"\\n\\n[FIGURES D√âTECT√âES: {len(figures_data)}]\"\n",
    "\n",
    "    result = None\n",
    "\n",
    "    # === TENTATIVE 1: LangExtract (nouvelle API) ===\n",
    "    try:\n",
    "        import langextract as lx\n",
    "\n",
    "        # Essayer diff√©rentes signatures d'API\n",
    "        if hasattr(lx, \"extract\"):\n",
    "            # Nouvelle API sans prompt\n",
    "            try:\n",
    "                extraction = lx.extract(enriched_text)\n",
    "                result = _safe_json(extraction)\n",
    "            except TypeError:\n",
    "                # Essayer avec d'autres param√®tres\n",
    "                try:\n",
    "                    extraction = lx.extract(enriched_text, model=LANGEXTRACT_CONFIG[\"model\"])\n",
    "                    result = _safe_json(extraction)\n",
    "                except:\n",
    "                    pass\n",
    "        elif hasattr(lx, \"LangExtract\"):\n",
    "            try:\n",
    "                extractor = lx.LangExtract()\n",
    "                extraction = extractor.extract(enriched_text)\n",
    "                result = _safe_json(extraction)\n",
    "            except:\n",
    "                pass\n",
    "    except Exception as e:\n",
    "        pass  # Continuer avec Ollama\n",
    "\n",
    "    # === TENTATIVE 2: Ollama directement ===\n",
    "    if result is None and OLLAMA_AVAILABLE:\n",
    "        try:\n",
    "            import requests\n",
    "\n",
    "            ollama_payload = {\n",
    "                \"model\": \"gemma3:4b\",\n",
    "                \"prompt\": f\"{EXTRACTION_PROMPT}\\n\\nDOCUMENT:\\n{enriched_text[:10000]}\",\n",
    "                \"stream\": False,\n",
    "                \"options\": {\"temperature\": 0.1}\n",
    "            }\n",
    "\n",
    "            resp = requests.post(\n",
    "                \"http://localhost:11434/api/generate\",\n",
    "                json=ollama_payload,\n",
    "                timeout=120\n",
    "            )\n",
    "\n",
    "            if resp.status_code == 200:\n",
    "                ollama_result = resp.json()\n",
    "                response_text = ollama_result.get(\"response\", \"\")\n",
    "\n",
    "                # Extraire le JSON de la r√©ponse\n",
    "                json_match = re.search(r'\\{[\\s\\S]*\\}', response_text)\n",
    "                if json_match:\n",
    "                    try:\n",
    "                        result = json.loads(json_match.group())\n",
    "                    except json.JSONDecodeError:\n",
    "                        result = {\"raw_response\": response_text[:500]}\n",
    "                else:\n",
    "                    result = {\"raw_response\": response_text[:500]}\n",
    "        except Exception as e:\n",
    "            result = {\"status\": \"error\", \"error\": f\"Ollama: {str(e)}\"}\n",
    "\n",
    "    # === FALLBACK: Extraction basique sans LLM ===\n",
    "    if result is None:\n",
    "        result = {\n",
    "            \"status\": \"fallback\",\n",
    "            \"note\": \"Extraction LLM indisponible, donn√©es de base uniquement\",\n",
    "            \"references_count\": references_data.get(\"reference_count\", 0) if references_data else 0,\n",
    "            \"figures_count\": len(figures_data) if figures_data else 0,\n",
    "        }\n",
    "\n",
    "    # Sauvegarder le r√©sultat\n",
    "    extraction_path = doc_folders[\"analyses\"] / f\"{doc_name}_langextract.json\"\n",
    "    with open(extraction_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "print(f\"‚úÖ LangExtract configur√©\")\n",
    "print(f\"   Provider: {LANGEXTRACT_CONFIG['provider']}\")\n",
    "print(f\"   Model: {LANGEXTRACT_CONFIG['model']}\")\n",
    "print(f\"   Fallback Ollama: {'Activ√©' if OLLAMA_AVAILABLE else 'D√©sactiv√©'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216e2839",
   "metadata": {
    "id": "216e2839"
   },
   "source": [
    "## √âtape 10 ‚Äî Test sur un PDF (optionnel)\n",
    "Permet de valider la configuration avant le batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b595a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4b595a6",
    "outputId": "4fb45cdd-5553-4a12-a854-216bd156f193"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Test sur:  Against Sovereignty in Cyberspace.pdf\n",
      "‚è≥ Conversion en cours...\n",
      "\n",
      "   DEBUG: Tentative de conversion initiale pour Against Sovereignty in Cyberspace.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recognizing Layout: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23/23 [00:12<00:00,  1.82it/s]\n",
      "Running OCR Error Detection: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 22.53it/s]\n",
      "Detecting bboxes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.59it/s]\n",
      "Recognizing Text: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 41/41 [01:19<00:00,  1.95s/it]\n",
      "Recognizing tables: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.93it/s]\n",
      "Detecting bboxes: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   DEBUG: Premi√®re conversion r√©ussie.\n",
      "‚úÖ Conversion r√©ussie en 104.1s! \n",
      "   üìÇ Dossier: /content/drive/MyDrive/G√©opolitique et Souverainet√© Num√©riques/ALL/ALLMD/Against Sovereignty in Cyberspace\n",
      "   üìÑ Markdown: /content/drive/MyDrive/G√©opolitique et Souverainet√© Num√©riques/ALL/ALLMD/Against Sovereignty in Cyberspace/Against Sovereignty in Cyberspace.md\n",
      "   üñºÔ∏è Figures: 0\n",
      "   üìö R√©f√©rences: 0\n"
     ]
    }
   ],
   "source": [
    "pdf_files = sorted([p for p in INPUT_DIR.iterdir() if p.suffix.lower() == \".pdf\"])\n",
    "\n",
    "if pdf_files:\n",
    "    sample_path = pdf_files[0]\n",
    "    sample_name = sample_path.stem\n",
    "    print(f\"üß™ Test sur: {sample_path.name}\")\n",
    "    print(\"‚è≥ Conversion en cours...\\n\")\n",
    "\n",
    "    test_start = time.time()\n",
    "    sample_result = convert_pdf_complete(sample_path, sample_name)\n",
    "    test_elapsed = time.time() - test_start\n",
    "\n",
    "    if sample_result.get(\"error\"):\n",
    "        print(f\"\\n‚ùå Erreur: {sample_result['error']}\")\n",
    "    else:\n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        print(\"‚úÖ CONVERSION R√âUSSIE!\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"‚è±Ô∏è Dur√©e: {test_elapsed:.1f}s\")\n",
    "        print(f\"\\nüìÇ Dossier de sortie:\")\n",
    "        print(f\"   {sample_result['doc_folder']}\")\n",
    "\n",
    "        print(f\"\\nüìÑ MARKDOWN:\")\n",
    "        print(f\"   Fichier: {sample_result['markdown_path']}\")\n",
    "        # Afficher un extrait du contenu\n",
    "        try:\n",
    "            with open(sample_result['markdown_path'], 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            print(f\"   Taille: {len(content):,} caract√®res\")\n",
    "            print(f\"   Lignes: {content.count(chr(10)):,}\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        print(f\"\\nüñºÔ∏è FIGURES:\")\n",
    "        figures = sample_result.get('figures', [])\n",
    "        print(f\"   D√©tect√©es dans le texte: {len(figures)}\")\n",
    "        print(f\"   Images sauvegard√©es: {len(sample_result.get('figures_paths', []))}\")\n",
    "        if figures:\n",
    "            print(\"   Exemples:\")\n",
    "            for fig in figures[:5]:\n",
    "                fig_type = fig.get('type', 'unknown')\n",
    "                label = fig.get('label', '')[:60]\n",
    "                print(f\"      [{fig_type}] {label}\")\n",
    "            if len(figures) > 5:\n",
    "                print(f\"      ... et {len(figures)-5} autres\")\n",
    "\n",
    "        print(f\"\\nüìö R√âF√âRENCES:\")\n",
    "        refs = sample_result.get('references', {})\n",
    "        ref_count = refs.get('reference_count', 0)\n",
    "        print(f\"   Nombre: {ref_count}\")\n",
    "        if ref_count > 0:\n",
    "            print(\"   Exemples:\")\n",
    "            for ref in refs.get('references_list', [])[:3]:\n",
    "                print(f\"      ‚Ä¢ {ref[:80]}...\")\n",
    "            if ref_count > 3:\n",
    "                print(f\"      ... et {ref_count-3} autres\")\n",
    "        elif refs.get('references_text'):\n",
    "            print(f\"   ‚ö†Ô∏è Section trouv√©e mais parsing √©chou√©\")\n",
    "            print(f\"   Texte brut ({len(refs['references_text'])} chars):\")\n",
    "            print(f\"      {refs['references_text'][:200]}...\")\n",
    "\n",
    "        print(f\"\\nüìã M√âTADONN√âES:\")\n",
    "        meta = sample_result.get('metadata', {})\n",
    "        if meta.get('title'):\n",
    "            print(f\"   Titre: {meta['title'][:80]}\")\n",
    "        if meta.get('abstract'):\n",
    "            print(f\"   Abstract: {meta['abstract'][:100]}...\")\n",
    "        if meta.get('keywords'):\n",
    "            print(f\"   Keywords: {', '.join(meta['keywords'][:5])}\")\n",
    "        if meta.get('date'):\n",
    "            print(f\"   Date: {meta['date']}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "        # === DIAGNOSTIC AVANC√â si pas de figures/refs ===\n",
    "        if len(figures) == 0 or ref_count == 0:\n",
    "            print(\"\\nüîç DIAGNOSTIC (extraction insuffisante):\")\n",
    "            try:\n",
    "                with open(sample_result['markdown_path'], 'r', encoding='utf-8') as f:\n",
    "                    md_content = f.read()\n",
    "\n",
    "                # Chercher des patterns de figures\n",
    "                fig_matches = re.findall(r\"(?i)(figure|fig\\.?|table|tableau)\\s*\\d\", md_content)\n",
    "                print(f\"   Mentions 'figure/table' trouv√©es: {len(fig_matches)}\")\n",
    "                if fig_matches:\n",
    "                    print(f\"      Exemples: {fig_matches[:5]}\")\n",
    "\n",
    "                # Chercher des images markdown\n",
    "                img_matches = re.findall(r\"!\\[[^\\]]*\\]\\([^)]+\\)\", md_content)\n",
    "                print(f\"   Images markdown (![]()): {len(img_matches)}\")\n",
    "\n",
    "                # Chercher des sections de r√©f√©rences\n",
    "                ref_sections = re.findall(r\"(?i)(references?|bibliography|bibliographie)\", md_content)\n",
    "                print(f\"   Mentions 'references/bibliography': {len(ref_sections)}\")\n",
    "\n",
    "                # Chercher des citations num√©rot√©es\n",
    "                citations = re.findall(r\"\\[\\d+\\]\", md_content)\n",
    "                print(f\"   Citations num√©riques [n]: {len(citations)}\")\n",
    "\n",
    "                # Afficher la fin du document (souvent les r√©f√©rences)\n",
    "                print(f\"\\n   üìú Fin du document (2000 derniers chars):\")\n",
    "                print(\"   \" + \"-\"*50)\n",
    "                print(md_content[-2000:].replace('\\n', '\\n   '))\n",
    "            except Exception as e:\n",
    "                print(f\"   Erreur diagnostic: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Aucun PDF trouv√© dans le dossier d'entr√©e.\")\n",
    "    print(f\"   V√©rifiez le chemin: {INPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e110727a",
   "metadata": {
    "id": "e110727a"
   },
   "source": [
    "## √âtape 11 ‚Äî Pipeline complet avec reprise\n",
    "Traitement batch + logs + reprise automatique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc4b352",
   "metadata": {
    "id": "acc4b352"
   },
   "outputs": [],
   "source": [
    "import time as time_module\n",
    "\n",
    "# === CONFIGURATION GROS FICHIERS ===\n",
    "MAX_PAGES_BEFORE_WARNING = 100\n",
    "MAX_PAGES_BEFORE_SKIP = 500  # Fichiers > 500 pages seront trait√©s en dernier\n",
    "MEMORY_CHECK_INTERVAL = 5   # V√©rifier la m√©moire tous les N fichiers\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Retourne l'utilisation m√©moire en GB.\"\"\"\n",
    "    try:\n",
    "        import psutil\n",
    "        process = psutil.Process()\n",
    "        return process.memory_info().rss / 1e9\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def process_all_documents():\n",
    "    \"\"\"Traite tous les documents PDF avec reprise automatique et gestion m√©moire.\"\"\"\n",
    "\n",
    "    all_pdfs = sorted([p for p in INPUT_DIR.iterdir() if p.suffix.lower() == \".pdf\"])\n",
    "\n",
    "    if not all_pdfs:\n",
    "        print(\"‚ùå Aucun PDF trouv√© dans le dossier d'entr√©e!\")\n",
    "        print(f\"   Chemin v√©rifi√©: {INPUT_DIR}\")\n",
    "        return [], []\n",
    "\n",
    "    # Trier les PDFs par taille (plus petits d'abord pour √©viter les crashs)\n",
    "    print(\"üìä Analyse des fichiers...\")\n",
    "    pdf_info = []\n",
    "    for pdf in all_pdfs:\n",
    "        page_count = get_pdf_page_count(pdf)\n",
    "        pdf_info.append({\"path\": pdf, \"pages\": page_count})\n",
    "\n",
    "    # Trier: petits fichiers d'abord, tr√®s gros en dernier\n",
    "    pdf_info.sort(key=lambda x: (x[\"pages\"] > MAX_PAGES_BEFORE_SKIP, x[\"pages\"]))\n",
    "\n",
    "    log_file = GLOBAL_LOGS_DIR / f\"processing_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "    progress_file = GLOBAL_LOGS_DIR / \"progress.json\"\n",
    "\n",
    "    def log_message(message):\n",
    "        print(message)\n",
    "        with open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(message + \"\\n\")\n",
    "\n",
    "    processed_files = set()\n",
    "    if progress_file.exists():\n",
    "        try:\n",
    "            with open(progress_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                progress_data = json.load(f)\n",
    "                processed_files = set(progress_data.get(\"processed\", []))\n",
    "            log_message(f\"üìÇ Reprise: {len(processed_files)} fichiers d√©j√† trait√©s\")\n",
    "        except Exception:\n",
    "            processed_files = set()\n",
    "\n",
    "    remaining_pdfs = [p for p in pdf_info if p[\"path\"].name not in processed_files]\n",
    "\n",
    "    # Statistiques\n",
    "    small_files = sum(1 for p in remaining_pdfs if p[\"pages\"] <= MAX_PAGES_BEFORE_WARNING)\n",
    "    medium_files = sum(1 for p in remaining_pdfs if MAX_PAGES_BEFORE_WARNING < p[\"pages\"] <= MAX_PAGES_BEFORE_SKIP)\n",
    "    large_files = sum(1 for p in remaining_pdfs if p[\"pages\"] > MAX_PAGES_BEFORE_SKIP)\n",
    "\n",
    "    log_message(\"=\"*60)\n",
    "    log_message(f\"üìÑ Total PDFs: {len(all_pdfs)}\")\n",
    "    log_message(f\"‚úÖ D√©j√† trait√©s: {len(processed_files)}\")\n",
    "    log_message(f\"‚è≥ Restants: {len(remaining_pdfs)}\")\n",
    "    log_message(f\"   ‚Ä¢ Petits (<{MAX_PAGES_BEFORE_WARNING} pages): {small_files}\")\n",
    "    log_message(f\"   ‚Ä¢ Moyens ({MAX_PAGES_BEFORE_WARNING}-{MAX_PAGES_BEFORE_SKIP} pages): {medium_files}\")\n",
    "    log_message(f\"   ‚Ä¢ Gros (>{MAX_PAGES_BEFORE_SKIP} pages): {large_files}\")\n",
    "    log_message(\"=\"*60)\n",
    "\n",
    "    if not remaining_pdfs:\n",
    "        log_message(\"‚úÖ Tous les fichiers ont d√©j√† √©t√© trait√©s!\")\n",
    "        return [], []\n",
    "\n",
    "    results = []\n",
    "    errors = []\n",
    "    start_time = time_module.time()\n",
    "\n",
    "    def save_progress():\n",
    "        with open(progress_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"processed\": list(processed_files),\n",
    "                \"last_update\": datetime.now().isoformat()\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    for idx, pdf_data in enumerate(remaining_pdfs, 1):\n",
    "        pdf_path = pdf_data[\"path\"]\n",
    "        page_count = pdf_data[\"pages\"]\n",
    "        doc_name = pdf_path.stem\n",
    "        doc_start_time = time_module.time()\n",
    "\n",
    "        # V√©rification m√©moire p√©riodique\n",
    "        if idx % MEMORY_CHECK_INTERVAL == 0:\n",
    "            mem_usage = get_memory_usage()\n",
    "            if mem_usage > 10:  # > 10 GB utilis√©s\n",
    "                log_message(f\"   ‚ö†Ô∏è M√©moire √©lev√©e ({mem_usage:.1f} GB) - Lib√©ration...\")\n",
    "                clear_memory()\n",
    "\n",
    "        log_message(f\"\\n[{idx}/{len(remaining_pdfs)}] üöÄ Traitement: {pdf_path.name}\")\n",
    "        log_message(f\"   üìÑ Pages: {page_count}\")\n",
    "\n",
    "        if page_count > MAX_PAGES_BEFORE_SKIP:\n",
    "            log_message(f\"   ‚ö†Ô∏è Document tr√®s volumineux - Traitement en mode ultra-l√©ger\")\n",
    "\n",
    "        conversion_result = convert_pdf_complete(pdf_path, doc_name)\n",
    "\n",
    "        if conversion_result.get(\"error\"):\n",
    "            errors.append({\"file\": pdf_path.name, \"error\": conversion_result[\"error\"], \"pages\": page_count})\n",
    "            log_message(f\"   ‚ùå Erreur conversion: {conversion_result['error'][:100]}\")\n",
    "            processed_files.add(pdf_path.name)\n",
    "            save_progress()\n",
    "            clear_memory()\n",
    "            continue\n",
    "\n",
    "        doc_folders = get_doc_folders(doc_name)\n",
    "\n",
    "        log_message(\n",
    "            f\"   üñºÔ∏è Figures: {len(conversion_result['figures'])} trouv√©es, \"\n",
    "            f\"{len(conversion_result['figures_paths'])} sauvegard√©es\"\n",
    "        )\n",
    "        log_message(\n",
    "            f\"   üìö R√©f√©rences: {conversion_result['references'].get('reference_count', 0)} extraites\"\n",
    "        )\n",
    "\n",
    "        extraction = None\n",
    "        # D√©sactiver LangExtract pour les tr√®s gros fichiers (√©conomie m√©moire)\n",
    "        if USE_LANGEXTRACT and page_count <= MAX_PAGES_BEFORE_SKIP:\n",
    "            log_message(\"   üîç Extraction LangExtract...\")\n",
    "            try:\n",
    "                with open(conversion_result['markdown_path'], 'r', encoding='utf-8') as f:\n",
    "                    md_content = f.read()\n",
    "                extraction = extract_with_langextract(\n",
    "                    md_content,\n",
    "                    doc_name,\n",
    "                    doc_folders,\n",
    "                    conversion_result['references'],\n",
    "                    conversion_result['figures'],\n",
    "                )\n",
    "                del md_content\n",
    "                if extraction.get(\"status\") == \"error\":\n",
    "                    log_message(f\"   ‚ö†Ô∏è LangExtract: {extraction.get('error', 'Erreur inconnue')[:50]}\")\n",
    "                else:\n",
    "                    log_message(\"   ‚úÖ LangExtract termin√©\")\n",
    "            except Exception as e:\n",
    "                log_message(f\"   ‚ö†Ô∏è LangExtract ignor√©: {str(e)[:50]}\")\n",
    "        elif page_count > MAX_PAGES_BEFORE_SKIP:\n",
    "            log_message(\"   ‚è≠Ô∏è LangExtract ignor√© (document trop volumineux)\")\n",
    "\n",
    "        analysis = {\n",
    "            \"doc_name\": doc_name,\n",
    "            \"source_pdf\": str(pdf_path),\n",
    "            \"doc_folder\": str(doc_folders[\"root\"]),\n",
    "            \"processed_at\": datetime.now().isoformat(),\n",
    "            \"processing_time_seconds\": round(time_module.time() - doc_start_time, 2),\n",
    "            \"page_count\": page_count,\n",
    "            \"conversion\": {\n",
    "                \"markdown_path\": conversion_result['markdown_path'],\n",
    "                \"figures_count\": len(conversion_result['figures']),\n",
    "                \"figures_saved\": len(conversion_result['figures_paths']),\n",
    "                \"figures_paths\": conversion_result['figures_paths'],\n",
    "                \"references_count\": conversion_result['references'].get('reference_count', 0),\n",
    "            },\n",
    "            \"references\": conversion_result['references'],\n",
    "            \"figures\": conversion_result['figures'],\n",
    "            \"langextract\": extraction,\n",
    "        }\n",
    "\n",
    "        analysis_path = doc_folders[\"analyses\"] / f\"{doc_name}_analysis.json\"\n",
    "        with open(analysis_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(analysis, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        doc_log_path = doc_folders[\"logs\"] / \"processing.log\"\n",
    "        with open(doc_log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"Trait√© le: {datetime.now().isoformat()}\\n\")\n",
    "            f.write(f\"Pages: {page_count}\\n\")\n",
    "            f.write(f\"Dur√©e: {analysis['processing_time_seconds']} secondes\\n\")\n",
    "            f.write(f\"Figures: {len(conversion_result['figures_paths'])}\\n\")\n",
    "            f.write(f\"R√©f√©rences: {conversion_result['references'].get('reference_count', 0)}\\n\")\n",
    "\n",
    "        results.append(analysis)\n",
    "        processed_files.add(pdf_path.name)\n",
    "        save_progress()\n",
    "\n",
    "        elapsed = time_module.time() - doc_start_time\n",
    "        log_message(f\"   ‚è±Ô∏è Dur√©e: {elapsed:.1f}s\")\n",
    "\n",
    "        # Lib√©rer la m√©moire apr√®s chaque document\n",
    "        del conversion_result, analysis\n",
    "        clear_memory()\n",
    "\n",
    "    total_time = time_module.time() - start_time\n",
    "    log_message(\"\\n\" + \"=\"*60)\n",
    "    log_message(f\"‚úÖ Traitement termin√© en {total_time/60:.1f} minutes\")\n",
    "    log_message(f\"   üìÑ R√©ussis: {len(results)}\")\n",
    "    log_message(f\"   ‚ùå Erreurs: {len(errors)}\")\n",
    "    log_message(\"=\"*60)\n",
    "\n",
    "    return results, errors\n",
    "\n",
    "\n",
    "def export_all_results(results):\n",
    "    \"\"\"Exporte les r√©sultats globaux.\"\"\"\n",
    "    if not results:\n",
    "        print(\"‚ÑπÔ∏è Aucun r√©sultat √† exporter.\")\n",
    "        return\n",
    "\n",
    "    summary_path = GLOBAL_LOGS_DIR / \"_SUMMARY.json\"\n",
    "    report_md_path = GLOBAL_LOGS_DIR / \"_REPORT.md\"\n",
    "\n",
    "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    all_references = []\n",
    "    for data in results:\n",
    "        refs = data.get(\"references\", {})\n",
    "        if refs.get(\"references_list\"):\n",
    "            for ref in refs[\"references_list\"]:\n",
    "                all_references.append({\n",
    "                    \"document\": data.get(\"doc_name\", \"\"),\n",
    "                    \"reference\": ref,\n",
    "                })\n",
    "\n",
    "    if all_references:\n",
    "        biblio_path = GLOBAL_LOGS_DIR / \"_BIBLIOGRAPHIE_COMPLETE.json\"\n",
    "        with open(biblio_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(all_references, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"üìö Bibliographie: {biblio_path} ({len(all_references)} r√©f√©rences)\")\n",
    "\n",
    "    all_figures = []\n",
    "    for data in results:\n",
    "        for fig_path in data.get(\"conversion\", {}).get(\"figures_paths\", []):\n",
    "            all_figures.append({\n",
    "                \"document\": data.get(\"doc_name\", \"\"),\n",
    "                \"path\": fig_path,\n",
    "            })\n",
    "\n",
    "    if all_figures:\n",
    "        figures_index_path = GLOBAL_LOGS_DIR / \"_INDEX_FIGURES.json\"\n",
    "        with open(figures_index_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(all_figures, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"üñºÔ∏è Index figures: {figures_index_path} ({len(all_figures)} figures)\")\n",
    "\n",
    "    # Statistiques par taille\n",
    "    small = [r for r in results if r.get(\"page_count\", 0) <= 100]\n",
    "    medium = [r for r in results if 100 < r.get(\"page_count\", 0) <= 500]\n",
    "    large = [r for r in results if r.get(\"page_count\", 0) > 500]\n",
    "\n",
    "    md_lines = []\n",
    "    md_lines.append(\"# Rapport de traitement MFEGSN\\n\\n\")\n",
    "    md_lines.append(f\"**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M')}\\n\\n\")\n",
    "    md_lines.append(\"## Statistiques globales\\n\\n\")\n",
    "    md_lines.append(f\"- **Total documents trait√©s:** {len(results)}\\n\")\n",
    "    md_lines.append(f\"- **Total figures extraites:** {len(all_figures)}\\n\")\n",
    "    md_lines.append(f\"- **Total r√©f√©rences:** {len(all_references)}\\n\\n\")\n",
    "    md_lines.append(\"### Par taille de document\\n\\n\")\n",
    "    md_lines.append(f\"- Petits (‚â§100 pages): {len(small)}\\n\")\n",
    "    md_lines.append(f\"- Moyens (101-500 pages): {len(medium)}\\n\")\n",
    "    md_lines.append(f\"- Gros (>500 pages): {len(large)}\\n\\n\")\n",
    "    md_lines.append(\"## Documents trait√©s\\n\\n\")\n",
    "\n",
    "    for data in results:\n",
    "        conv = data.get(\"conversion\", {})\n",
    "        pages = data.get(\"page_count\", 0)\n",
    "        md_lines.append(f\"### {data.get('doc_name', '')}\\n\\n\")\n",
    "        md_lines.append(f\"- **Pages:** {pages}\\n\")\n",
    "        md_lines.append(f\"- **Figures:** {conv.get('figures_count', 0)}\\n\")\n",
    "        md_lines.append(f\"- **R√©f√©rences:** {conv.get('references_count', 0)}\\n\")\n",
    "        md_lines.append(f\"- **Dur√©e:** {data.get('processing_time_seconds', 0)}s\\n\\n\")\n",
    "\n",
    "    with open(report_md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\".join(md_lines))\n",
    "\n",
    "    print(f\"‚úÖ R√©sultats export√©s:\")\n",
    "    print(f\"   üìä Summary: {summary_path}\")\n",
    "    print(f\"   üìù Report: {report_md_path}\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Fonctions de pipeline d√©finies.\")\n",
    "print(\"   ‚Ä¢ Tri intelligent: petits fichiers trait√©s en premier\")\n",
    "print(\"   ‚Ä¢ Lib√©ration m√©moire automatique\")\n",
    "print(\"   ‚Ä¢ Gros fichiers (>500 pages): mode ultra-l√©ger\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830ddf11",
   "metadata": {
    "id": "830ddf11"
   },
   "source": [
    "## √âtape 12 ‚Äî Lancer le traitement\n",
    "Ex√©cutez cette cellule pour lancer le traitement complet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0f8f92",
   "metadata": {
    "id": "4c0f8f92"
   },
   "outputs": [],
   "source": [
    "# Recharger la liste des PDFs depuis INPUT_DIR pour s'assurer qu'elle est √† jour\n",
    "print(\"üìÇ V√©rification du dossier d'entr√©e...\")\n",
    "print(f\"   Chemin: {INPUT_DIR}\")\n",
    "\n",
    "# Recharger la liste des fichiers\n",
    "pdf_files = sorted([p for p in INPUT_DIR.iterdir() if p.suffix.lower() == \".pdf\"])\n",
    "print(f\"   PDFs trouv√©s: {len(pdf_files)}\")\n",
    "\n",
    "if len(pdf_files) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è ATTENTION: Aucun PDF trouv√©!\")\n",
    "    print(\"   V√©rifiez que:\")\n",
    "    print(\"   1. Le chemin INPUT_DIR est correct\")\n",
    "    print(\"   2. Des fichiers PDF sont pr√©sents dans ce dossier\")\n",
    "    print(\"   3. Google Drive est bien mont√©\")\n",
    "else:\n",
    "    for i, pdf in enumerate(pdf_files[:5], 1):\n",
    "        print(f\"   {i}. {pdf.name}\")\n",
    "    if len(pdf_files) > 5:\n",
    "        print(f\"   ... et {len(pdf_files) - 5} autres\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ D√©marrage du pipeline...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results, errors = process_all_documents()\n",
    "\n",
    "if results:\n",
    "    export_all_results(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ PIPELINE TERMIN√â\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if errors:\n",
    "    print(f\"\\n‚ö†Ô∏è {len(errors)} erreur(s) rencontr√©e(s):\")\n",
    "    for err in errors[:5]:\n",
    "        print(f\"   - {err['file']}: {err['error'][:50]}...\")\n",
    "    if len(errors) > 5:\n",
    "        print(f\"   ... et {len(errors) - 5} autres erreurs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utils001",
   "metadata": {
    "id": "utils001"
   },
   "source": [
    "## Utilitaires ‚Äî Gestion du cache\n",
    "Cellules optionnelles pour g√©rer le cache des mod√®les sur Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utils002",
   "metadata": {
    "id": "utils002"
   },
   "outputs": [],
   "source": [
    "# === AFFICHER LA TAILLE DU CACHE ===\n",
    "def show_cache_info():\n",
    "    \"\"\"Affiche les informations sur le cache Drive.\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"üíæ INFORMATIONS CACHE\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for name, path in [(\"HuggingFace\", HF_CACHE_DRIVE),\n",
    "                       (\"Torch\", TORCH_CACHE_DRIVE),\n",
    "                       (\"Datalab\", DATALAB_CACHE_DRIVE)]:\n",
    "        size = get_dir_size(path)\n",
    "        print(f\"üìÇ {name}: {size:.2f} GB\")\n",
    "\n",
    "    total = get_dir_size(DRIVE_CACHE_DIR)\n",
    "    print(f\"\\nüì¶ Total: {total:.2f} GB\")\n",
    "    print(f\"üìç Emplacement: {DRIVE_CACHE_DIR}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Appeler la fonction pour afficher les infos\n",
    "show_cache_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utils003",
   "metadata": {
    "id": "utils003"
   },
   "outputs": [],
   "source": [
    "# === SUPPRIMER LE CACHE (si n√©cessaire) ===\n",
    "# ‚ö†Ô∏è ATTENTION:  Cela supprimera tous les mod√®les en cache!\n",
    "# D√©commentez les lignes ci-dessous pour ex√©cuter.\n",
    "\n",
    "# import shutil\n",
    "#\n",
    "# if DRIVE_CACHE_DIR. exists():\n",
    "#     print(f\"‚ö†Ô∏è Suppression du cache:  {DRIVE_CACHE_DIR}\")\n",
    "#     shutil.rmtree(DRIVE_CACHE_DIR)\n",
    "#     print(\"‚úÖ Cache supprim√©.  Les mod√®les seront ret√©l√©charg√©s au prochain lancement.\")\n",
    "# else:\n",
    "#     print(\"‚ÑπÔ∏è Aucun cache √† supprimer.\")\n",
    "\n",
    "print(\"‚ÑπÔ∏è D√©commentez le code ci-dessus pour supprimer le cache.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
