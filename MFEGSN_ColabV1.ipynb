{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b54619af",
   "metadata": {},
   "source": [
    "# MFEGSN ‚Äî Pipeline Colab (Marker + LangExtract)\n",
    "\n",
    "Notebook optimis√© pour une ex√©cution **pas √† pas** sur Google Colab.\n",
    "Il s√©pare clairement les **blocs texte** (explications) et les **blocs code** (ex√©cution).\n",
    "\n",
    "**Ordre recommand√© :** √âtapes 1 ‚Üí 10, avec tests optionnels si besoin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c063508",
   "metadata": {},
   "source": [
    "## √âtape 1 ‚Äî Installer les d√©pendances\n",
    "- Syst√®mes : zstd (requis pour Ollama).\n",
    "- Python : marker-pdf, langextract, pillow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7a1e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©pendances syst√®me\n",
    "!apt-get update -qq\n",
    "!apt-get install -y zstd -qq\n",
    "\n",
    "# D√©pendances Python\n",
    "!python -m pip install -q --upgrade pip\n",
    "!python -m pip install -q marker-pdf[full] langextract google-generativeai pillow\n",
    "\n",
    "print(\"‚úÖ D√©pendances install√©es.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014f8fe4",
   "metadata": {},
   "source": [
    "## √âtape 2 ‚Äî Monter Google Drive\n",
    "Ex√©cutez cette cellule si vos PDF sont sur Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dce4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9950b81",
   "metadata": {},
   "source": [
    "## √âtape 3 ‚Äî Configurer les dossiers\n",
    "Modifiez les chemins ci-dessous selon votre Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749433f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# === MODIFIEZ CES CHEMINS ===\n",
    "INPUT_DIR = Path(\"/content/drive/MyDrive/G√©opolitique et Souverainet√© Num√©riques/ALL/ALLPDF\")\n",
    "OUTPUT_DIR = Path(\"/content/drive/MyDrive/G√©opolitique et Souverainet√© Num√©riques/ALL/ALLMD\")\n",
    "\n",
    "assert INPUT_DIR.exists(), f\"‚ùå Dossier introuvable : {INPUT_DIR}\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FIGURES_DIR = OUTPUT_DIR / \"_FIGURES\"\n",
    "REFERENCES_DIR = OUTPUT_DIR / \"_REFERENCES\"\n",
    "ANALYSES_DIR = OUTPUT_DIR / \"_ANALYSES\"\n",
    "LOGS_DIR = OUTPUT_DIR / \"_LOGS\"\n",
    "\n",
    "for p in [FIGURES_DIR, REFERENCES_DIR, ANALYSES_DIR, LOGS_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pdf_files = sorted([p for p in INPUT_DIR.iterdir() if p.suffix.lower() == \".pdf\"])\n",
    "print(\"=\" * 60)\n",
    "print(\"üìÅ CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìÇ Entr√©e  : {INPUT_DIR}\")\n",
    "print(f\"üìÇ Sortie  : {OUTPUT_DIR}\")\n",
    "print(f\"üìÑ PDFs    : {len(pdf_files)}\")\n",
    "print(f\"üñºÔ∏è  Figures : {FIGURES_DIR}\")\n",
    "print(f\"üìö R√©f√©rences : {REFERENCES_DIR}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5c801b",
   "metadata": {},
   "source": [
    "## √âtape 4 ‚Äî (Optionnel) Ollama + Gemma 3 4B\n",
    "Activez uniquement si vous utilisez LangExtract avec un mod√®le local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352143f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_OLLAMA = False  # Mettre True si vous voulez Gemma via Ollama\n",
    "\n",
    "if USE_OLLAMA:\n",
    "    import subprocess\n",
    "    import time\n",
    "\n",
    "    # Installer Ollama (si besoin)\n",
    "    !curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "    # D√©marrer Ollama en arri√®re-plan\n",
    "    ollama_process = subprocess.Popen(\n",
    "        [\"ollama\", \"serve\"],\n",
    "        stdout=subprocess.DEVNULL,\n",
    "        stderr=subprocess.DEVNULL\n",
    "    )\n",
    "    print(\"‚è≥ D√©marrage d'Ollama...\")\n",
    "    time.sleep(10)\n",
    "\n",
    "    # T√©l√©charger Gemma 3 4B\n",
    "    print(\"üì• T√©l√©chargement de Gemma 3 4B (‚âà3GB)...\")\n",
    "    !ollama pull gemma3:4b\n",
    "    !ollama list\n",
    "    print(\"‚úÖ Gemma 3 4B pr√™t !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ae817d",
   "metadata": {},
   "source": [
    "## √âtape 5 ‚Äî Configurer Marker\n",
    "R√©glages optimis√©s (2 workers + extraction figures + r√©f√©rences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0f4c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import shutil\n",
    "import base64\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from marker.converters.pdf import PdfConverter\n",
    "from marker.models import create_model_dict\n",
    "from marker.config.parser import ConfigParser\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üñ•Ô∏è  Device : {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"   GPU :  {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   VRAM : {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "marker_config = {\n",
    "    \"workers\": 2,\n",
    "    \"extract_images\": True,\n",
    "    \"images_as_base64\": False,\n",
    "    \"use_llm\": False,\n",
    "    \"force_ocr\": False,\n",
    "    \"languages\": [\"fr\", \"en\"],\n",
    "    \"paginate_output\": True,\n",
    "    \"batch_size\": 4 if device == \"cuda\" else 2,\n",
    "}\n",
    "\n",
    "print(\"üì• Chargement des mod√®les Marker (premi√®re fois = t√©l√©chargement)...\")\n",
    "print(f\"‚öôÔ∏è  Configuration : {marker_config['workers']} workers, batch_size={marker_config['batch_size']}\")\n",
    "\n",
    "model_dict = create_model_dict()\n",
    "config_parser = ConfigParser(marker_config)\n",
    "converter = PdfConverter(\n",
    "    config=config_parser.generate_config_dict(),\n",
    "    artifact_dict=model_dict,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Marker configur√© avec 2 workers !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0fc302",
   "metadata": {},
   "source": [
    "## √âtape 6 ‚Äî Fonctions utilitaires\n",
    "Extraction des r√©f√©rences, figures et conversion PDF ‚Üí Markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a23cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_references_from_markdown(markdown_text):\n",
    "    \"\"\"Extrait la section r√©f√©rences/bibliographie du Markdown.\"\"\"\n",
    "    references = {\n",
    "        \"references_text\": \"\",\n",
    "        \"references_list\": [],\n",
    "        \"reference_count\": 0,\n",
    "    }\n",
    "\n",
    "    ref_patterns = [\n",
    "        r\"(?i)(?:^|\\n)#{1,3}\\s*(references|r√©f√©rences|bibliography|bibliographie|works\\s*cited|sources?)\\s*[\\s:]*\\n([\\s\\S]*?)(?=\\n#{1,3}\\s|\\Z)\",\n",
    "        r\"(?i)(?:^|\\n)\\*\\*(references|r√©f√©rences|bibliography|bibliographie)\\*\\*\\s*[\\s:]*\\n([\\s\\S]*?)(?=\\n\\*\\*|\\n#{1,3}|\\Z)\",\n",
    "    ]\n",
    "\n",
    "    for pattern in ref_patterns:\n",
    "        match = re.search(pattern, markdown_text, re.MULTILINE)\n",
    "        if match:\n",
    "            ref_section = match.group(2).strip()\n",
    "            references[\"references_text\"] = ref_section\n",
    "\n",
    "            ref_lines = []\n",
    "            lines = ref_section.split(\"\\n\")\n",
    "            current_ref = \"\"\n",
    "\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    if current_ref:\n",
    "                        ref_lines.append(current_ref.strip())\n",
    "                        current_ref = \"\"\n",
    "                    continue\n",
    "\n",
    "                if re.match(r\"^(\\[\\d+\\]|\\d+\\.|[-‚Ä¢]|\\([A-Z])\", line):\n",
    "                    if current_ref:\n",
    "                        ref_lines.append(current_ref.strip())\n",
    "                    current_ref = line\n",
    "                else:\n",
    "                    current_ref += \" \" + line\n",
    "\n",
    "            if current_ref:\n",
    "                ref_lines.append(current_ref.strip())\n",
    "\n",
    "            references[\"references_list\"] = [r for r in ref_lines if len(r) > 20]\n",
    "            references[\"reference_count\"] = len(references[\"references_list\"])\n",
    "            break\n",
    "\n",
    "    return references\n",
    "\n",
    "\n",
    "def extract_figures_info(markdown_text, images_dict):\n",
    "    \"\"\"Extrait les informations sur les figures du document.\"\"\"\n",
    "    figures = []\n",
    "    fig_pattern = r\"(?i)(figure|fig\\.)\\s*(\\d+)?\\s*[:]?\\s*(.{0,120})\"\n",
    "\n",
    "    for match in re.finditer(fig_pattern, markdown_text):\n",
    "        title = (match.group(3) or \"\").strip()\n",
    "        figures.append({\n",
    "            \"label\": match.group(0).strip(),\n",
    "            \"title\": title,\n",
    "        })\n",
    "\n",
    "    if images_dict:\n",
    "        for img_name in images_dict.keys():\n",
    "            existing = any(f.get(\"path\") == img_name for f in figures)\n",
    "            if not existing:\n",
    "                figures.append({\n",
    "                    \"label\": str(img_name),\n",
    "                    \"title\": \"\",\n",
    "                    \"path\": str(img_name),\n",
    "                })\n",
    "\n",
    "    return figures\n",
    "\n",
    "\n",
    "def save_figures(images_dict, doc_name, figures_base_folder):\n",
    "    \"\"\"Sauvegarde les figures extraites dans un dossier d√©di√©.\"\"\"\n",
    "    if not images_dict:\n",
    "        return []\n",
    "\n",
    "    doc_figures_folder = figures_base_folder / doc_name\n",
    "    doc_figures_folder.mkdir(parents=True, exist_ok=True)\n",
    "    saved_paths = []\n",
    "\n",
    "    for img_name, img_data in images_dict.items():\n",
    "        safe_name = re.sub(r\"[^a-zA-Z0-9_-]+\", \"_\", str(img_name))\n",
    "        img_path = doc_figures_folder / f\"{safe_name}.png\"\n",
    "\n",
    "        try:\n",
    "            if isinstance(img_data, Image.Image):\n",
    "                img_data.save(img_path)\n",
    "            elif isinstance(img_data, (bytes, bytearray)):\n",
    "                with open(img_path, \"wb\") as f:\n",
    "                    f.write(img_data)\n",
    "            elif isinstance(img_data, str):\n",
    "                if img_data.startswith(\"data:image\"):\n",
    "                    b64_data = img_data.split(\",\", 1)[1]\n",
    "                    with open(img_path, \"wb\") as f:\n",
    "                        f.write(base64.b64decode(b64_data))\n",
    "                elif Path(img_data).exists():\n",
    "                    shutil.copy(img_data, img_path)\n",
    "                else:\n",
    "                    with open(img_path, \"wb\") as f:\n",
    "                        f.write(base64.b64decode(img_data))\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            saved_paths.append(str(img_path))\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return saved_paths\n",
    "\n",
    "\n",
    "def convert_pdf_complete(pdf_path, doc_name):\n",
    "    \"\"\"Conversion compl√®te d'un PDF avec extraction figures et r√©f√©rences.\"\"\"\n",
    "    result_data = {\n",
    "        \"doc_name\": doc_name,\n",
    "        \"markdown_path\": \"\",\n",
    "        \"figures\": [],\n",
    "        \"figures_paths\": [],\n",
    "        \"references\": {},\n",
    "        \"error\": None,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        result = converter(str(pdf_path))\n",
    "        markdown_text = getattr(result, \"markdown\", \"\") or \"\"\n",
    "        images_dict = getattr(result, \"images\", {}) or {}\n",
    "\n",
    "        md_path = OUTPUT_DIR / f\"{doc_name}.md\"\n",
    "        with open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(markdown_text)\n",
    "\n",
    "        result_data[\"markdown_path\"] = str(md_path)\n",
    "        result_data[\"figures_paths\"] = save_figures(images_dict, doc_name, FIGURES_DIR)\n",
    "        result_data[\"figures\"] = extract_figures_info(markdown_text, images_dict)\n",
    "\n",
    "        result_data[\"references\"] = extract_references_from_markdown(markdown_text)\n",
    "        if result_data[\"references\"].get(\"reference_count\", 0) > 0:\n",
    "            ref_path = REFERENCES_DIR / f\"{doc_name}_references.json\"\n",
    "            with open(ref_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(result_data[\"references\"], f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        return result_data\n",
    "    except Exception as e:\n",
    "        result_data[\"error\"] = str(e)\n",
    "        return result_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f235df66",
   "metadata": {},
   "source": [
    "## √âtape 7 ‚Äî LangExtract (optionnel)\n",
    "Activez uniquement si vous souhaitez l'extraction structur√©e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4261fafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_LANGEXTRACT = False  # Mettre True pour activer LangExtract\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Vous √™tes un assistant d'analyse pour des documents en sciences sociales.\n",
    "Retournez un JSON structur√© avec les sections suivantes :\n",
    "\n",
    "1. CONTEXTE\n",
    "- Th√®me principal\n",
    "- Zone g√©ographique\n",
    "- P√©riode\n",
    "\n",
    "2. ACTEURS\n",
    "- Institutions\n",
    "- Pays\n",
    "- Organisations\n",
    "\n",
    "3. CONCEPTS CL√âS\n",
    "- Mots-cl√©s\n",
    "- Concepts\n",
    "\n",
    "4. DONN√âES\n",
    "- Chiffres cl√©s (si disponibles)\n",
    "\n",
    "5. R√âF√âRENCES\n",
    "- Principales r√©f√©rences cit√©es\n",
    "\n",
    "6. FIGURES ET TABLEAUX\n",
    "- Liste des figures mentionn√©es\n",
    "\n",
    "R√©pondez uniquement avec un JSON valide.\n",
    "\"\"\"\n",
    "\n",
    "def _safe_json(obj):\n",
    "    try:\n",
    "        return json.loads(json.dumps(obj))\n",
    "    except Exception:\n",
    "        return {\"raw\": str(obj)}\n",
    "\n",
    "\n",
    "def extract_with_langextract(markdown_text, doc_name, references_data=None, figures_data=None):\n",
    "    \"\"\"Extraction structur√©e avec LangExtract (optionnel).\"\"\"\n",
    "    if not USE_LANGEXTRACT:\n",
    "        return {\"status\": \"skipped\", \"reason\": \"USE_LANGEXTRACT=False\"}\n",
    "\n",
    "    enriched_text = markdown_text\n",
    "\n",
    "    if references_data and references_data.get(\"reference_count\", 0) > 0:\n",
    "        enriched_text += \"\\n\\n## R√âF√âRENCES\\n\"\n",
    "        enriched_text += f\"Nombre de r√©f√©rences : {references_data['reference_count']}\\n\"\n",
    "        for i, ref in enumerate(references_data.get(\"references_list\", [])[:20], 1):\n",
    "            enriched_text += f\"[{i}] {ref}\\n\"\n",
    "\n",
    "    if figures_data:\n",
    "        enriched_text += \"\\n\\n## FIGURES IDENTIFI√âES\\n\"\n",
    "        enriched_text += f\"Nombre de figures : {len(figures_data)}\\n\"\n",
    "        for fig in figures_data[:10]:\n",
    "            enriched_text += f\"- {fig.get('label', '')} {fig.get('title', '')}\\n\"\n",
    "\n",
    "    try:\n",
    "        import langextract as lx\n",
    "        if hasattr(lx, \"extract\"):\n",
    "            extraction = lx.extract(enriched_text, prompt=PROMPT_TEMPLATE)\n",
    "        elif hasattr(lx, \"LangExtract\"):\n",
    "            extractor = lx.LangExtract()\n",
    "            extraction = extractor.extract(enriched_text, prompt=PROMPT_TEMPLATE)\n",
    "        else:\n",
    "            return {\"status\": \"error\", \"error\": \"API LangExtract introuvable\"}\n",
    "\n",
    "        return _safe_json(extraction)\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216e2839",
   "metadata": {},
   "source": [
    "## √âtape 8 ‚Äî Test sur un PDF (optionnel)\n",
    "Permet de valider la configuration avant le batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b595a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pdf_files:\n",
    "    sample_path = pdf_files[0]\n",
    "    sample_name = sample_path.stem\n",
    "    sample_result = convert_pdf_complete(sample_path, sample_name)\n",
    "    sample_result\n",
    "else:\n",
    "    print(\"Aucun PDF trouv√© dans le dossier d'entr√©e.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e110727a",
   "metadata": {},
   "source": [
    "## √âtape 9 ‚Äî Pipeline complet avec reprise\n",
    "Traitement batch + logs + reprise automatique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc4b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_documents():\n",
    "    log_file = LOGS_DIR / f\"processing_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "    progress_file = LOGS_DIR / \"progress.json\"\n",
    "\n",
    "    def log_message(message):\n",
    "        print(message)\n",
    "        with open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(message + \"\\n\")\n",
    "\n",
    "    processed_files = set()\n",
    "    if progress_file.exists():\n",
    "        try:\n",
    "            with open(progress_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                progress_data = json.load(f)\n",
    "                processed_files = set(progress_data.get(\"processed\", []))\n",
    "            log_message(f\"üìÇ Reprise : {len(processed_files)} fichiers d√©j√† trait√©s\")\n",
    "        except Exception:\n",
    "            processed_files = set()\n",
    "\n",
    "    all_pdfs = sorted([p for p in INPUT_DIR.iterdir() if p.suffix.lower() == \".pdf\"])\n",
    "    remaining_pdfs = [p for p in all_pdfs if p.name not in processed_files]\n",
    "\n",
    "    log_message(f\"üìÑ Total : {len(all_pdfs)} | Restants : {len(remaining_pdfs)}\")\n",
    "\n",
    "    results = []\n",
    "    errors = []\n",
    "\n",
    "    def save_progress():\n",
    "        with open(progress_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\"processed\": list(processed_files)}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    for pdf_path in remaining_pdfs:\n",
    "        doc_name = pdf_path.stem\n",
    "        log_message(f\"\\nüöÄ Traitement : {pdf_path.name}\")\n",
    "\n",
    "        conversion_result = convert_pdf_complete(pdf_path, doc_name)\n",
    "        if conversion_result.get(\"error\"):\n",
    "            errors.append({\"file\": pdf_path.name, \"error\": conversion_result[\"error\"]})\n",
    "            log_message(f\"‚ùå Erreur conversion : {conversion_result['error']}\")\n",
    "            continue\n",
    "\n",
    "        log_message(\n",
    "            f\"  üñºÔ∏è  Figures : {len(conversion_result['figures'])} trouv√©es, \"\n",
    "            f\"{len(conversion_result['figures_paths'])} sauvegard√©es\"\n",
    "        )\n",
    "        log_message(\n",
    "            f\"  üìö R√©f√©rences : {conversion_result['references'].get('reference_count', 0)} extraites\"\n",
    "        )\n",
    "\n",
    "        extraction = None\n",
    "        if USE_LANGEXTRACT:\n",
    "            log_message(\"  üîç Extraction LangExtract...\")\n",
    "            extraction = extract_with_langextract(\n",
    "                open(conversion_result['markdown_path'], 'r', encoding='utf-8').read(),\n",
    "                doc_name,\n",
    "                conversion_result['references'],\n",
    "                conversion_result['figures'],\n",
    "            )\n",
    "\n",
    "        analysis = {\n",
    "            \"doc_name\": doc_name,\n",
    "            \"source_pdf\": str(pdf_path),\n",
    "            \"processed_at\": datetime.now().isoformat(),\n",
    "            \"conversion\": {\n",
    "                \"markdown_path\": conversion_result['markdown_path'],\n",
    "                \"figures_count\": len(conversion_result['figures']),\n",
    "                \"figures_saved\": len(conversion_result['figures_paths']),\n",
    "                \"figures_paths\": conversion_result['figures_paths'],\n",
    "                \"references_count\": conversion_result['references'].get('reference_count', 0),\n",
    "            },\n",
    "            \"references\": conversion_result['references'],\n",
    "            \"figures\": conversion_result['figures'],\n",
    "            \"langextract\": extraction,\n",
    "        }\n",
    "\n",
    "        analysis_path = ANALYSES_DIR / f\"{doc_name}_analysis.json\"\n",
    "        with open(analysis_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(analysis, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        results.append(analysis)\n",
    "        processed_files.add(pdf_path.name)\n",
    "        save_progress()\n",
    "\n",
    "    return results, errors\n",
    "\n",
    "\n",
    "def export_all_results(results):\n",
    "    summary_path = ANALYSES_DIR / \"_SUMMARY.json\"\n",
    "    report_md_path = ANALYSES_DIR / \"_REPORT.md\"\n",
    "\n",
    "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Bibliographie compl√®te\n",
    "    all_references = []\n",
    "    for data in results:\n",
    "        refs = data.get(\"references\", {})\n",
    "        if refs.get(\"references_list\"):\n",
    "            for ref in refs[\"references_list\"]:\n",
    "                all_references.append({\n",
    "                    \"document\": data.get(\"doc_name\", \"\"),\n",
    "                    \"reference\": ref,\n",
    "                })\n",
    "\n",
    "    if all_references:\n",
    "        biblio_path = REFERENCES_DIR / \"_BIBLIOGRAPHIE_COMPLETE.json\"\n",
    "        with open(biblio_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(all_references, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"üìö Bibliographie : {biblio_path} ({len(all_references)} r√©f√©rences)\")\n",
    "\n",
    "    # Index des figures\n",
    "    all_figures = []\n",
    "    for data in results:\n",
    "        for fig_path in data.get(\"conversion\", {}).get(\"figures_paths\", []):\n",
    "            all_figures.append({\n",
    "                \"document\": data.get(\"doc_name\", \"\"),\n",
    "                \"path\": fig_path,\n",
    "            })\n",
    "\n",
    "    if all_figures:\n",
    "        figures_index_path = FIGURES_DIR / \"_INDEX_FIGURES.json\"\n",
    "        with open(figures_index_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(all_figures, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"üñºÔ∏è  Index figures : {figures_index_path} ({len(all_figures)} figures)\")\n",
    "\n",
    "    # Rapport Markdown\n",
    "    md_lines = []\n",
    "    md_lines.append(\"# Rapport de traitement\\n\")\n",
    "    md_lines.append(f\"- Total documents : {len(results)}\\n\")\n",
    "    md_lines.append(f\"- Total figures extraites : {len(all_figures)}\\n\")\n",
    "    md_lines.append(f\"- Total r√©f√©rences : {len(all_references)}\\n\\n\")\n",
    "\n",
    "    for data in results:\n",
    "        conv = data.get(\"conversion\", {})\n",
    "        md_lines.append(f\"## {data.get('doc_name', '')}\\n\")\n",
    "        md_lines.append(f\"- Markdown : {conv.get('markdown_path', '')}\\n\")\n",
    "        md_lines.append(f\"- Figures : {conv.get('figures_count', 0)}\\n\")\n",
    "        md_lines.append(f\"- R√©f√©rences : {conv.get('references_count', 0)}\\n\\n\")\n",
    "\n",
    "    with open(report_md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\".join(md_lines))\n",
    "\n",
    "    print(f\"‚úÖ R√©sultats export√©s : {summary_path} | {report_md_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830ddf11",
   "metadata": {},
   "source": [
    "## √âtape 10 ‚Äî Lancer le traitement\n",
    "D√©commentez si n√©cessaire, puis ex√©cutez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0f8f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, errors = process_all_documents()\n",
    "export_all_results(results)\n",
    "\n",
    "print(\"‚úÖ Termin√©.\")\n",
    "if errors:\n",
    "    print(f\"‚ö†Ô∏è  Erreurs : {len(errors)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
